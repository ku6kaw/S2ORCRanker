{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff8ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertPreTrainedModel, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import accelerate\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90dd937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set for embedding generation.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "# ▼▼▼ 訓練済みのS-BERT (Contrastive) モデルのパス ▼▼▼\n",
    "TRAINED_MODEL_PATH = \"models/sbert_contrastive_v1/best_model\" \n",
    "\n",
    "# 出力ファイル（ベクトルとDOIのマップ）\n",
    "EMBEDDINGS_OUTPUT_FILE = \"data/processed/embeddings_sbert_contrastive.npy\"\n",
    "DOI_MAP_OUTPUT_FILE = \"data/processed/embeddings_doi_map.json\"\n",
    "\n",
    "# モデルのハイパーパラメータ\n",
    "MODEL_CHECKPOINT = \"allenai/scibert_scivocab_uncased\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 128 # 推論（Inference）なので、訓練時より大きいバッチサイズが使える\n",
    "\n",
    "print(\"Configuration set for embedding generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47e1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'SiameseContrastiveModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. カスタムモデルクラスの定義 (CLS Pooling) ---\n",
    "# (notebooks/18c... と同一の定義)\n",
    "\n",
    "class SiameseContrastiveModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    S-BERT (Contrastive) モデル (CLS Pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SiameseContrastiveModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_embedding(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        単一のアブストラクトを入力とし、CLSベクトルを返すヘルパー関数\n",
    "        \"\"\"\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output # CLSトークンのベクトル\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        input_ids_b=None,\n",
    "        attention_mask_b=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 訓練用のforwardパス（評価では直接使わない）\n",
    "        output_a = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "        vec_x = output_a.pooler_output\n",
    "        vec_y = output_b.pooler_output\n",
    "        return SequenceClassifierOutput(loss=None, logits=(vec_x, vec_y))\n",
    "\n",
    "print(\"Custom model class 'SiameseContrastiveModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb330e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from: models/sbert_contrastive_v1/best_model\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 訓練済みモデルとトークナイザのロード ---\n",
    "print(f\"Loading tokenizer from: {MODEL_CHECKPOINT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(f\"Loading trained model from: {TRAINED_MODEL_PATH}\")\n",
    "# 訓練済みの重みをロード\n",
    "model = SiameseContrastiveModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval() # ★ 必須: モデルを「評価モード」に切り替える\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894079d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database generator defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. データベースからのデータ読み込み (ジェネレータ) ---\n",
    "\n",
    "def get_abstract_batches(db_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    DBからアブストラクトをバッチ単位で読み込むジェネレータ\n",
    "    \"\"\"\n",
    "    print(f\"Opening database connection: {db_path}\")\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # 'papers'テーブルの総数を取得 (進捗表示用)\n",
    "        total_rows = cursor.execute(\"SELECT COUNT(doi) FROM papers WHERE abstract IS NOT NULL\").fetchone()[0]\n",
    "        print(f\"Total abstracts to process: {total_rows:,}\")\n",
    "        \n",
    "        cursor.execute(\"SELECT doi, abstract FROM papers WHERE abstract IS NOT NULL\")\n",
    "        \n",
    "        batch = []\n",
    "        for row in tqdm(cursor, total=total_rows, desc=\"Reading Abstracts\"):\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "print(\"Database generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b142d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening database connection: data/processed/s2orc_filtered.db\n",
      "Total abstracts to process: 11,619,136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a0dc3cd4e7476891dcc7ab5dc5a0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Abstracts:   0%|          | 0/11619136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;66;03m# CLSトークン\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# GPUからCPUにデータを戻し、Numpy配列に変換\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     28\u001b[0m         all_dois\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(dois))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 最後に、リストに分割されているNumpy配列を一つの巨大な配列に結合\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 6. 全アブストラクトのベクトル化を実行 ---\n",
    "all_embeddings = []\n",
    "all_dois = []\n",
    "\n",
    "# torch.no_grad() で勾配計算を無効化し、メモリ消費を抑えて高速化\n",
    "with torch.no_grad():\n",
    "    # DBから1000件ずつ論文を読み込む\n",
    "    for batch in get_abstract_batches(DB_PATH, batch_size=1000):\n",
    "        \n",
    "        dois, abstracts = zip(*batch)\n",
    "        \n",
    "        # 1000件のテキストをトークナイズ\n",
    "        inputs = tokenizer(\n",
    "            list(abstracts), \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=MAX_LENGTH, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # --- モデルを使ってベクトル化 ---\n",
    "        # (Siameseモデル本体(bert)を使ってベクトルを計算)\n",
    "        outputs = model.bert(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        embeddings = outputs.pooler_output # CLSトークン\n",
    "        \n",
    "        # GPUからCPUにデータを戻し、Numpy配列に変換\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_dois.extend(list(dois))\n",
    "\n",
    "# 最後に、リストに分割されているNumpy配列を一つの巨大な配列に結合\n",
    "print(\"Concatenating all embeddings...\")\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "print(f\"Embedding generation complete. Shape: {final_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64993645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. 最終的なベクトルDBファイルとDOIマップの保存 ---\n",
    "import json\n",
    "\n",
    "# ベクトルをNumpyファイルとして保存\n",
    "print(f\"Saving embeddings to {EMBEDDINGS_OUTPUT_FILE}...\")\n",
    "np.save(EMBEDDINGS_OUTPUT_FILE, final_embeddings)\n",
    "\n",
    "# DOIとインデックス（Numpy配列の何行目か）の対応表をJSONで保存\n",
    "doi_to_index_map = {doi: i for i, doi in enumerate(all_dois)}\n",
    "\n",
    "print(f\"Saving DOI-to-Index map to {DOI_MAP_OUTPUT_FILE}...\")\n",
    "with open(DOI_MAP_OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(doi_to_index_map, f)\n",
    "\n",
    "print(\"\\n--- Step 1 Complete ---\")\n",
    "print(f\"Total embeddings saved: {len(doi_to_index_map)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

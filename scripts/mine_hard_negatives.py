# scripts/mine_hard_negatives.py

import sys
import os
import hydra
from omegaconf import DictConfig, OmegaConf
import pandas as pd
import numpy as np
import torch
import faiss
import json
import sqlite3
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoConfig
import adapters # Adapterå¯¾å¿œ

# srcã¸ã®ãƒ‘ã‚¹ã‚’é€šã™
sys.path.append(os.getcwd())
from src.utils.cleaning import clean_text
# ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ­ãƒ¼ãƒ‰ãƒ­ã‚¸ãƒƒã‚¯å…±é€šåŒ–ã®ãŸã‚ï¼‰
from src.modeling.bi_encoder import SiameseBiEncoder

def load_resources(cfg, device):
    """ãƒ¢ãƒ‡ãƒ«ã¨Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ (Adapterå¯¾å¿œç‰ˆ)"""
    
    # --- 1. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (encode_corpus.pyã¨åŒæ§˜ã®ãƒ­ã‚¸ãƒƒã‚¯) ---
    print(f"Loading model config from: {cfg.mining.model_name}")
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š (è¨­å®šã«ãªã‘ã‚Œã° SPECTER2 base ã¨ä»®å®š)
    base_model_name = "allenai/specter2_base"
    
    try:
        config = AutoConfig.from_pretrained(cfg.mining.model_name)
    except:
        config = AutoConfig.from_pretrained(base_model_name)

    # æ§‹é€ ã®åˆæœŸåŒ–
    model = SiameseBiEncoder.from_pretrained(base_model_name, config=config)
    
    # Adapteræ§‹é€ ã®åˆæœŸåŒ–
    adapter_name = cfg.mining.get("adapter_name", None)
    if adapter_name:
        print(f"ðŸ”„ Initializing Adapter structure: {adapter_name}")
        adapters.init(model.bert)
        loaded_name = model.bert.load_adapter(adapter_name, source="hf", set_active=True)
        model.bert.set_active_adapters(loaded_name)
        print(f"   Adapter '{loaded_name}' structure initialized.")

    # é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ðŸ“‚ Loading trained state_dict from: {cfg.mining.model_name}")
    state_dict_path = os.path.join(cfg.mining.model_name, "pytorch_model.bin")
    if not os.path.exists(state_dict_path):
        state_dict_path = os.path.join(cfg.mining.model_name, "model.safetensors")
        from safetensors.torch import load_file
        state_dict = load_file(state_dict_path)
    else:
        state_dict = torch.load(state_dict_path, map_location="cpu")
    
    keys = model.load_state_dict(state_dict, strict=False)
    print(f"   Missing keys: {len(keys.missing_keys)}")
    
    model.to(device)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)

    # --- 2. Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ ---
    print(f"Loading Faiss index: {cfg.data.faiss_index_file}")
    index = faiss.read_index(cfg.data.faiss_index_file)
    
    # --- 3. DOIãƒžãƒƒãƒ—ã®ãƒ­ãƒ¼ãƒ‰ (ãƒªã‚¹ãƒˆå¯¾å¿œä¿®æ­£) ---
    print(f"Loading DOI map: {cfg.data.doi_map_file}")
    with open(cfg.data.doi_map_file, 'r') as f:
        doi_data = json.load(f)
    
    # ãƒªã‚¹ãƒˆã®å ´åˆ (encode_corpus.pyã®å‡ºåŠ›) ã¨ è¾žæ›¸ã®å ´åˆ ã‚’åˆ†å²å‡¦ç†
    if isinstance(doi_data, list):
        # ãƒªã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒãã®ã¾ã¾IDã«ãªã‚‹
        print(f"   DOI map is a list of {len(doi_data)} items.")
        id_to_doi = {i: doi for i, doi in enumerate(doi_data)}
    else:
        # è¾žæ›¸ {doi: id} ã®å ´åˆ -> é€†å¼•ã {id: doi} ã«å¤‰æ›
        print(f"   DOI map is a dict of {len(doi_data)} items.")
        id_to_doi = {v: k for k, v in doi_data.items()}
    
    return tokenizer, model, index, id_to_doi

def get_abstracts_from_db(dois, db_path):
    """SQLiteã‹ã‚‰DOIã«å¯¾å¿œã™ã‚‹ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ä¸€æ‹¬å–å¾—"""
    doi_to_text = {}
    
    with sqlite3.connect(db_path) as conn:
        chunk_size = 900 
        unique_dois = list(set(dois))
        
        for i in tqdm(range(0, len(unique_dois), chunk_size), desc="Querying DB"):
            chunk = unique_dois[i : i + chunk_size]
            placeholders = ','.join('?' for _ in chunk)
            query = f"SELECT doi, abstract FROM papers WHERE doi IN ({placeholders})"
            
            try:
                cursor = conn.execute(query, chunk)
                for row in cursor:
                    doi_to_text[row[0]] = row[1]
            except sqlite3.Error as e:
                print(f"Database error: {e}")
                
    return doi_to_text

@hydra.main(config_path="../configs", config_name="mining", version_base=None)
def main(cfg: DictConfig):
    print("=== Starting Hard Negative Mining (Fixed) ===")
    print(OmegaConf.to_yaml(cfg))
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print(f"Loading training data: {cfg.data.input_train_file}")
    df_train = pd.read_csv(cfg.data.input_train_file)
    
    # æ­£ä¾‹ãƒšã‚¢ã®ã¿ã‚’æŠ½å‡º
    df_positives = df_train[df_train['label'] == 1].copy().reset_index(drop=True)
    print(f"Loaded {len(df_positives):,} positive pairs.")

    # ã‚¢ãƒ³ã‚«ãƒ¼ -> æ­£è§£(Positive) ã®ãƒžãƒƒãƒ”ãƒ³ã‚° (é™¤å¤–ç”¨)
    anchor_to_positives = df_positives.groupby('abstract_a')['abstract_b'].apply(set).to_dict()
    unique_anchors = list(anchor_to_positives.keys())
    print(f"Unique anchors to query: {len(unique_anchors):,}")

    # 2. ãƒªã‚½ãƒ¼ã‚¹ã®ãƒ­ãƒ¼ãƒ‰
    tokenizer, model, index, id_to_doi = load_resources(cfg, device)

    # 3. æ¤œç´¢å®Ÿè¡Œ
    anchor_to_candidates = {}
    batch_size = cfg.mining.batch_size
    
    print("Starting dense retrieval...")
    for i in tqdm(range(0, len(unique_anchors), batch_size), desc="Mining"):
        batch_anchors = unique_anchors[i : i + batch_size]
        
        inputs = tokenizer(
            batch_anchors, padding=True, truncation=True, 
            max_length=cfg.mining.max_length, return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            # SiameseBiEncoderã‚’ä½¿ã†ãŸã‚ã€pooler_outputã§ã¯ãªãoutput_vectors=Trueã®çµæžœã‚’ä½¿ã†
            outputs = model(
                input_ids=inputs['input_ids'], 
                attention_mask=inputs['attention_mask'],
                output_vectors=True
            )
            embeddings = outputs.logits.float().cpu().numpy()
        
        # Faissæ¤œç´¢
        distances, indices = index.search(embeddings, cfg.mining.search_top_k)
        
        for j, anchor_text in enumerate(batch_anchors):
            result_ids = indices[j]
            # IDãŒæœ‰åŠ¹ç¯„å›²å†…ã‹ã¤ãƒžãƒƒãƒ—ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            candidate_dois = [id_to_doi[rid] for rid in result_ids if rid != -1 and rid in id_to_doi]
            anchor_to_candidates[anchor_text] = candidate_dois

    # 4. ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    print("Fetching texts for candidates...")
    all_candidate_dois = []
    for dois in anchor_to_candidates.values():
        all_candidate_dois.extend(dois)
    
    doi_to_text_map = get_abstracts_from_db(all_candidate_dois, cfg.data.db_path)
    print(f"Retrieved {len(doi_to_text_map):,} abstracts from DB.")

    # 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
    final_rows = []
    neg_ratio = cfg.mining.neg_ratio
    min_len = cfg.mining.min_text_length
    
    print(f"Constructing final dataset (Ratio 1:{neg_ratio})...")
    
    for _, row in tqdm(df_positives.iterrows(), total=len(df_positives), desc="Building Pairs"):
        anchor = row['abstract_a']
        positive = row['abstract_b']
        data_paper_doi = row.get('data_paper_doi', None)
        
        # æ­£ä¾‹
        final_rows.append({
            'abstract_a': anchor,
            'abstract_b': positive,
            'label': 1,
            'data_paper_doi': data_paper_doi
        })
        
        # è² ä¾‹
        candidates = anchor_to_candidates.get(anchor, [])
        true_positives = anchor_to_positives.get(anchor, set())
        
        added_count = 0
        for neg_doi in candidates:
            if added_count >= neg_ratio:
                break
            
            raw_text = doi_to_text_map.get(neg_doi, "")
            if not raw_text:
                continue
                
            neg_text = clean_text(raw_text)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if len(neg_text) < min_len: continue
            if neg_text == anchor: continue
            if neg_text in true_positives: continue # False Negativeå›žé¿
            
            final_rows.append({
                'abstract_a': anchor,
                'abstract_b': neg_text,
                'label': 0,
                'data_paper_doi': None
            })
            added_count += 1
    
    # 6. ä¿å­˜
    df_final = pd.DataFrame(final_rows)
    print(f"Dataset created. Shape: {df_final.shape}")
    print("Label distribution:")
    print(df_final['label'].value_counts())
    
    output_dir = os.path.dirname(cfg.data.output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    df_final.to_csv(cfg.data.output_file, index=False)
    print(f"Saved to: {cfg.data.output_file}")

if __name__ == "__main__":
    main()
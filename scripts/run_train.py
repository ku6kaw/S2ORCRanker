# scripts/run_train.py

import sys
import os
import hydra
from omegaconf import DictConfig, OmegaConf
import torch
from transformers import AutoTokenizer, TrainingArguments, AutoConfig, EarlyStoppingCallback
from torch.optim import AdamW
import wandb
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import adapters

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.getcwd())

from src.modeling.bi_encoder import SiameseBiEncoder
from src.modeling.cross_encoder import CrossEncoderMarginModel
from src.training.dataset import TextRankingDataset
from src.training.trainer import BiEncoderPairTrainer, MarginRankingTrainer, ContrastiveTrainer, MultipleNegativesRankingTrainer

def create_optimizer_grouped_parameters(model, base_lr, head_lr, weight_decay):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’4ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã€å­¦ç¿’ç‡ã¨Weight Decayã‚’é©ç”¨ã™ã‚‹ã€‚
    1. Head (Decayã‚ã‚Š)
    2. Head (Decayãªã—)
    3. Base (Decayã‚ã‚Š)
    4. Base (Decayãªã—)
    """
    # Weight Decayã‚’é©ç”¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
    no_decay = ["bias", "LayerNorm.weight"]
    
    # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¨ã¿ãªã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    # Bi-Encoder: "classifier_head"
    # Cross-Encoder (Longformer): "scorer.classifier" ç­‰
    head_keywords = ["classifier", "head", "score"]

    optimizer_grouped_parameters = []
    
    # å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èµ°æŸ»ã—ã¦æŒ¯ã‚Šåˆ†ã‘
    head_params_decay = []
    head_params_no_decay = []
    base_params_decay = []
    base_params_no_decay = []

    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        
        # ãƒ˜ãƒƒãƒ‰ã‹ã©ã†ã‹åˆ¤å®š
        is_head = any(k in name for k in head_keywords)
        # Decayãªã—ã‹ã©ã†ã‹åˆ¤å®š
        is_no_decay = any(nd in name for nd in no_decay)

        if is_head:
            if is_no_decay:
                head_params_no_decay.append(param)
            else:
                head_params_decay.append(param)
        else:
            if is_no_decay:
                base_params_no_decay.append(param)
            else:
                base_params_decay.append(param)

    # ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©ã®ä½œæˆ
    if head_params_decay:
        optimizer_grouped_parameters.append({
            "params": head_params_decay,
            "weight_decay": weight_decay,
            "lr": head_lr,
            "name": "head_decay"
        })
    if head_params_no_decay:
        optimizer_grouped_parameters.append({
            "params": head_params_no_decay,
            "weight_decay": 0.0,
            "lr": head_lr,
            "name": "head_no_decay"
        })
    if base_params_decay:
        optimizer_grouped_parameters.append({
            "params": base_params_decay,
            "weight_decay": weight_decay,
            "lr": base_lr,
            "name": "base_decay"
        })
    if base_params_no_decay:
        optimizer_grouped_parameters.append({
            "params": base_params_no_decay,
            "weight_decay": 0.0,
            "lr": base_lr,
            "name": "base_no_decay"
        })

    return optimizer_grouped_parameters

def compute_metrics(eval_pred):
    """
    Bi-Encoder (BCE Loss) ç”¨ã®è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
    """
    predictions, labels = eval_pred
    # predictionsã¯ãƒ­ã‚¸ãƒƒãƒˆ(ã‚¹ã‚³ã‚¢)ãªã®ã§ã€0ã‚’é–¾å€¤ã¨ã—ã¦0/1ã«å¤‰æ›
    # (Sigmoidã‚’é€šã™ã¨ 0.5 ãŒé–¾å€¤ã«ãªã‚‹ã®ã¨åŒç¾©)
    preds = (predictions > 0).astype(int).reshape(-1)
    labels = labels.astype(int).reshape(-1)

    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


    """
    Contrastive Loss (è·é›¢å­¦ç¿’) ç”¨ã®è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã€‚
    è·é›¢ãŒ threshold ä»¥ä¸‹ãªã‚‰ã€Œé¡ä¼¼ï¼ˆPositiveï¼‰ã€ã¨åˆ¤å®šã™ã‚‹ã€‚
    """
    # ContrastiveTrainerã¯ (vec_a, vec_b) ã‚’è¿”ã™ã®ã§ã€ã“ã‚Œã‚’å—ã‘å–ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
    # Hugging Face Trainerã® compute_metrics ã¯ (predictions, label_ids) ã‚’å—ã‘å–ã‚‹ä»•æ§˜ã€‚
    # predictions ã¯ logits ã®ã‚¿ãƒ—ãƒ«ã«ãªã‚‹ã¯ãšã€‚
    
    logits, labels = eval_pred
    # logits ã¯ (vec_a, vec_b) ã®ã‚¿ãƒ—ãƒ«
    vec_a = torch.tensor(logits[0])
    vec_b = torch.tensor(logits[1])
    
    # è·é›¢è¨ˆç®— (Euclidean)
    distances = torch.nn.functional.pairwise_distance(vec_a, vec_b).numpy()
    
    # äºˆæ¸¬: è·é›¢ãŒé–¾å€¤ä»¥ä¸‹ãªã‚‰ 1 (Positive), ãã‚Œä»¥å¤–ã¯ 0 (Negative)
    # â€» Contrastiveã§ã¯ã€Œè·é›¢ãŒè¿‘ã„ã»ã©ä¼¼ã¦ã„ã‚‹ã€ãŸã‚
    preds = (distances < threshold).astype(int)
    labels = labels.astype(int).reshape(-1)

    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'mean_distance': float(distances.mean()) # å¹³å‡è·é›¢ã‚‚ãƒ­ã‚°ã«æ®‹ã™ã¨ä¾¿åˆ©
    }
    
@hydra.main(config_path="../configs", config_name="train", version_base=None)
def main(cfg: DictConfig):
    print(f"=== Starting Training: {cfg.model.type} ===")
    print(OmegaConf.to_yaml(cfg))
    
    if cfg.logging.use_wandb:
        wandb.init(
            project=cfg.logging.project_name,
            name=cfg.logging.run_name,
            tags=cfg.logging.tags,
            config=OmegaConf.to_container(cfg, resolve=True)
        )
    
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)
    dataset_handler = TextRankingDataset(cfg, tokenizer)
    tokenized_datasets = dataset_handler.load_and_prepare()
    
    print(f"Loading model: {cfg.model.name}")
    model_config = AutoConfig.from_pretrained(cfg.model.name)
    
    # --- â˜… Trainerã¨ãƒ¢ãƒ‡ãƒ«ã®é¸æŠãƒ­ã‚¸ãƒƒã‚¯ (ä¿®æ­£) ---
    compute_metrics_func = None

    if cfg.model.type == "cross_encoder":
        # Cross-Encoderã®å ´åˆ
        model_config.num_labels = 1
        model = CrossEncoderMarginModel.from_pretrained(cfg.model.name, config=model_config)
        trainer_cls = MarginRankingTrainer
        
    else: # bi_encoder
        head_type = cfg.model.get("head_type", "ranknet")
        # â˜…è¿½åŠ : æå¤±é–¢æ•°ã®ã‚¿ã‚¤ãƒ—è¨­å®š (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯äº’æ›æ€§ã®ãŸã‚ pair_score)
        loss_type = cfg.training.get("loss_type", "pair_score")
        
        print(f"Bi-Encoder Head: {head_type}, Loss: {loss_type}")
        
        model = SiameseBiEncoder.from_pretrained(
            cfg.model.name, 
            config=model_config, 
            head_type=head_type
        )
        
        adapter_name = cfg.model.get("adapter_name", None)
        if adapter_name:
            print(f"ğŸ”„ Loading Adapter: {adapter_name}")
            
            # adaptersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§åˆæœŸåŒ–
            adapters.init(model.bert)
            
            # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®å†…éƒ¨å(ä¾‹: '[PRX]')ã‚’å–å¾—
            loaded_name = model.bert.load_adapter(adapter_name, source="hf", set_active=True)
            
            # å¿µã®ãŸã‚æ˜ç¤ºçš„ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
            model.bert.set_active_adapters(loaded_name)
            print(f"âœ… Adapter '{loaded_name}' activated.")
            
            if cfg.model.get("freeze_base", False):
                print("â„ï¸  Freezing base model parameters (Training Adapter only)")
                # å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€æ—¦å‡çµ
                for param in model.bert.parameters():
                    param.requires_grad = False
                
                # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼éƒ¨åˆ†ã¨åˆ†é¡ãƒ˜ãƒƒãƒ‰(ã‚‚ã—ã‚ã‚Œã°)ã®ã¿è§£å‡
                for name, param in model.named_parameters():
                    if "adapter" in name or "classifier_head" in name:
                        param.requires_grad = True
                        
                # ç¢ºèª: å­¦ç¿’å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                all_params = sum(p.numel() for p in model.parameters())
                print(f"   Trainable params: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)")
        
        if loss_type == "mnrl":
            # Multiple Negatives Ranking Loss
            print("Using MultipleNegativesRankingTrainer (Batch Negatives)")
            trainer_cls = MultipleNegativesRankingTrainer
        
        elif head_type == "none":
            # Contrastive
            print("Using ContrastiveTrainer (Distance-based)")
            trainer_cls = ContrastiveTrainer
        else:
            # RankNet / BCE
            print("Using BiEncoderPairTrainer (Classification Head)")
            trainer_cls = BiEncoderPairTrainer
            compute_metrics_func = compute_metrics

    # 4. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ä½œæˆ
    base_lr = cfg.training.learning_rate
    head_lr = cfg.training.get("head_learning_rate", base_lr)
    
    optimizer_grouped_parameters = create_optimizer_grouped_parameters(
        model, base_lr, head_lr, cfg.training.weight_decay
    )
    optimizer = AdamW(optimizer_grouped_parameters)

    # 5. Arguments
    output_dir = cfg.training.output_dir
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=cfg.training.batch_size,
        per_device_eval_batch_size=cfg.training.batch_size,
        num_train_epochs=cfg.training.epochs,
        learning_rate=base_lr, 
        weight_decay=cfg.training.weight_decay,
        warmup_ratio=cfg.training.warmup_ratio,
        logging_strategy="steps",
        logging_steps=cfg.training.logging_steps,
        eval_strategy="steps",
        eval_steps=cfg.training.eval_steps,
        save_strategy="steps",
        save_steps=cfg.training.save_steps,
        save_total_limit=cfg.training.save_total_limit,
        load_best_model_at_end=True,
        report_to="wandb" if cfg.logging.use_wandb else "none",
        run_name=cfg.logging.run_name, 
        remove_unused_columns=False,
        metric_for_best_model="loss",
        greater_is_better=False,
    )
    
    # â˜… ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®æº–å‚™
    callbacks = []
    if cfg.training.get("patience"):
        print(f"Early stopping enabled with patience: {cfg.training.patience}")
        callbacks.append(EarlyStoppingCallback(early_stopping_patience=cfg.training.patience))
        
    # 6. TraineråˆæœŸåŒ–
    # å…±é€šã®å¼•æ•°
    trainer_kwargs = {
        "model": model,
        "args": args,
        "train_dataset": tokenized_datasets["train"],
        "eval_dataset": tokenized_datasets["validation"],
        "tokenizer": tokenizer,
        "optimizers": (optimizer, None),
        "compute_metrics": compute_metrics_func,
        "callbacks": callbacks
    }

    # Trainerã‚¯ãƒ©ã‚¹ã«å¿œã˜ãŸè¿½åŠ å¼•æ•°
    if trainer_cls == MultipleNegativesRankingTrainer:
        trainer_kwargs["scale"] = cfg.training.get("scale", 20.0)
    elif trainer_cls in [ContrastiveTrainer, MarginRankingTrainer]:
        trainer_kwargs["margin"] = cfg.training.margin
    
    # TraineråˆæœŸåŒ–
    trainer = trainer_cls(**trainer_kwargs)
    
    print("Starting training...")
    trainer.train()
    
    best_model_path = os.path.join(output_dir, "best_model")
    print(f"Saving best model to {best_model_path}")
    trainer.save_model(best_model_path)
    
    if cfg.logging.use_wandb:
        wandb.finish()

if __name__ == "__main__":
    main()
# scripts/encode_corpus.py (ãƒ¡ãƒ¢ãƒªå®‰å…¨ãƒ»é«˜é€ŸåŒ–ç‰ˆ)

import sys
import os
import hydra
from omegaconf import DictConfig, OmegaConf
import torch
import sqlite3
import numpy as np
import json
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoConfig

sys.path.append(os.getcwd())
from src.modeling.bi_encoder import SiameseBiEncoder
from src.utils.cleaning import clean_text

def get_total_count(db_path):
    """DBå†…ã®è«–æ–‡ç·æ•°ã‚’å–å¾—"""
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(doi) FROM papers")
        try:
            count = cursor.fetchone()[0]
        except TypeError:
            count = 0
    return count

def fetch_data_generator(db_path, batch_size, debug=False):
    """
    DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒã‚µã‚¤ã‚ºã”ã¨ã«ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒˆã™ã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„å‹ï¼‰ã€‚
    """
    limit_clause = " LIMIT 5000" if debug else ""
    
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        # ã‚«ãƒ¼ã‚½ãƒ«ã§å°‘ã—ãšã¤èª­ã¿è¾¼ã‚€
        cursor.execute(f"SELECT doi, abstract FROM papers WHERE abstract IS NOT NULL AND length(abstract) > 10{limit_clause}")
        
        batch_dois = []
        batch_texts = []
        
        while True:
            # fetchmanyã§ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã ã‘å–å¾—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡è‰¯ï¼‰
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break
                
            for row in rows:
                doi, abstract = row
                cleaned = clean_text(abstract)
                if cleaned:
                    batch_dois.append(doi)
                    batch_texts.append(cleaned)
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§æ¸›ã£ãŸåˆ†ã€batch_sizeã«æº€ãŸãªã„å ´åˆãŒã‚ã‚‹ãŒãã®ã¾ã¾é€ã‚‹
            if batch_dois:
                yield batch_dois, batch_texts
                batch_dois = []
                batch_texts = []

@hydra.main(config_path="../configs", config_name="evaluate", version_base=None)
def main(cfg: DictConfig):
    print("=== Starting Memory-Safe Corpus Encoding ===")
    print(OmegaConf.to_yaml(cfg))
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # FP16 (Mixed Precision) ã®æœ‰åŠ¹åŒ–
    use_fp16 = torch.cuda.is_available()
    if use_fp16:
        print("ğŸš€ FP16 (Mixed Precision) Enabled for speedup.")

    if not os.path.exists(cfg.data.output_dir):
        os.makedirs(cfg.data.output_dir)
    
    embeddings_path = os.path.join(cfg.data.output_dir, cfg.data.embeddings_file)
    doi_map_path = os.path.join(cfg.data.output_dir, cfg.data.doi_map_file)

    # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print(f"Loading model from: {cfg.model.path}")
    config = AutoConfig.from_pretrained(cfg.model.path)
    model = SiameseBiEncoder.from_pretrained(cfg.model.path, config=config)
    model.to(device)
    model.eval()
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(cfg.model.path)
    except:
        tokenizer = AutoTokenizer.from_pretrained(cfg.model.base_name)

    # 2. ç·æ•°ã‚«ã‚¦ãƒ³ãƒˆ & Memmapæº–å‚™
    total_papers = get_total_count(cfg.data.db_path)
    if cfg.get("debug", False):
        total_papers = 5000
        
    print(f"Total papers (estimate): {total_papers:,}")
    hidden_size = config.hidden_size 
    
    print(f"Creating memmap file at {embeddings_path}...")
    all_embeddings = np.memmap(
        embeddings_path, 
        dtype='float32', 
        mode='w+', 
        shape=(total_papers, hidden_size)
    )

    # 3. æ¨è«–ãƒ«ãƒ¼ãƒ—
    batch_size = cfg.model.batch_size
    # ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å…¨ï¼‰
    data_gen = fetch_data_generator(cfg.data.db_path, batch_size, debug=cfg.get("debug", False))
    
    doi_list = [] 
    current_idx = 0
    
    # æ¨è«–
    with torch.no_grad():
        # tqdmã®ãƒˆãƒ¼ã‚¿ãƒ«ã¯æ¦‚ç®—
        pbar = tqdm(data_gen, total=(total_papers // batch_size) + 1, desc="Encoding")
        
        for batch_dois, batch_texts in pbar:
            if not batch_texts:
                continue

            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = tokenizer(
                batch_texts, 
                padding=True, 
                truncation=True, 
                max_length=cfg.model.max_length, 
                return_tensors="pt"
            ).to(device)
            
            # FP16 æ¨è«– (é«˜é€ŸåŒ–ã®è‚)
            with torch.amp.autocast('cuda', enabled=use_fp16):
                outputs = model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    output_vectors=True
                )
                # float32ã«æˆ»ã—ã¦CPUã¸
                embeddings = outputs.logits.float().cpu().numpy()
            
            # æ›¸ãè¾¼ã¿
            n_samples = len(embeddings)
            
            # memmapã®ã‚µã‚¤ã‚ºè¶…éãƒã‚§ãƒƒã‚¯ï¼ˆæ¨å®šã‚ˆã‚Šå¤šã‹ã£ãŸå ´åˆï¼‰
            if current_idx + n_samples > total_papers:
                # å¿…è¦ãªã‚‰ãƒªã‚µã‚¤ã‚ºç­‰ã®å‡¦ç†ã‚‚å¯èƒ½ã ãŒã€ä»Šå›ã¯åˆ‡ã‚Šæ¨ã¦ã‚‹ã‹ã€
                # ã¾ãŸã¯ä½™è£•ã‚’æŒã£ã¦ç¢ºä¿ã—ã¦ãŠãè¨­è¨ˆã«ã™ã‚‹ã€‚
                # ç°¡æ˜“çš„ã«ã“ã“ã§æ‰“ã¡åˆ‡ã‚‹ï¼ˆç·æ•°ã¯get_total_countã§æ­£ç¢ºãªã¯ãšã ãŒã€debugæ™‚ã¯æ³¨æ„ï¼‰
                if cfg.get("debug", False):
                    break
                
                # å®Ÿé‹ç”¨ã§ã‚µã‚¤ã‚ºä¸è¶³ãŒèµ·ããŸå ´åˆã®ç·Šæ€¥å›é¿ï¼ˆã¯ã¿å‡ºã—ãŸåˆ†ã¯ç„¡è¦–ï¼‰
                n_samples = total_papers - current_idx
                embeddings = embeddings[:n_samples]
                batch_dois = batch_dois[:n_samples]

            all_embeddings[current_idx : current_idx + n_samples] = embeddings
            doi_list.extend(batch_dois)
            current_idx += n_samples
            
            if current_idx >= total_papers:
                break

    print(f"Encoding complete. Valid vectors: {current_idx:,}")

    # 4. DOIãƒãƒƒãƒ—ä¿å­˜
    print(f"Saving DOI map to {doi_map_path}...")
    with open(doi_map_path, 'w') as f:
        json.dump(doi_list, f)

    # ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    all_embeddings.flush()
    print("Done.")

if __name__ == "__main__":
    main()
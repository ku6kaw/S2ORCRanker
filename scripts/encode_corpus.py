# scripts/encode_corpus.py

import sys
import os
import hydra
from omegaconf import DictConfig, OmegaConf
import torch
import sqlite3
import numpy as np
import json
import math
from tqdm.auto import tqdm
from torch.utils.data import IterableDataset, DataLoader
from transformers import AutoTokenizer, AutoConfig

sys.path.append(os.getcwd())
from src.modeling.bi_encoder import SiameseBiEncoder
from src.utils.cleaning import clean_text

def get_db_stats(db_path):
    """DBã®è¡Œæ•°ã¨æœ€å¤§rowidã‚’å–å¾—"""
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        # é«˜é€ŸãªCount (*)
        cursor.execute("SELECT MAX(rowid), COUNT(*) FROM papers")
        max_id, count = cursor.fetchone()
    return max_id, count

class SQLiteDataset(IterableDataset):
    def __init__(self, db_path, max_rowid, debug=False):
        self.db_path = db_path
        self.max_rowid = max_rowid
        self.debug = debug

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        
        # æ‹…å½“ç¯„å›²ã®æ±ºå®šï¼ˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        if worker_info is None:
            # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹
            start = 0
            end = self.max_rowid
        else:
            # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹: å…¨ä½“ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã§åˆ†å‰²
            per_worker = int(math.ceil((self.max_rowid + 1) / worker_info.num_workers))
            worker_id = worker_info.id
            start = worker_id * per_worker
            end = min(start + per_worker, self.max_rowid + 1)

        # ãƒ‡ãƒãƒƒã‚°æ™‚ã¯ç¯„å›²ã‚’æ¥µå°ã«
        if self.debug:
            end = min(start + 1000, end)

        # DBæ¥ç¶šã¨èª­ã¿è¾¼ã¿
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # rowidã‚’ä½¿ã£ã¦ç¯„å›²æŒ‡å®šèª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰
            query = f"""
                SELECT doi, abstract 
                FROM papers 
                WHERE rowid >= ? AND rowid < ? AND abstract IS NOT NULL AND length(abstract) > 10
            """
            cursor.execute(query, (start, end))
            
            while True:
                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ãªãã€ã‚ã‚‹ç¨‹åº¦ã¾ã¨ã‚ã¦fetchã—ã¦Pythonå´ã§yieldã™ã‚‹
                rows = cursor.fetchmany(1000)
                if not rows:
                    break
                
                for doi, abstract in rows:
                    cleaned_text = clean_text(abstract)
                    if cleaned_text:
                        yield doi, cleaned_text

class CollateFn:
    """ãƒãƒƒãƒåŒ–ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§è¡Œã†ãŸã‚ã®Collateé–¢æ•°"""
    def __init__(self, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __call__(self, batch):
        # batch ã¯ [(doi, text), (doi, text), ...] ã®ãƒªã‚¹ãƒˆ
        dois = [item[0] for item in batch]
        texts = [item[1] for item in batch]
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        return dois, inputs

@hydra.main(config_path="../configs", config_name="evaluate", version_base=None)
def main(cfg: DictConfig):
    print("=== Starting Optimized Corpus Encoding ===")
    print(OmegaConf.to_yaml(cfg))
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    use_fp16 = torch.cuda.is_available()
    if use_fp16:
        print("ğŸš€ FP16 (Mixed Precision) Enabled.")

    if not os.path.exists(cfg.data.output_dir):
        os.makedirs(cfg.data.output_dir)
    
    embeddings_path = os.path.join(cfg.data.output_dir, cfg.data.embeddings_file)
    doi_map_path = os.path.join(cfg.data.output_dir, cfg.data.doi_map_file)

    # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print(f"Loading model from: {cfg.model.path}")
    config = AutoConfig.from_pretrained(cfg.model.path)
    model = SiameseBiEncoder.from_pretrained(cfg.model.path, config=config)
    model.to(device)
    model.eval()
    
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ï¼ˆPyTorch 2.0+ï¼‰
    if hasattr(torch, "compile"):
        try:
            print("Compiling model...")
            model = torch.compile(model)
        except:
            pass

    try:
        tokenizer = AutoTokenizer.from_pretrained(cfg.model.path)
    except:
        tokenizer = AutoTokenizer.from_pretrained(cfg.model.base_name)

    # 2. DBçµ±è¨ˆå–å¾—ã¨Datasetæº–å‚™
    print("Analyzing Database...")
    max_rowid, total_count = get_db_stats(cfg.data.db_path)
    print(f"Max RowID: {max_rowid}, Total Count: {total_count:,}")
    
    dataset = SQLiteDataset(cfg.data.db_path, max_rowid, debug=cfg.get("debug", False))
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®š
    batch_size = cfg.model.batch_size
    num_workers = 4 # CPUã‚³ã‚¢æ•°ã«å¿œã˜ã¦èª¿æ•´ï¼ˆ4ã€œ8æ¨å¥¨ï¼‰
    
    collate_fn = CollateFn(tokenizer, cfg.model.max_length)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        prefetch_factor=2 # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå…ˆèª­ã¿ã™ã‚‹ãƒãƒƒãƒæ•°
    )

    # 3. Memmapæº–å‚™
    # ãƒ‡ãƒãƒƒã‚°æ™‚ã¯ã‚µã‚¤ã‚ºãŒå°ã•ã„ã®ã§èª¿æ•´
    output_shape = (total_count, config.hidden_size) if not cfg.get("debug", False) else (num_workers * 1000, config.hidden_size)
    
    print(f"Creating memmap file at {embeddings_path}...")
    all_embeddings = np.memmap(
        embeddings_path, 
        dtype='float32', 
        mode='w+', 
        shape=output_shape
    )

    # 4. æ¨è«–ãƒ«ãƒ¼ãƒ—
    doi_list = []
    current_idx = 0
    
    print("Starting inference...")
    # tqdmã®totalã¯æ¦‚ç®—ï¼ˆtotal_count / batch_sizeï¼‰
    total_batches = (output_shape[0] // batch_size) + 1
    
    with torch.no_grad():
        for batch_dois, batch_inputs in tqdm(dataloader, total=total_batches, desc="Encoding"):
            # GPUè»¢é€ (non_blocking=Trueã§é«˜é€ŸåŒ–)
            input_ids = batch_inputs['input_ids'].to(device, non_blocking=True)
            attention_mask = batch_inputs['attention_mask'].to(device, non_blocking=True)
            
            # FP16æ¨è«–
            with torch.amp.autocast('cuda', enabled=use_fp16):
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_vectors=True
                )
                # float32ã«æˆ»ã—ã¦CPUã¸
                embeddings = outputs.logits.float().cpu().numpy()
            
            n_samples = len(embeddings)
            
            # å®¹é‡ãƒã‚§ãƒƒã‚¯
            if current_idx + n_samples > output_shape[0]:
                 # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã®æ‹¡å¼µã¯ã§ããªã„ã®ã§ã€ã¯ã¿å‡ºã—ãŸåˆ†ã¯åˆ‡ã‚Šæ¨ã¦ã‚‹ã‹ã€
                 # æœ¬æ¥ã¯ã‚‚ã£ã¨å¤§ããç¢ºä¿ã—ã¦ãŠãã¹ãã€‚ã“ã“ã§ã¯å®‰å…¨ã«break
                 break
            
            all_embeddings[current_idx : current_idx + n_samples] = embeddings
            doi_list.extend(batch_dois)
            current_idx += n_samples

    print(f"Encoding complete. Valid vectors: {current_idx:,}")

    # 5. DOIãƒãƒƒãƒ—ä¿å­˜
    print(f"Saving DOI map to {doi_map_path}...")
    with open(doi_map_path, 'w') as f:
        json.dump(doi_list, f)
        
    # 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆå®Ÿéš›ã®ä»¶æ•°ã‚’è¨˜éŒ²ï¼‰
    # å¿…è¦ãªã‚‰memmapã‚’ãƒªã‚µã‚¤ã‚ºã™ã‚‹å‡¦ç†ã‚’ã“ã“ã«å…¥ã‚Œã¦ã‚‚è‰¯ã„ãŒã€
    # DOIãƒªã‚¹ãƒˆã®é•·ã•ã¨current_idxãŒä¸€è‡´ã—ã¦ã„ã‚Œã°ã€èª­ã¿è¾¼ã¿æ™‚ã«åˆ¶å¾¡å¯èƒ½ã€‚

    all_embeddings.flush()
    print("Done.")

if __name__ == "__main__":
    main()
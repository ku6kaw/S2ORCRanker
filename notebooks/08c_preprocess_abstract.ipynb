{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c53fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings and cleaning patterns defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "INPUT_FILE = \"data/processed/training_dataset_abstract_cleaned.csv\" # 元のファイル\n",
    "OUTPUT_FILE = \"data/processed/training_dataset_abstract_cleaned_v2.csv\" # 新しい出力ファイル名\n",
    "\n",
    "# アブストラクトの終わり（＝本文の始まり）を示す可能性が非常に高いキーワード\n",
    "# （大文字と小文字を区別しないように、すべて小文字で定義）\n",
    "STOP_WORDS = [\n",
    "    'introduction',\n",
    "    'keywords',\n",
    "    'key words',\n",
    "    'references',\n",
    "    'acknowledgments',\n",
    "    'acknowledgements',\n",
    "    'bibliography',\n",
    "    'pubmed abstract',\n",
    "    'publisher full text',\n",
    "    'full text'\n",
    "]\n",
    "\n",
    "# 正規表現パターンを作成 ( | で区切る)\n",
    "# \\b は単語の境界を意味し、\"introduction\" が \"subintroduction\" にマッチしないようにする\n",
    "STOP_PATTERN = re.compile(r'\\b(' + '|'.join(STOP_WORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# URLを検出する正規表現 (簡略版)\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "# メールアドレスを検出する正規表現\n",
    "EMAIL_PATTERN = re.compile(r'\\S*@\\S*\\s?')\n",
    "\n",
    "# 非ASCII文字を検出する正規表現 (ASCII文字以外すべて)\n",
    "NON_ASCII_PATTERN = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "# 特殊記号を検出する正規表現 (句読点と一般的な記号は残し、それ以外を削除)\n",
    "# 今回は極端な特殊記号（例：絵文字、一部の数学記号など）に限定し、\n",
    "# 句読点（,.!?\"'）やハイフン、括弧などは残す方針で\n",
    "SPECIAL_CHARS_PATTERN = re.compile(r'[^\\w\\s\\.\\,\\!\\?\\-\\'\\(\\)\\[\\]\\{\\}\\<\\>\\/\\=\\+\\*\\%]') # 基本的な記号は残す\n",
    "# もし、より厳密にアルファベットと数字、基本的な句読点のみにしたい場合は以下\n",
    "# SPECIAL_CHARS_PATTERN = re.compile(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\-:\\'\\(\\)\\{\\}\\[\\]]')\n",
    "\n",
    "\n",
    "print(\"Settings and cleaning patterns defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2f7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is abstract. See more at https://example.com. Contact us at user@domain.com. We found α=0.05. Some strange char: ™, ©, 『Test』. This is body...\n",
      "Cleaned:  This is abstract. See more at Contact us at We found =0.05. Some strange char , , Test. This is body...\n",
      "\n",
      "Original (no body): This is a clean abstract. No body here. 123.-_()[]{}<>/?=+%*\n",
      "Cleaned (no body):  This is a clean abstract. No body here. 123.-_()[]{}<>/?=+%*\n"
     ]
    }
   ],
   "source": [
    "# --- 2. クリーニング関数の定義 ---\n",
    "\n",
    "def comprehensive_clean_abstract(text):\n",
    "    \"\"\"\n",
    "    テキストを受け取り、以下のクリーニングを行う\n",
    "    1. 末尾の本文ノイズの切り捨て\n",
    "    2. URLの削除\n",
    "    3. メールアドレスの削除\n",
    "    4. 非ASCII文字の削除\n",
    "    5. 特殊記号の削除 (一部残す)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 1. 末尾の本文ノイズの切り捨て (STOP_PATTERNを使用)\n",
    "    match = STOP_PATTERN.search(text)\n",
    "    if match:\n",
    "        text = text[:match.start()]\n",
    "    \n",
    "    # 2. URLの削除\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    \n",
    "    # 3. メールアドレスの削除\n",
    "    text = EMAIL_PATTERN.sub('', text)\n",
    "\n",
    "    # 4. 非ASCII文字の削除\n",
    "    text = NON_ASCII_PATTERN.sub('', text)\n",
    "    \n",
    "    # 5. 特殊記号の削除 (一般的な句読点や記号は残す)\n",
    "    text = SPECIAL_CHARS_PATTERN.sub('', text)\n",
    "    \n",
    "    # 連続する空白文字を1つにまとめる\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# テスト実行\n",
    "test_text = \"This is abstract. See more at https://example.com. Contact us at user@domain.com. We found α=0.05. Some strange char: ™, ©, 『Test』. This is body...\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Cleaned:  {comprehensive_clean_abstract(test_text)}\")\n",
    "\n",
    "test_text_no_body = \"This is a clean abstract. No body here. 123.-_()[]{}<>/?=+%*\"\n",
    "print(f\"\\nOriginal (no body): {test_text_no_body}\")\n",
    "print(f\"Cleaned (no body):  {comprehensive_clean_abstract(test_text_no_body)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb67aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from data/processed/training_dataset_abstract_cleaned.csv...\n",
      "Applying comprehensive cleaning function to 'abstract_a'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84be258c1b4d70ac2d28da67749163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying comprehensive cleaning function to 'abstract_b'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9b5c8c57d24abd935e2cdb678a3a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. データセットの読み込みと前処理の実行 ---\n",
    "print(f\"Loading dataset from {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b'])\n",
    "\n",
    "print(\"Applying comprehensive cleaning function to 'abstract_a'...\")\n",
    "df['abstract_a_cleaned'] = df['abstract_a'].progress_apply(comprehensive_clean_abstract)\n",
    "\n",
    "print(\"Applying comprehensive cleaning function to 'abstract_b'...\")\n",
    "df['abstract_b_cleaned'] = df['abstract_b'].progress_apply(comprehensive_clean_abstract)\n",
    "\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e01fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cleaned dataset to data/processed/training_dataset_abstract_cleaned_v2.csv...\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 最終データセットの保存 ---\n",
    "\n",
    "# 元のアブストラクトを削除し、クリーニング後のものだけを残す\n",
    "df_final = df.drop(columns=['abstract_a', 'abstract_b'])\n",
    "df_final = df_final.rename(columns={\n",
    "    'abstract_a_cleaned': 'abstract_a',\n",
    "    'abstract_b_cleaned': 'abstract_b'\n",
    "})\n",
    "\n",
    "# 必要なカラムの順序に並び替え\n",
    "df_final = df_final[['abstract_a', 'abstract_b', 'label', 'data_paper_doi']]\n",
    "\n",
    "print(f\"Saving cleaned dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings defined. All abstracts will be truncated to 3500 characters.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "# 読み込むクリーニング済みファイル\n",
    "INPUT_FILE = \"data/processed/training_dataset_abstract_cleaned_v2.csv\" \n",
    "# 最終的な訓練データセット\n",
    "OUTPUT_FILE = \"data/processed/training_dataset_abstract_cleaned_v3.csv\" \n",
    "\n",
    "# ▼▼▼ 最終的な文字数上限（ハードリミット）を設定 ▼▼▼\n",
    "# 75%が約1500文字、一般的な上限が2500文字であることから、\n",
    "# 3000文字あたりを「絶対にノイズ」と判断する閾値として設定\n",
    "MAX_CHAR_LIMIT = 3500\n",
    "\n",
    "print(f\"Settings defined. All abstracts will be truncated to {MAX_CHAR_LIMIT} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b34905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 3505\n",
      "Cleaned Length:  3500\n"
     ]
    }
   ],
   "source": [
    "# --- 2. クリーニング関数の定義 ---\n",
    "\n",
    "def truncate_text(text):\n",
    "    \"\"\"\n",
    "    テキストを受け取り、先頭から MAX_CHAR_LIMIT 文字だけを残す\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Pythonのスライス機能を使って、先頭からMAX_CHAR_LIMIT文字目までを取得\n",
    "    return text[:MAX_CHAR_LIMIT]\n",
    "\n",
    "# テスト実行\n",
    "test_text = \"A\" * (MAX_CHAR_LIMIT + 5)\n",
    "print(f\"Original Length: {len(test_text)}\")\n",
    "print(f\"Cleaned Length:  {len(truncate_text(test_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80242937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from data/processed/training_dataset_abstract_cleaned_v2.csv...\n",
      "Applying truncation to 'abstract_a'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb49621fdf47436694ad48ac362282a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying truncation to 'abstract_b'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c828bb5cde8b42fab70e4293edde4b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. データセットの読み込みと前処理の実行 ---\n",
    "print(f\"Loading dataset from {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b'])\n",
    "\n",
    "print(\"Applying truncation to 'abstract_a'...\")\n",
    "df['abstract_a_cleaned'] = df['abstract_a'].progress_apply(truncate_text)\n",
    "\n",
    "print(\"Applying truncation to 'abstract_b'...\")\n",
    "df['abstract_b_cleaned'] = df['abstract_b'].progress_apply(truncate_text)\n",
    "\n",
    "print(\"Truncation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a784f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final cleaned dataset to data/processed/training_dataset_abstract_v3.csv...\n",
      "Save complete.\n",
      "\n",
      "Max length in final 'abstract_a': 3500\n",
      "Max length in final 'abstract_b': 3500\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 最終データセットの保存 ---\n",
    "\n",
    "# 元のアブストラクトを削除し、クリーニング後のものだけを残す\n",
    "df_final = df.drop(columns=['abstract_a', 'abstract_b', 'char_len_a', 'char_len_b', 'max_char_len'], errors='ignore') # 以前の分析列も削除\n",
    "df_final = df_final.rename(columns={\n",
    "    'abstract_a_cleaned': 'abstract_a',\n",
    "    'abstract_b_cleaned': 'abstract_b'\n",
    "})\n",
    "\n",
    "# 必要なカラムの順序に並び替え\n",
    "df_final = df_final[['abstract_a', 'abstract_b', 'label', 'data_paper_doi']]\n",
    "\n",
    "print(f\"Saving final cleaned dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Save complete.\")\n",
    "\n",
    "# 念のため、処理後の最大文字数を確認\n",
    "print(\"\\nMax length in final 'abstract_a':\", df_final['abstract_a'].str.len().max())\n",
    "print(\"Max length in final 'abstract_b':\", df_final['abstract_b'].str.len().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

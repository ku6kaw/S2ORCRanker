{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- è¨ºæ–­ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ ---\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- è¨­å®šï¼ˆå…ƒã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ã‚³ãƒ”ãƒ¼ï¼‰ ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "\n",
    "# --- èª¿æŸ»é–‹å§‹ ---\n",
    "print(\"=\"*50)\n",
    "print(\"--- Starting Investigation: Why 46 instead of 50? ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. è©•ä¾¡å¯¾è±¡ã®å…¨ãƒ‡ãƒ¼ã‚¿ãƒšãƒ¼ãƒ‘ãƒ¼DOIã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "df_eval_papers = pd.read_csv(EVAL_PAPERS_FILE)\n",
    "all_data_paper_dois = tuple(df_eval_papers['cited_datapaper_doi'].unique())\n",
    "\n",
    "print(f\"Found {len(all_data_paper_dois)} unique data paper DOIs in the input file.\\n\")\n",
    "\n",
    "valid_queries_count = 0\n",
    "skipped_queries = []\n",
    "\n",
    "# 2. å„ãƒ‡ãƒ¼ã‚¿ãƒšãƒ¼ãƒ‘ãƒ¼DOIã‚’ãƒ«ãƒ¼ãƒ—ã§ãƒã‚§ãƒƒã‚¯\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    for i, data_paper_doi in enumerate(tqdm(all_data_paper_dois, desc=\"Checking each DOI\")):\n",
    "        print(f\"\\n--- [{i+1}/{len(all_data_paper_dois)}] Checking DOI: {data_paper_doi} ---\")\n",
    "        \n",
    "        # --- åŸå› 1ã®ãƒã‚§ãƒƒã‚¯ ---\n",
    "        # ã“ã®ãƒ‡ãƒ¼ã‚¿ãƒšãƒ¼ãƒ‘ãƒ¼ã‚’å¼•ç”¨ã—ã€ã‹ã¤äººé–“ãŒã€Œæ­£è§£ã€ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸè«–æ–‡ã‚’å…¨ã¦å–å¾—\n",
    "        query_gt = \"SELECT citing_doi FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        gt_rows = conn.execute(query_gt, (data_paper_doi,)).fetchall()\n",
    "        ground_truth_dois = {row[0] for row in gt_rows}\n",
    "        \n",
    "        print(f\"Found {len(ground_truth_dois)} annotated positive papers.\")\n",
    "\n",
    "        # æ¡ä»¶: æ­£è§£è«–æ–‡ãŒ2ä»¶ä»¥ä¸Šãªã„ã¨ã€ã‚¯ã‚¨ãƒªã¨æ®‹ã‚Šã®æ­£è§£ã«åˆ†ã‘ã‚‰ã‚Œãªã„\n",
    "        if len(ground_truth_dois) < 2:\n",
    "            reason = f\"Skipped: Not enough positive papers found. (Found {len(ground_truth_dois)}, required >= 2)\"\n",
    "            print(f\"ğŸ”´ {reason}\")\n",
    "            skipped_queries.append({\"doi\": data_paper_doi, \"reason\": reason})\n",
    "            continue # ã“ã®DOIã®å‡¦ç†ã‚’ä¸­æ–­ã—ã€æ¬¡ã«é€²ã‚€\n",
    "\n",
    "        # --- åŸå› 2ã®ãƒã‚§ãƒƒã‚¯ ---\n",
    "        # 1ã¤ã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦å–ã‚Šå‡ºã™\n",
    "        query_doi = ground_truth_dois.pop()\n",
    "        \n",
    "        # ã‚¯ã‚¨ãƒªè«–æ–‡ã®ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "        query_text_res = conn.execute(\"SELECT abstract FROM papers WHERE doi = ?\", (query_doi,)).fetchone()\n",
    "        \n",
    "        if not (query_text_res and query_text_res[0]):\n",
    "            reason = f\"Skipped: The selected query paper ({query_doi}) has no abstract.\"\n",
    "            print(f\"ğŸ”´ {reason}\")\n",
    "            skipped_queries.append({\"doi\": data_paper_doi, \"reason\": reason})\n",
    "            continue\n",
    "\n",
    "        # --- åŸå› 3ã®ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰ ---\n",
    "        #æ®‹ã‚Šã®æ­£è§£è«–æ–‡ãŒDBã«å­˜åœ¨ã™ã‚‹ã‹\n",
    "        if not ground_truth_dois:\n",
    "            reason = f\"Skipped: After popping one query, no ground truth papers remain.\"\n",
    "            print(f\"ğŸ”´ {reason}\")\n",
    "            skipped_queries.append({\"doi\": data_paper_doi, \"reason\": reason})\n",
    "            continue\n",
    "\n",
    "        # ã“ã“ã¾ã§åˆ°é”ã™ã‚Œã°æœ‰åŠ¹ãªã‚¯ã‚¨ãƒª\n",
    "        print(f\"âœ… Success: This DOI will generate a valid query.\")\n",
    "        print(f\"   - Query DOI: {query_doi}\")\n",
    "        print(f\"   - Number of ground truths: {len(ground_truth_dois)}\")\n",
    "        valid_queries_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Investigation Complete ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nTotal valid queries that can be generated: {valid_queries_count}\")\n",
    "print(f\"Total skipped data paper DOIs: {len(skipped_queries)}\")\n",
    "\n",
    "if skipped_queries:\n",
    "    print(\"\\n--- Details of Skipped DOIs ---\")\n",
    "    for item in skipped_queries:\n",
    "        print(f\"- DOI: {item['doi']}\")\n",
    "        print(f\"  Reason: {item['reason']}\")\n",
    "else:\n",
    "    print(\"\\nNo DOIs were skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a702e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Final Evaluation Set: data/datapapers/sampled/evaluation_data_papers_50_v2.csv ---\n",
      "âœ… Loaded 50 data papers from file.\n",
      "\n",
      "==================================================\n",
      "âœ… SUCCESS: All 50 papers in the file have 2 or more 'Human: Used' entries.\n",
      "   The evaluation script will now run on all 50 queries.\n",
      "==================================================\n",
      "\n",
      "Full list of counts:\n",
      "| data_paper_doi                |   human_used_count |\n",
      "|:------------------------------|-------------------:|\n",
      "| 10.1007/S11558-019-09344-2    |                 45 |\n",
      "| 10.5194/ESSD-13-3907-2021     |                 28 |\n",
      "| 10.1002/GDJ3.44               |                 21 |\n",
      "| 10.1016/J.DIB.2017.10.024     |                 13 |\n",
      "| 10.1038/S41597-021-00997-6    |                 11 |\n",
      "| 10.1038/S41597-019-0236-X     |                 12 |\n",
      "| 10.5194/ESSD-13-2895-2021     |                  4 |\n",
      "| 10.1080/20964471.2019.1625528 |                  7 |\n",
      "| 10.1038/S41597-020-0386-X     |                 11 |\n",
      "| 10.1038/S41597-020-0520-9     |                  4 |\n",
      "| 10.1016/J.DIB.2017.04.013     |                  9 |\n",
      "| 10.1038/S41597-019-0341-X     |                  4 |\n",
      "| 10.1038/S41597-021-00937-4    |                  8 |\n",
      "| 10.1038/S41597-020-0401-2     |                  7 |\n",
      "| 10.1038/S41597-021-01054-Y    |                  4 |\n",
      "| 10.1038/S41597-020-0460-4     |                  5 |\n",
      "| 10.1038/S41597-019-0183-6     |                  7 |\n",
      "| 10.5194/ESSD-13-3607-2021     |                  5 |\n",
      "| 10.1038/S41597-021-00909-8    |                  4 |\n",
      "| 10.1038/S41597-020-0488-5     |                  2 |\n",
      "| 10.1038/S41597-020-00763-0    |                  4 |\n",
      "| 10.1038/S41597-020-00609-9    |                  3 |\n",
      "| 10.1038/S41597-019-0089-3     |                  5 |\n",
      "| 10.1016/J.DIB.2017.09.038     |                  4 |\n",
      "| 10.1002/LOL2.10203            |                  4 |\n",
      "| 10.1038/S41597-022-01284-8    |                  5 |\n",
      "| 10.1038/S41597-020-0377-Y     |                  5 |\n",
      "| 10.1038/S41597-020-00680-2    |                  5 |\n",
      "| 10.1038/S41597-019-0325-X     |                  3 |\n",
      "| 10.1038/S41597-019-0169-4     |                  3 |\n",
      "| 10.1016/J.GDATA.2016.08.008   |                  2 |\n",
      "| 10.1016/J.COMNET.2021.108001  |                  4 |\n",
      "| 10.5194/ESSD-13-5423-2021     |                  4 |\n",
      "| 10.1038/S41597-022-01383-6    |                  4 |\n",
      "| 10.1038/S41597-021-00955-2    |                  3 |\n",
      "| 10.1038/S41597-020-0459-X     |                  4 |\n",
      "| 10.1038/S41597-020-0363-4     |                  3 |\n",
      "| 10.1038/S41597-020-00712-X    |                  3 |\n",
      "| 10.1038/S41597-020-00610-2    |                  4 |\n",
      "| 10.1038/S41597-019-0219-Y     |                  2 |\n",
      "| 10.1038/S41597-019-0074-X     |                  2 |\n",
      "| 10.1016/J.DIB.2020.105537     |                  2 |\n",
      "| 10.1016/J.DIB.2016.11.056     |                  2 |\n",
      "| 10.1002/GDJ3.77               |                  2 |\n",
      "| 10.1002/ECY.2663              |                  2 |\n",
      "| 10.1038/S41597-019-0217-0     |                  3 |\n",
      "| 10.1038/S41597-019-0056-Z     |                  3 |\n",
      "| 10.1016/J.DIB.2019.104340     |                  3 |\n",
      "| 10.1016/J.DIB.2018.11.111     |                  5 |\n",
      "| 10.1016/J.DIB.2020.106438     |                  3 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# --- 1. è¨­å®š ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "# â–¼â–¼â–¼ æ¤œè¨¼å¯¾è±¡ã®ã€Œæ–°ã—ã„ã€è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ« â–¼â–¼â–¼\n",
    "FINAL_EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "\n",
    "print(f\"--- Verifying Final Evaluation Set: {FINAL_EVAL_PAPERS_FILE} ---\")\n",
    "\n",
    "# --- 2. æ–°ã—ã„è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿è«–æ–‡ï¼ˆ50ä»¶ï¼‰ã®ãƒªã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ ---\n",
    "try:\n",
    "    df_eval_papers = pd.read_csv(FINAL_EVAL_PAPERS_FILE)\n",
    "    eval_data_paper_dois = tuple(df_eval_papers['cited_datapaper_doi'].unique())\n",
    "    \n",
    "    if len(df_eval_papers) != 50:\n",
    "        print(f\"âš ï¸ Warning: File does not contain 50 papers. Found: {len(df_eval_papers)}\")\n",
    "    else:\n",
    "        print(f\"âœ… Loaded {len(df_eval_papers)} data papers from file.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. DBã‚’èª¿æŸ» ---\n",
    "results = []\n",
    "try:\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        for data_paper_doi in eval_data_paper_dois:\n",
    "            # è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ(22b)ã®ãƒ­ã‚¸ãƒƒã‚¯ã¨åŒæ§˜ã«ã€\n",
    "            # æ­£è§£DOI (Human=1) ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            query_gt = \"SELECT COUNT(citing_doi) FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "            count = conn.execute(query_gt, (data_paper_doi,)).fetchone()[0]\n",
    "            results.append({\n",
    "                'data_paper_doi': data_paper_doi,\n",
    "                'human_used_count': count\n",
    "            })\n",
    "\n",
    "    # --- 4. çµæœã®è¡¨ç¤º ---\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # æ­£è§£ãŒ2ä»¶æœªæº€ã®ã‚‚ã®ï¼ˆï¼è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§é™¤å¤–ã•ã‚Œã¦ã—ã¾ã†ã‚‚ã®ï¼‰ã‚’è¡¨ç¤º\n",
    "    df_invalid = df_results[df_results['human_used_count'] < 2]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if not df_invalid.empty:\n",
    "        print(f\"âŒ FAILED: Found {len(df_invalid)} data papers with < 2 'Human: Used' entries:\")\n",
    "        print(df_invalid)\n",
    "        print(\"   (These papers will be skipped by the evaluation script)\")\n",
    "    else:\n",
    "        print(f\"âœ… SUCCESS: All {len(df_results)} papers in the file have 2 or more 'Human: Used' entries.\")\n",
    "        print(\"   The evaluation script will now run on all 50 queries.\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nFull list of counts:\")\n",
    "    print(df_results.to_markdown(index=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ğŸ’¥ An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b697717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Training Dataset (Abstract Version with DOIs) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59eaad1d0d140f5b0484ff3557c43cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Positive Pairs:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7,063 positive pairs.\n",
      "Generated 28,252 negative pairs.\n",
      "Fetching abstracts for 29,523 unique papers...\n",
      "Saving final dataset with 35,315 pairs to data/processed/training_dataset_abstract_with_dois.csv...\n",
      "\n",
      "--- Process Complete ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi_a</th>\n",
       "      <th>doi_b</th>\n",
       "      <th>abstract_a</th>\n",
       "      <th>abstract_b</th>\n",
       "      <th>label</th>\n",
       "      <th>data_paper_doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/ENVIRONSCIPROC2023026031</td>\n",
       "      <td>10.1007/S00484-023-02531-2</td>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>The modern unambiguous climate change reveals ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.3390/ENVIRONSCIPROC2023026031</td>\n",
       "      <td>10.1007/S00704-022-04129-X</td>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>Outdoor thermal comfort (OTC) surveys require ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1007/S00484-023-02531-2</td>\n",
       "      <td>10.1007/S00704-022-04129-X</td>\n",
       "      <td>The modern unambiguous climate change reveals ...</td>\n",
       "      <td>Outdoor thermal comfort (OTC) surveys require ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3390/ENVIRONSCIPROC2023026031</td>\n",
       "      <td>10.1038/S41598-023-44286-1</td>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>In the months of March-June, India experiences...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.3390/ENVIRONSCIPROC2023026031</td>\n",
       "      <td>10.1029/2023GL104850</td>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>Extreme compound heatwaves (ECHWs) have the po...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              doi_a                       doi_b  \\\n",
       "0  10.3390/ENVIRONSCIPROC2023026031  10.1007/S00484-023-02531-2   \n",
       "1  10.3390/ENVIRONSCIPROC2023026031  10.1007/S00704-022-04129-X   \n",
       "2        10.1007/S00484-023-02531-2  10.1007/S00704-022-04129-X   \n",
       "3  10.3390/ENVIRONSCIPROC2023026031  10.1038/S41598-023-44286-1   \n",
       "4  10.3390/ENVIRONSCIPROC2023026031        10.1029/2023GL104850   \n",
       "\n",
       "                                          abstract_a  \\\n",
       "0  We present climatology and trends of the UTCI ...   \n",
       "1  We present climatology and trends of the UTCI ...   \n",
       "2  The modern unambiguous climate change reveals ...   \n",
       "3  We present climatology and trends of the UTCI ...   \n",
       "4  We present climatology and trends of the UTCI ...   \n",
       "\n",
       "                                          abstract_b  label    data_paper_doi  \n",
       "0  The modern unambiguous climate change reveals ...      1  10.1002/GDJ3.102  \n",
       "1  Outdoor thermal comfort (OTC) surveys require ...      1  10.1002/GDJ3.102  \n",
       "2  Outdoor thermal comfort (OTC) surveys require ...      1  10.1002/GDJ3.102  \n",
       "3  In the months of March-June, India experiences...      1  10.1002/GDJ3.102  \n",
       "4  Extreme compound heatwaves (ECHWs) have the po...      1  10.1002/GDJ3.102  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Step 1: Ë®≠ÂÆö ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "TRAINING_DATAPAPERS_FILE = \"data/datapapers/sampled/training_data_papers_50.csv\"\n",
    "# ‚ñº‚ñº‚ñº ‰øÆÊ≠£ÁÇπ: Âá∫Âäõ„Éï„Ç°„Ç§„É´Âêç„ÇíÂ§âÊõ¥ ‚ñº‚ñº‚ñº\n",
    "OUTPUT_FILE = \"data/processed/training_dataset_abstract_with_dois.csv\"\n",
    "NEGATIVE_SAMPLE_RATIO = 4\n",
    "\n",
    "def create_training_dataset_with_dois():\n",
    "    print(\"--- Creating Training Dataset (Abstract Version with DOIs) ---\")\n",
    "\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            \n",
    "            # --- 1. Ë®ìÁ∑¥Áî®„ÅÆ„Éá„Éº„ÇøË´ñÊñáDOI„É™„Çπ„Éà„ÇíÂèñÂæó ---\n",
    "            df_train_papers = pd.read_csv(TRAINING_DATAPAPERS_FILE)\n",
    "            train_dois = tuple(df_train_papers['cited_datapaper_doi'].unique())\n",
    "            \n",
    "            # --- 2. Ê≠£‰æã„Éö„Ç¢„ÅÆÂÖÉ„Å®„Å™„ÇãË´ñÊñá„É™„Çπ„Éà„ÇíÂèñÂæó ---\n",
    "            placeholders = ','.join('?' for _ in train_dois)\n",
    "            query = f\"\"\"\n",
    "                SELECT\n",
    "                    cited_datapaper_doi,\n",
    "                    citing_doi,\n",
    "                    human_annotation_status\n",
    "                FROM\n",
    "                    positive_candidates\n",
    "                WHERE\n",
    "                    cited_datapaper_doi IN ({placeholders})\n",
    "                    AND (human_annotation_status = 1 OR (llm_annotation_status = 1 AND human_annotation_status = 0))\n",
    "            \"\"\"\n",
    "            df_candidates = pd.read_sql_query(query, conn, params=train_dois)\n",
    "\n",
    "            # --- 3. Ê≠£‰æã„Éö„Ç¢„ÅÆ‰ΩúÊàê (DOI„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ) ---\n",
    "            positive_pairs = []\n",
    "            grouped = df_candidates.groupby('cited_datapaper_doi')\n",
    "            \n",
    "            for data_paper_doi, group in tqdm(grouped, desc=\"Generating Positive Pairs\"):\n",
    "                human_used_dois = group[group['human_annotation_status'] == 1]['citing_doi'].tolist()\n",
    "                llm_used_dois = group[group['human_annotation_status'] == 0]['citing_doi'].tolist()\n",
    "                \n",
    "                if not human_used_dois:\n",
    "                    continue \n",
    "\n",
    "                # (Á∑ë, Á∑ë) „Éö„Ç¢\n",
    "                for pair in itertools.combinations(human_used_dois, 2):\n",
    "                    positive_pairs.append({\n",
    "                        'doi_a': pair[0], \n",
    "                        'doi_b': pair[1], \n",
    "                        'label': 1,\n",
    "                        'data_paper_doi': data_paper_doi\n",
    "                    })\n",
    "                    \n",
    "                # (Á∑ë, ÁôΩ) „Éö„Ç¢\n",
    "                for human_doi in human_used_dois:\n",
    "                    for llm_doi in llm_used_dois:\n",
    "                        positive_pairs.append({\n",
    "                            'doi_a': human_doi, \n",
    "                            'doi_b': llm_doi, \n",
    "                            'label': 1,\n",
    "                            'data_paper_doi': data_paper_doi\n",
    "                        })\n",
    "\n",
    "            df_positive = pd.DataFrame(positive_pairs)\n",
    "            print(f\"Generated {len(df_positive):,} positive pairs.\")\n",
    "            \n",
    "            # --- 4. Ë≤†‰æã„Éö„Ç¢„ÅÆ‰ΩúÊàê (Easy Negative) ---\n",
    "            num_negatives_to_sample = len(df_positive) * NEGATIVE_SAMPLE_RATIO\n",
    "            positive_dois = set(df_positive['doi_a']) | set(df_positive['doi_b'])\n",
    "            anchor_dois_for_negative = df_positive['doi_a'].sample(n=num_negatives_to_sample, replace=True).tolist()\n",
    "            \n",
    "            query_random = f\"\"\"\n",
    "                SELECT doi FROM papers \n",
    "                WHERE doi NOT IN (SELECT doi FROM full_texts) \n",
    "                  AND abstract IS NOT NULL AND abstract != ''\n",
    "                ORDER BY RANDOM() \n",
    "                LIMIT {num_negatives_to_sample * 2}\n",
    "            \"\"\"\n",
    "            df_random_papers = pd.read_sql_query(query_random, conn)\n",
    "            random_dois = df_random_papers[~df_random_papers['doi'].isin(positive_dois)]['doi'].tolist()\n",
    "            \n",
    "            if len(random_dois) < num_negatives_to_sample:\n",
    "                num_negatives_to_sample = len(random_dois)\n",
    "            \n",
    "            negative_pairs = []\n",
    "            for i in range(num_negatives_to_sample):\n",
    "                negative_pairs.append({\n",
    "                    'doi_a': anchor_dois_for_negative[i], \n",
    "                    'doi_b': random_dois[i], \n",
    "                    'label': 0,\n",
    "                    'data_paper_doi': None\n",
    "                })\n",
    "            \n",
    "            df_negative = pd.DataFrame(negative_pairs)\n",
    "            print(f\"Generated {len(df_negative):,} negative pairs.\")\n",
    "\n",
    "            # --- 5. ÂÖ®„Éö„Ç¢„ÅÆÁµêÂêà„Å®„Ç¢„Éñ„Çπ„Éà„É©„ÇØ„Éà„ÅÆÂèñÂæó ---\n",
    "            df_final_pairs = pd.concat([df_positive, df_negative]).reset_index(drop=True)\n",
    "            all_dois = set(df_final_pairs['doi_a']) | set(df_final_pairs['doi_b'])\n",
    "            \n",
    "            print(f\"Fetching abstracts for {len(all_dois):,} unique papers...\")\n",
    "            query_texts = f\"SELECT doi, abstract FROM papers WHERE doi IN ({','.join('?'*len(all_dois))})\"\n",
    "            df_texts = pd.read_sql_query(query_texts, conn, params=list(all_dois))\n",
    "            text_map = dict(zip(df_texts['doi'], df_texts['abstract']))\n",
    "\n",
    "            # --- 6. ÊúÄÁµÇ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúÊàê„Å®‰øùÂ≠ò ---\n",
    "            df_final_dataset = pd.DataFrame()\n",
    "            # ‚ñº‚ñº‚ñº ‰øÆÊ≠£ÁÇπ: DOI„ÅÆÂàó„ÇíËøΩÂä† ‚ñº‚ñº‚ñº\n",
    "            df_final_dataset['doi_a'] = df_final_pairs['doi_a']\n",
    "            df_final_dataset['doi_b'] = df_final_pairs['doi_b']\n",
    "            df_final_dataset['abstract_a'] = df_final_pairs['doi_a'].map(text_map)\n",
    "            df_final_dataset['abstract_b'] = df_final_pairs['doi_b'].map(text_map)\n",
    "            df_final_dataset['label'] = df_final_pairs['label']\n",
    "            df_final_dataset['data_paper_doi'] = df_final_pairs['data_paper_doi']\n",
    "            \n",
    "            df_final_dataset = df_final_dataset.dropna(subset=['abstract_a', 'abstract_b']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"Saving final dataset with {len(df_final_dataset):,} pairs to {OUTPUT_FILE}...\")\n",
    "            df_final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "            \n",
    "            print(\"\\n--- Process Complete ---\")\n",
    "            display(df_final_dataset.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üí• An error occurred: {e}\")\n",
    "\n",
    "# --- ÂÆüË°å ---\n",
    "create_training_dataset_with_dois()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

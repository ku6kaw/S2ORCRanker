{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6501d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import accelerate\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import faiss # Faissライブラリ\n",
    "import shutil # 一時ファイルの削除用\n",
    "\n",
    "# CUDAのデバッグ用\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a0be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set. Inference Batch Size: 512\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "MODEL_CHECKPOINT = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# --- 出力ファイル ---\n",
    "# (これらは最終的に生成されるファイル)\n",
    "EMBEDDINGS_OUTPUT_FILE = \"data/processed/pretrained_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_OUTPUT_FILE = \"data/processed/pretrained_doi_map.json\"\n",
    "FAISS_INDEX_OUTPUT_FILE = \"data/processed/pretrained_scibert.faiss\"\n",
    "\n",
    "# --- 一時ディレクトリ (★再開機能の核) ---\n",
    "TEMP_EMBED_DIR = \"data/processed/embeddings_tmp\"\n",
    "TEMP_DOI_DIR = \"data/processed/dois_tmp\"\n",
    "\n",
    "# --- ハイパーパラメータ ---\n",
    "MAX_LENGTH = 512\n",
    "DB_READ_BATCH_SIZE = 1000 # DBから一度に読み込む行数\n",
    "INFERENCE_BATCH_SIZE = 512 # GPUで一度に処理するバッチサイズ\n",
    "\n",
    "print(f\"Configuration set. Inference Batch Size: {INFERENCE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf40cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398de9ced39f43868a01b0f7c9162b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b113c3928274bbc8f46caa0a6d3c41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PRE-TRAINED model: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67150b4367f46b991ff9fbfda7edc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 事前学習済みモデルとトークナイザのロード ---\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_CHECKPOINT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(f\"Loading PRE-TRAINED model: {MODEL_CHECKPOINT}\")\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "model.eval() # 評価モードに設定\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a64bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database generator defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. データベースからのデータ読み込み (ジェネレータ) ---\n",
    "\n",
    "def get_abstract_batches(db_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    DBからアブストラクトをバッチ単位で読み込むジェネレータ\n",
    "    \"\"\"\n",
    "    print(f\"Opening database connection: {db_path}\")\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        query = \"SELECT COUNT(doi) FROM papers WHERE abstract IS NOT NULL AND abstract != ''\"\n",
    "        total_rows = cursor.execute(query).fetchone()[0]\n",
    "        print(f\"Total abstracts to process: {total_rows:,}\")\n",
    "        \n",
    "        # 最初に総行数を返す\n",
    "        yield total_rows \n",
    "        \n",
    "        cursor.execute(\"SELECT doi, abstract FROM papers WHERE abstract IS NOT NULL AND abstract != ''\")\n",
    "        \n",
    "        batch = []\n",
    "        for row in cursor: # ここではtqdmを使わない\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "print(\"Database generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac062150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/processed/dois_tmp for completed batches...\n",
      "Found 1 completed batches to skip.\n",
      "Opening database connection: data/processed/s2orc_filtered.db\n",
      "Total abstracts to process: 11,619,136\n",
      "Starting/Resuming embedding generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced9100bb784166a169ec7205653010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing Batches:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation complete. All batches saved in chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 全アブストラクトのベクトル化を実行 ---\n",
    "\n",
    "# 一時ディレクトリを作成\n",
    "os.makedirs(TEMP_EMBED_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DOI_DIR, exist_ok=True)\n",
    "\n",
    "# --- ★再開機能★ ---\n",
    "# 既に処理済みのバッチインデックスをスキャン\n",
    "completed_indices = set()\n",
    "print(f\"Scanning {TEMP_DOI_DIR} for completed batches...\")\n",
    "for f in os.listdir(TEMP_DOI_DIR):\n",
    "    if f.startswith('batch_') and f.endswith('.json'):\n",
    "        try:\n",
    "            # batch_00001.json -> 1\n",
    "            completed_indices.add(int(f[6:11]))\n",
    "        except:\n",
    "            pass # ファイル名が不正な場合は無視\n",
    "print(f\"Found {len(completed_indices)} completed batches to skip.\")\n",
    "# --- ここまで ---\n",
    "\n",
    "# torch.no_grad() で勾配計算を無効化\n",
    "with torch.no_grad():\n",
    "    \n",
    "    batch_generator = get_abstract_batches(DB_PATH, batch_size=DB_READ_BATCH_SIZE)\n",
    "    total_rows = next(batch_generator)\n",
    "    total_batches = (total_rows + DB_READ_BATCH_SIZE - 1) // DB_READ_BATCH_SIZE\n",
    "    \n",
    "    print(\"Starting/Resuming embedding generation...\")\n",
    "    \n",
    "    # tqdmプログレスバーを設定\n",
    "    pbar = tqdm(enumerate(batch_generator), total=total_batches, desc=\"Vectorizing Batches\")\n",
    "    \n",
    "    for i, batch in pbar:\n",
    "        \n",
    "        # --- ★再開機能★ ---\n",
    "        if i in completed_indices:\n",
    "            pbar.set_description(f\"Skipping Batch {i} (already processed)\")\n",
    "            continue # このバッチは処理済みなのでスキップ\n",
    "        # --- ここまで ---\n",
    "\n",
    "        pbar.set_description(f\"Processing Batch {i}\")\n",
    "        dois, abstracts = zip(*batch)\n",
    "        \n",
    "        # バッチ(1000件)をさらに小さなサブバッチ(512件)に分けてGPUで処理\n",
    "        # (GPUメモリが少ない場合のため)\n",
    "        batch_embeddings_list = []\n",
    "        for j in range(0, len(abstracts), INFERENCE_BATCH_SIZE):\n",
    "            sub_batch_abstracts = abstracts[j : j + INFERENCE_BATCH_SIZE]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                list(sub_batch_abstracts), \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=MAX_LENGTH, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.pooler_output\n",
    "            batch_embeddings_list.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        # サブバッチを結合\n",
    "        embeddings_cpu = np.vstack(batch_embeddings_list).astype(np.float32)\n",
    "        \n",
    "        # --- チャンク（一時ファイル）としてディスクに保存 ---\n",
    "        embed_filename = os.path.join(TEMP_EMBED_DIR, f\"batch_{i:05d}.npy\")\n",
    "        doi_filename = os.path.join(TEMP_DOI_DIR, f\"batch_{i:05d}.json\")\n",
    "        \n",
    "        np.save(embed_filename, embeddings_cpu)\n",
    "        # ★DOIファイルを最後に保存する（=このバッチの「完了」フラグ）\n",
    "        with open(doi_filename, 'w') as f:\n",
    "            json.dump(dois, f)\n",
    "\n",
    "print(f\"\\nEmbedding generation complete. All batches saved in chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbb7716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging embedding chunks from disk...\n",
      "Reading 11620 DOI chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5523a1e8be4d7abcebdbf3a23959b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading DOI chunks:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors to merge: 11,619,136\n",
      "Merging 11620 Embedding chunks into data/processed/pretrained_scibert_cls_embeddings.npy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe99bca877a44cdae68e80c4090d271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging Embedding Chunks:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings saved to data/processed/pretrained_scibert_cls_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# --- 6. チャンクの結合と保存 ---\n",
    "print(\"Merging embedding chunks from disk...\")\n",
    "\n",
    "# 1. 保存されたDOIチャンクをすべて読み込み、最終的なDOIリストと総数を確定\n",
    "all_dois = []\n",
    "doi_files = sorted([f for f in os.listdir(TEMP_DOI_DIR) if f.endswith('.json')])\n",
    "total_rows_processed = 0\n",
    "\n",
    "print(f\"Reading {len(doi_files)} DOI chunks...\")\n",
    "for f in tqdm(doi_files, desc=\"Reading DOI chunks\"):\n",
    "    with open(os.path.join(TEMP_DOI_DIR, f), 'r') as fp:\n",
    "        batch_dois = json.load(fp)\n",
    "        all_dois.extend(batch_dois)\n",
    "        total_rows_processed += len(batch_dois)\n",
    "\n",
    "print(f\"Total vectors to merge: {total_rows_processed:,}\")\n",
    "\n",
    "# 2. 最終的な巨大Numpy配列（メモリマップファイル）をディスク上に作成\n",
    "#    (Shape: [総論文数, 768])\n",
    "d = 768 # SciBERTの隠れ層サイズ\n",
    "final_embeddings = np.memmap(\n",
    "    EMBEDDINGS_OUTPUT_FILE, \n",
    "    dtype=np.float32, \n",
    "    mode='w+', \n",
    "    shape=(total_rows_processed, d)\n",
    ")\n",
    "\n",
    "# 3. 一時ファイル（NPY）を順番に読み込み、巨大な配列に書き込む\n",
    "current_index = 0\n",
    "print(f\"Merging {len(doi_files)} Embedding chunks into {EMBEDDINGS_OUTPUT_FILE}...\")\n",
    "for f in tqdm(doi_files, desc=\"Merging Embedding Chunks\"):\n",
    "    # batch_00001.json -> batch_00001.npy\n",
    "    batch_npy_file = os.path.join(TEMP_EMBED_DIR, f.replace('.json', '.npy'))\n",
    "    \n",
    "    batch_data = np.load(batch_npy_file)\n",
    "    \n",
    "    start_index = current_index\n",
    "    end_index = start_index + len(batch_data)\n",
    "    \n",
    "    final_embeddings[start_index:end_index] = batch_data\n",
    "    current_index = end_index\n",
    "\n",
    "# 4. メモリマップを閉じる（ディスクへの書き込みを確定）\n",
    "final_embeddings.flush()\n",
    "del final_embeddings # メモリマップファイルへの参照を閉じる\n",
    "print(f\"Final embeddings saved to {EMBEDDINGS_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82aa81e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DOI-to-Index map to data/processed/pretrained_doi_map.json...\n",
      "Cleaning up temporary directories...\n",
      "\n",
      "--- Step 1 (Embedding Generation) Complete ---\n",
      "Total embeddings saved: 11619136\n"
     ]
    }
   ],
   "source": [
    "# --- 7. DOIマップの保存と一時ファイルの削除 ---\n",
    "\n",
    "# 1. DOIマップの保存\n",
    "print(f\"Saving DOI-to-Index map to {DOI_MAP_OUTPUT_FILE}...\")\n",
    "doi_to_index_map = {doi: i for i, doi in enumerate(all_dois)}\n",
    "with open(DOI_MAP_OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(doi_to_index_map, f)\n",
    "\n",
    "# 2. 一時ディレクトリの削除\n",
    "print(f\"Cleaning up temporary directories...\")\n",
    "shutil.rmtree(TEMP_EMBED_DIR)\n",
    "shutil.rmtree(TEMP_DOI_DIR)\n",
    "\n",
    "print(\"\\n--- Step 1 (Embedding Generation) Complete ---\")\n",
    "print(f\"Total embeddings saved: {len(doi_to_index_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a3adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Faiss Index ---\n",
      "File size: 33.24 GB\n",
      "Calculated vector count: 11,619,136\n",
      "Loading embeddings from data/processed/pretrained_scibert_cls_embeddings.npy (memmap read mode)...\n",
      "Faiss index type: IndexFlatL2 (Dimensions: 768)\n",
      "Adding vectors to the index (this may take time)...\n",
      "Total vectors in index: 11619136\n",
      "Saving Faiss index to data/processed/pretrained_scibert.faiss...\n",
      "\n",
      "--- Faiss Indexing Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Faissインデックスの構築と保存 ---\n",
    "print(\"\\n--- Building Faiss Index ---\")\n",
    "d = 768 # SciBERTの次元数\n",
    "\n",
    "# 1. ファイルサイズから行数（ベクトル数）を計算\n",
    "file_size = os.path.getsize(EMBEDDINGS_OUTPUT_FILE)\n",
    "dtype_size = np.dtype(np.float32).itemsize # 4バイト\n",
    "total_vectors = file_size // (d * dtype_size)\n",
    "\n",
    "print(f\"File size: {file_size / (1024**3):.2f} GB\")\n",
    "print(f\"Calculated vector count: {total_vectors:,}\")\n",
    "\n",
    "# 2. np.memmap で読み込みモード ('r') で開く\n",
    "print(f\"Loading embeddings from {EMBEDDINGS_OUTPUT_FILE} (memmap read mode)...\")\n",
    "embeddings_mmap = np.memmap(\n",
    "    EMBEDDINGS_OUTPUT_FILE,\n",
    "    dtype=np.float32,\n",
    "    mode='r',\n",
    "    shape=(total_vectors, d)\n",
    ")\n",
    "\n",
    "# 3. インデックスの構築\n",
    "# (IndexFlatL2は全データをRAM上のインデックスにコピーするため、十分なRAMが必要です)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(f\"Faiss index type: IndexFlatL2 (Dimensions: {d})\")\n",
    "\n",
    "# ベクトルをインデックスに追加\n",
    "# (mmapを使うことで、ディスクから少しずつ読み出してRAM上のインデックスにコピーされる)\n",
    "print(\"Adding vectors to the index (this may take time)...\")\n",
    "index.add(embeddings_mmap)\n",
    "\n",
    "print(f\"Total vectors in index: {index.ntotal}\")\n",
    "\n",
    "# 4. インデックスをディスクに保存\n",
    "print(f\"Saving Faiss index to {FAISS_INDEX_OUTPUT_FILE}...\")\n",
    "faiss.write_index(index, FAISS_INDEX_OUTPUT_FILE)\n",
    "\n",
    "print(\"\\n--- Faiss Indexing Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca2ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0065261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings defined. Ratio 1:3 (Hard Negatives only).\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "\n",
    "# 入力: 既存の訓練データ (v3) - ここから正例のみを抽出します\n",
    "INPUT_TRAIN_FILE = \"data/processed/training_dataset_abstract_cleaned_v3.csv\"\n",
    "\n",
    "# 参照: 構築したFaissインデックスとDOIマップ\n",
    "FAISS_INDEX_FILE = \"data/processed/pretrained_scibert.faiss\"\n",
    "DOI_MAP_FILE = \"data/processed/pretrained_doi_map.json\"\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "\n",
    "# 出力: Hard Negativeのみで構成された新しい訓練データ\n",
    "OUTPUT_FILE = \"data/processed/training_dataset_hard_negatives_1to3.csv\"\n",
    "\n",
    "# モデル (クエリのベクトル化用)\n",
    "MODEL_CHECKPOINT = \"allenai/scibert_scivocab_uncased\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# マイニング設定\n",
    "# 正例1に対して負例3を作るため、フィルタリング(正解の除外)余裕を見て多めに検索\n",
    "SEARCH_TOP_K = 50 \n",
    "NEG_RATIO = 3 # 正例 : 負例 = 1 : 3\n",
    "\n",
    "print(f\"Settings defined. Ratio 1:{NEG_RATIO} (Hard Negatives only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988506a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: data/processed/training_dataset_abstract_cleaned_v3.csv\n",
      "Loaded 7,013 positive pairs.\n",
      "Unique anchors to query: 369\n",
      "Anchor-to-Positives map created.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. データの読み込み ---\n",
    "print(f\"Loading training data: {INPUT_TRAIN_FILE}\")\n",
    "df_train = pd.read_csv(INPUT_TRAIN_FILE)\n",
    "df_train = df_train.dropna(subset=['abstract_a', 'abstract_b'])\n",
    "\n",
    "# 正例ペアのみを抽出\n",
    "df_positives = df_train[df_train['label'] == 1].copy().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df_positives):,} positive pairs.\")\n",
    "\n",
    "# ユニークなアンカー(abstract_a)のリストを作成（検索回数を減らすため）\n",
    "unique_anchors = df_positives['abstract_a'].unique()\n",
    "print(f\"Unique anchors to query: {len(unique_anchors):,}\")\n",
    "\n",
    "# アンカー -> 正解アブストラクト(abstract_b) のマッピングを作成\n",
    "# (検索結果から「正解」を除外してHard Negativeにするため)\n",
    "# 1つのアンカーに対して複数の正解がある場合に対応するため set で保持\n",
    "anchor_to_positives = df_positives.groupby('abstract_a')['abstract_b'].apply(set).to_dict()\n",
    "print(\"Anchor-to-Positives map created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ff2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Faiss index: data/processed/pretrained_scibert.faiss\n",
      "Index loaded. Total vectors: 11,619,136\n",
      "Loading DOI map: data/processed/pretrained_doi_map.json\n",
      "DOI map loaded and inverted.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. リソースのロード ---\n",
    "\n",
    "# モデルとトークナイザ\n",
    "print(f\"Loading model: {MODEL_CHECKPOINT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Faissインデックス\n",
    "print(f\"Loading Faiss index: {FAISS_INDEX_FILE}\")\n",
    "index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "print(f\"Index loaded. Total vectors: {index.ntotal:,}\")\n",
    "\n",
    "# DOIマップ (Index ID -> DOI)\n",
    "print(f\"Loading DOI map: {DOI_MAP_FILE}\")\n",
    "with open(DOI_MAP_FILE, 'r') as f:\n",
    "    doi_map = json.load(f)\n",
    "# 検索結果のIDからDOIを引けるように逆転させる\n",
    "id_to_doi = {v: k for k, v in doi_map.items()}\n",
    "print(\"DOI map loaded and inverted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec70c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d51e62ad0947488e36a2baa7e62ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mining Hard Negatives:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search complete. Candidates collected.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. アンカーのベクトル化と検索 ---\n",
    "# アンカーごとに「Hard Negative候補のDOIリスト」を保持する辞書\n",
    "anchor_to_hard_neg_dois = {}\n",
    "\n",
    "print(\"Starting search...\")\n",
    "\n",
    "all_anchors = list(unique_anchors)\n",
    "# バッチ処理で検索\n",
    "for i in tqdm(range(0, len(all_anchors), BATCH_SIZE), desc=\"Mining Hard Negatives\"):\n",
    "    batch_anchors = all_anchors[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # 1. ベクトル化\n",
    "    inputs = tokenizer(\n",
    "        batch_anchors, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.pooler_output.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # 2. Faissで検索\n",
    "    distances, indices = index.search(embeddings, SEARCH_TOP_K)\n",
    "    \n",
    "    # 3. 結果の保存\n",
    "    for j, anchor_text in enumerate(batch_anchors):\n",
    "        search_results_ids = indices[j]\n",
    "        \n",
    "        # IDをDOIに変換してリスト化\n",
    "        candidate_dois = []\n",
    "        for result_id in search_results_ids:\n",
    "            if result_id != -1 and result_id in id_to_doi:\n",
    "                candidate_dois.append(id_to_doi[result_id])\n",
    "        \n",
    "        anchor_to_hard_neg_dois[anchor_text] = candidate_dois\n",
    "\n",
    "print(\"Search complete. Candidates collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf51032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching text for 17,630 unique DOIs from DB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1c6475d14e4cd5a89eb3ef519eeb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying DB:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 17,630 abstracts.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. DBからHard Negativeのアブストラクトを取得 ---\n",
    "\n",
    "# 必要な全DOIのユニークリストを作成\n",
    "all_needed_dois = set()\n",
    "for dois in anchor_to_hard_neg_dois.values():\n",
    "    all_needed_dois.update(dois)\n",
    "\n",
    "needed_dois_list = list(all_needed_dois)\n",
    "print(f\"Fetching text for {len(needed_dois_list):,} unique DOIs from DB...\")\n",
    "\n",
    "doi_to_abstract_text = {}\n",
    "\n",
    "def get_abstracts_from_db(dois):\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        chunk_size = 1000\n",
    "        for i in tqdm(range(0, len(dois), chunk_size), desc=\"Querying DB\"):\n",
    "            chunk = dois[i:i+chunk_size]\n",
    "            placeholders = ','.join('?' for _ in chunk)\n",
    "            query = f\"SELECT doi, abstract FROM papers WHERE doi IN ({placeholders})\"\n",
    "            cursor = conn.execute(query, chunk)\n",
    "            for row in cursor:\n",
    "                doi_to_abstract_text[row[0]] = row[1]\n",
    "\n",
    "get_abstracts_from_db(needed_dois_list)\n",
    "print(f\"Retrieved {len(doi_to_abstract_text):,} abstracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64649c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. テキストのクリーニング ---\n",
    "# (notebooks/17c と同じロジック)\n",
    "\n",
    "STOP_WORDS = [\n",
    "    'introduction', 'keywords', 'key words', 'references', 'acknowledgments',\n",
    "    'acknowledgements', 'bibliography', 'pubmed abstract', 'publisher full text', 'full text'\n",
    "]\n",
    "STOP_PATTERN = re.compile(r'\\b(' + '|'.join(STOP_WORDS) + r')\\b', re.IGNORECASE)\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_PATTERN = re.compile(r'\\S*@\\S*\\s?')\n",
    "NON_ASCII_PATTERN = re.compile(r'[^\\x00-\\x7F]+')\n",
    "SPECIAL_CHARS_PATTERN = re.compile(r'[^\\w\\s\\.\\,\\!\\?\\-\\'\\(\\)\\[\\]\\{\\}\\<\\>\\/\\=\\+\\*\\%]')\n",
    "MAX_CHAR_LIMIT = 3000\n",
    "\n",
    "def clean_retrieved_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # 1. 本文切り捨て\n",
    "    match = STOP_PATTERN.search(text)\n",
    "    if match: text = text[:match.start()]\n",
    "    # 2. ノイズ削除\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = EMAIL_PATTERN.sub('', text)\n",
    "    text = NON_ASCII_PATTERN.sub('', text)\n",
    "    text = SPECIAL_CHARS_PATTERN.sub('', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # 3. 文字数制限\n",
    "    return text[:MAX_CHAR_LIMIT]\n",
    "\n",
    "print(\"Cleaning function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70981cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing dataset with 1:3 ratio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dd201eec614127949cf335f3d98da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Pairs:   0%|          | 0/7013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Construction Complete ---\n",
      "Total rows: 28,052\n",
      "Label distribution:\n",
      "label\n",
      "0    21039\n",
      "1     7013\n",
      "Name: count, dtype: int64\n",
      "Saving to data/processed/training_dataset_hard_negatives_1to3.csv...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. 最終データセットの構築 ---\n",
    "final_rows = []\n",
    "\n",
    "print(f\"Constructing dataset with 1:{NEG_RATIO} ratio...\")\n",
    "\n",
    "# 全正例ペアに対してループ\n",
    "for idx, row in tqdm(df_positives.iterrows(), total=len(df_positives), desc=\"Building Pairs\"):\n",
    "    anchor = row['abstract_a']\n",
    "    positive = row['abstract_b']\n",
    "    data_paper_doi = row['data_paper_doi']\n",
    "    \n",
    "    # 1. 正例ペアを追加\n",
    "    final_rows.append({\n",
    "        'abstract_a': anchor,\n",
    "        'abstract_b': positive,\n",
    "        'label': 1,\n",
    "        'data_paper_doi': data_paper_doi\n",
    "    })\n",
    "    \n",
    "    # 2. 負例ペア (Hard Negative) を3つ追加\n",
    "    # このアンカーに対する候補DOIリスト\n",
    "    candidate_dois = anchor_to_hard_neg_dois.get(anchor, [])\n",
    "    \n",
    "    # このアンカーに対する「正解セット」 (フィルタリング用)\n",
    "    true_positives = anchor_to_positives.get(anchor, set())\n",
    "    \n",
    "    added_negatives = 0\n",
    "    for neg_doi in candidate_dois:\n",
    "        if added_negatives >= NEG_RATIO:\n",
    "            break\n",
    "            \n",
    "        # テキスト取得\n",
    "        raw_text = doi_to_abstract_text.get(neg_doi, \"\")\n",
    "        if not raw_text: continue\n",
    "        \n",
    "        # クリーニング\n",
    "        neg_text = clean_retrieved_text(raw_text)\n",
    "        if len(neg_text) < 50: continue # 短すぎるものは除外\n",
    "        \n",
    "        # --- フィルタリング (自分自身 or 正解 との一致チェック) ---\n",
    "        if neg_text == anchor: continue\n",
    "        if neg_text in true_positives: continue\n",
    "        \n",
    "        # 合格 -> 負例ペアとして追加\n",
    "        final_rows.append({\n",
    "            'abstract_a': anchor,\n",
    "            'abstract_b': neg_text,\n",
    "            'label': 0,\n",
    "            'data_paper_doi': None\n",
    "        })\n",
    "        added_negatives += 1\n",
    "    \n",
    "    # もしHard Negativeが足りない場合（稀ですが）、ランダム負例で埋める\n",
    "    # (ここでは簡易的に、直前の負例を複製するか、スキップする)\n",
    "    # 今回はスキップ（比率が厳密でなくても、HardNegがあることが重要）\n",
    "\n",
    "df_final = pd.DataFrame(final_rows)\n",
    "\n",
    "print(\"\\n--- Dataset Construction Complete ---\")\n",
    "print(f\"Total rows: {len(df_final):,}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df_final['label'].value_counts())\n",
    "\n",
    "# 保存\n",
    "print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

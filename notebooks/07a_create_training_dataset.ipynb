{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dcc613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kurokawa\\Project\\s2orcRanker\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Training Dataset (Abstract Version) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Positive Pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:00<00:00, 4533.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7,063 positive pairs.\n",
      "Generated 28,252 negative pairs.\n",
      "Fetching abstracts for 29,523 unique papers...\n",
      "Saving final dataset with 35,315 pairs to ../data/processed/training_dataset_abstract.csv...\n",
      "\n",
      "--- Process Complete ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_a</th>\n",
       "      <th>abstract_b</th>\n",
       "      <th>label</th>\n",
       "      <th>data_paper_doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>The modern unambiguous climate change reveals ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>Outdoor thermal comfort (OTC) surveys require ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The modern unambiguous climate change reveals ...</td>\n",
       "      <td>Outdoor thermal comfort (OTC) surveys require ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>In the months of March-June, India experiences...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We present climatology and trends of the UTCI ...</td>\n",
       "      <td>Extreme compound heatwaves (ECHWs) have the po...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1002/GDJ3.102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          abstract_a  \\\n",
       "0  We present climatology and trends of the UTCI ...   \n",
       "1  We present climatology and trends of the UTCI ...   \n",
       "2  The modern unambiguous climate change reveals ...   \n",
       "3  We present climatology and trends of the UTCI ...   \n",
       "4  We present climatology and trends of the UTCI ...   \n",
       "\n",
       "                                          abstract_b  label    data_paper_doi  \n",
       "0  The modern unambiguous climate change reveals ...      1  10.1002/GDJ3.102  \n",
       "1  Outdoor thermal comfort (OTC) surveys require ...      1  10.1002/GDJ3.102  \n",
       "2  Outdoor thermal comfort (OTC) surveys require ...      1  10.1002/GDJ3.102  \n",
       "3  In the months of March-June, India experiences...      1  10.1002/GDJ3.102  \n",
       "4  Extreme compound heatwaves (ECHWs) have the po...      1  10.1002/GDJ3.102  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Step 1: è¨­å®š ---\n",
    "DB_PATH = \"../data/processed/s2orc_filtered.db\"\n",
    "# è¨“ç·´ç”¨ã«åˆ†å‰²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿è«–æ–‡ã®ãƒªã‚¹ãƒˆï¼ˆ150ä»¶ï¼‰\n",
    "TRAINING_DATAPAPERS_FILE = \"../data/datapapers/sampled/training_data_papers_50.csv\"\n",
    "OUTPUT_FILE = \"../data/processed/training_dataset_abstract.csv\"\n",
    "NEGATIVE_SAMPLE_RATIO = 4 # æ­£ä¾‹1ã«å¯¾ã™ã‚‹è² ä¾‹ã®æ•°\n",
    "\n",
    "def create_training_dataset_abstract():\n",
    "    \"\"\"\n",
    "    äººé–“ãŒã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸçµæœã«åŸºã¥ãã€\n",
    "    ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    print(\"--- Creating Final Training Dataset (Abstract Version) ---\")\n",
    "\n",
    "    if not os.path.exists(TRAINING_DATAPAPERS_FILE):\n",
    "        print(f\"âŒ Error: Training data paper file not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            \n",
    "            # --- 1. è¨“ç·´ç”¨ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡DOIãƒªã‚¹ãƒˆã‚’å–å¾— ---\n",
    "            df_train_papers = pd.read_csv(TRAINING_DATAPAPERS_FILE)\n",
    "            train_dois = tuple(df_train_papers['cited_datapaper_doi'].unique())\n",
    "            \n",
    "            # --- 2. æ­£ä¾‹ãƒšã‚¢ã®å…ƒã¨ãªã‚‹è«–æ–‡ãƒªã‚¹ãƒˆã‚’å–å¾— ---\n",
    "            placeholders = ','.join('?' for _ in train_dois)\n",
    "            query = f\"\"\"\n",
    "                SELECT\n",
    "                    cited_datapaper_doi,\n",
    "                    citing_doi,\n",
    "                    human_annotation_status\n",
    "                FROM\n",
    "                    positive_candidates\n",
    "                WHERE\n",
    "                    cited_datapaper_doi IN ({placeholders})\n",
    "                    AND (human_annotation_status = 1 OR (llm_annotation_status = 1 AND human_annotation_status = 0))\n",
    "            \"\"\"\n",
    "            df_candidates = pd.read_sql_query(query, conn, params=train_dois)\n",
    "\n",
    "            # --- 3. æ­£ä¾‹ãƒšã‚¢ã®ä½œæˆ (DOIã®çµ„ã¿åˆã‚ã›) ---\n",
    "            positive_pairs = []\n",
    "            grouped = df_candidates.groupby('cited_datapaper_doi')\n",
    "            \n",
    "            for data_paper_doi, group in tqdm(grouped, desc=\"Generating Positive Pairs\"):\n",
    "                human_used_dois = group[group['human_annotation_status'] == 1]['citing_doi'].tolist()\n",
    "                llm_used_dois = group[group['human_annotation_status'] == 0]['citing_doi'].tolist() # ç™½\n",
    "                \n",
    "                if not human_used_dois:\n",
    "                    continue \n",
    "\n",
    "                # (ç·‘, ç·‘) ãƒšã‚¢\n",
    "                for pair in itertools.combinations(human_used_dois, 2):\n",
    "                    positive_pairs.append({\n",
    "                        'doi_a': pair[0], \n",
    "                        'doi_b': pair[1], \n",
    "                        'label': 1,\n",
    "                        'data_paper_doi': data_paper_doi\n",
    "                    })\n",
    "                    \n",
    "                # (ç·‘, ç™½) ãƒšã‚¢\n",
    "                for human_doi in human_used_dois:\n",
    "                    for llm_doi in llm_used_dois:\n",
    "                        positive_pairs.append({\n",
    "                            'doi_a': human_doi, \n",
    "                            'doi_b': llm_doi, \n",
    "                            'label': 1,\n",
    "                            'data_paper_doi': data_paper_doi\n",
    "                        })\n",
    "\n",
    "            df_positive = pd.DataFrame(positive_pairs)\n",
    "            if df_positive.empty:\n",
    "                print(\"No positive pairs were generated.\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Generated {len(df_positive):,} positive pairs.\")\n",
    "            \n",
    "            # --- 4. è² ä¾‹ãƒšã‚¢ã®ä½œæˆ (Easy Negative) ---\n",
    "            num_negatives_to_sample = len(df_positive) * NEGATIVE_SAMPLE_RATIO\n",
    "            positive_dois = set(df_positive['doi_a']) | set(df_positive['doi_b'])\n",
    "            \n",
    "            # ã‚¢ãƒ³ã‚«ãƒ¼è«–æ–‡ï¼ˆdoi_aï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶\n",
    "            anchor_dois_for_negative = df_positive['doi_a'].sample(n=num_negatives_to_sample, replace=True).tolist()\n",
    "            \n",
    "            # DBã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«è«–æ–‡ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            # `papers`ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç›´æ¥ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å–å¾—ã§ãã‚‹è«–æ–‡ã‚’å¯¾è±¡ã«ã™ã‚‹\n",
    "            query_random = f\"\"\"\n",
    "                SELECT doi FROM papers \n",
    "                WHERE doi NOT IN (SELECT doi FROM full_texts) \n",
    "                  AND abstract IS NOT NULL AND abstract != ''\n",
    "                ORDER BY RANDOM() \n",
    "                LIMIT {num_negatives_to_sample * 2}\n",
    "            \"\"\"\n",
    "            df_random_papers = pd.read_sql_query(query_random, conn)\n",
    "            random_dois = df_random_papers[~df_random_papers['doi'].isin(positive_dois)]['doi'].tolist()\n",
    "            \n",
    "            if len(random_dois) < num_negatives_to_sample:\n",
    "                print(f\"Warning: Could only sample {len(random_dois)} negative papers.\")\n",
    "                num_negatives_to_sample = len(random_dois)\n",
    "            \n",
    "            negative_pairs = []\n",
    "            for i in range(num_negatives_to_sample):\n",
    "                negative_pairs.append({\n",
    "                    'doi_a': anchor_dois_for_negative[i], \n",
    "                    'doi_b': random_dois[i], \n",
    "                    'label': 0,\n",
    "                    'data_paper_doi': None # è² ä¾‹ãªã®ã§è©²å½“ãªã—\n",
    "                })\n",
    "            \n",
    "            df_negative = pd.DataFrame(negative_pairs)\n",
    "            print(f\"Generated {len(df_negative):,} negative pairs.\")\n",
    "\n",
    "            # --- 5. å…¨ãƒšã‚¢ã®çµåˆã¨ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã®å–å¾— ---\n",
    "            df_final_pairs = pd.concat([df_positive, df_negative]).reset_index(drop=True)\n",
    "            \n",
    "            all_dois = set(df_final_pairs['doi_a']) | set(df_final_pairs['doi_b'])\n",
    "            \n",
    "            print(f\"Fetching abstracts for {len(all_dois):,} unique papers...\")\n",
    "            query_texts = f\"SELECT doi, abstract FROM papers WHERE doi IN ({','.join('?'*len(all_dois))})\"\n",
    "            df_texts = pd.read_sql_query(query_texts, conn, params=list(all_dois))\n",
    "            text_map = dict(zip(df_texts['doi'], df_texts['abstract']))\n",
    "\n",
    "            # --- 6. æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã¨ä¿å­˜ ---\n",
    "            df_final_dataset = pd.DataFrame()\n",
    "            df_final_dataset['abstract_a'] = df_final_pairs['doi_a'].map(text_map)\n",
    "            df_final_dataset['abstract_b'] = df_final_pairs['doi_b'].map(text_map)\n",
    "            df_final_dataset['label'] = df_final_pairs['label']\n",
    "            df_final_dataset['data_paper_doi'] = df_final_pairs['data_paper_doi']\n",
    "            \n",
    "            # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãŒå–å¾—ã§ããªã‹ã£ãŸè¡Œã‚’å‰Šé™¤\n",
    "            df_final_dataset = df_final_dataset.dropna(subset=['abstract_a', 'abstract_b']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"Saving final dataset with {len(df_final_dataset):,} pairs to {OUTPUT_FILE}...\")\n",
    "            df_final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "            \n",
    "            print(\"\\n--- Process Complete ---\")\n",
    "            display(df_final_dataset.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ’¥ An error occurred: {e}\")\n",
    "\n",
    "# --- å®Ÿè¡Œ ---\n",
    "create_training_dataset_abstract()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

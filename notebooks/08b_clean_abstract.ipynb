{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0b36b6",
   "metadata": {},
   "source": [
    "## 長いアブストラクトの処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd411f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "INPUT_FILE = \"data/processed/training_dataset_abstract.csv\"\n",
    "OUTPUT_FILE = \"data/processed/training_dataset_abstract_cleaned.csv\"\n",
    "\n",
    "# アブストラクトの終わり（＝本文の始まり）を示す可能性が非常に高いキーワード\n",
    "# （大文字と小文字を区別しないように、すべて小文字で定義）\n",
    "STOP_WORDS = [\n",
    "    'introduction',\n",
    "    'keywords',\n",
    "    'key words',\n",
    "    'references',\n",
    "    'acknowledgments',\n",
    "    'acknowledgements',\n",
    "    'bibliography',\n",
    "    'pubmed abstract',\n",
    "    'publisher full text',\n",
    "    'full text'\n",
    "]\n",
    "\n",
    "# 正規表現パターンを作成 ( | で区切る)\n",
    "# \\b は単語の境界を意味し、\"introduction\" が \"subintroduction\" にマッチしないようにする\n",
    "STOP_PATTERN = re.compile(r'\\b(' + '|'.join(STOP_WORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "print(\"Settings and stop words defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5bc7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. クリーニング関数の定義 ---\n",
    "\n",
    "def clean_abstract(text):\n",
    "    \"\"\"\n",
    "    テキストを受け取り、ストップワードが最初に出現した位置で\n",
    "    テキストを切り捨てる（クリーニングする）\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # ストップワードを検索\n",
    "    match = STOP_PATTERN.search(text)\n",
    "    \n",
    "    if match:\n",
    "        # ストップワードが見つかった場合\n",
    "        # その単語が始まる位置（match.start()）までのテキストを返す\n",
    "        return text[:match.start()]\n",
    "    else:\n",
    "        # ストップワードが見つからなかった場合\n",
    "        # テキストは汚染されていないと判断し、そのまま返す\n",
    "        return text\n",
    "\n",
    "# テスト実行\n",
    "test_text = \"This is abstract. \\n\\n Keywords: NLP, ML. \\n\\n This is body...\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Cleaned:  {clean_abstract(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d185e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. データセットの読み込みと前処理の実行 ---\n",
    "print(f\"Loading dataset from {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b'])\n",
    "\n",
    "print(\"Applying cleaning function to 'abstract_a'...\")\n",
    "df['abstract_a_cleaned'] = df['abstract_a'].progress_apply(clean_abstract)\n",
    "\n",
    "print(\"Applying cleaning function to 'abstract_b'...\")\n",
    "df['abstract_b_cleaned'] = df['abstract_b'].progress_apply(clean_abstract)\n",
    "\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb9580",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. 最終データセットの保存 ---\n",
    "\n",
    "# 元のアブストラクトを削除し、クリーニング後のものだけを残す\n",
    "df_final = df.drop(columns=['abstract_a', 'abstract_b'])\n",
    "df_final = df_final.rename(columns={\n",
    "    'abstract_a_cleaned': 'abstract_a',\n",
    "    'abstract_b_cleaned': 'abstract_b'\n",
    "})\n",
    "\n",
    "# 必要なカラムの順序に並び替え\n",
    "df_final = df_final[['abstract_a', 'abstract_b', 'label', 'data_paper_doi']]\n",
    "\n",
    "print(f\"Saving cleaned dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Save complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a703f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fixing Evaluation Set (FINAL ATTEMPT) ---\n",
      "Output file: data/datapapers/sampled/evaluation_data_papers_50_v2.csv\n",
      "\n",
      "Loading current evaluation set: data/datapapers/sampled/evaluation_data_papers_50_v2.csv\n",
      "Found 48 valid papers (Human Used >= 2). Need 2 more.\n",
      "Total DOIs already in use (all known files): 203\n",
      "Available 'FRESH' pool size: 518\n",
      "\n",
      "==================================================\n",
      "✅ Selected 2 new 'FRESH' data papers:\n",
      "| cited_datapaper_doi       |   llm_used_count |\n",
      "|:--------------------------|-----------------:|\n",
      "| 10.1016/J.DIB.2018.11.111 |                5 |\n",
      "| 10.1016/J.DIB.2020.106438 |                3 |\n",
      "==================================================\n",
      "\n",
      "Updating DB for 2 new papers...\n",
      "✅ DB Update Complete. 8 rows affected.\n",
      "\n",
      "Verifying new papers (post-update)...\n",
      "  -> 10.1016/J.DIB.2018.11.111 is VALID (Count: 5)\n",
      "  -> 10.1016/J.DIB.2020.106438 is VALID (Count: 3)\n",
      "✅ All new replacements are valid.\n",
      "\n",
      "Saving final evaluation set with 50 papers to NEW FILE: data/datapapers/sampled/evaluation_data_papers_50_v2.csv...\n",
      "✅ New CSV file created successfully.\n",
      "\n",
      "--- Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "\n",
    "# ★全ての既知のファイルをリストアップ\n",
    "ORIGINAL_EVAL_FILE = \"data/datapapers/sampled/evaluation_data_papers_50.csv\"\n",
    "CURRENT_EVAL_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "TRAINING_PAPERS_FILE = \"data/datapapers/sampled/training_data_papers_50.csv\"\n",
    "\n",
    "# 新しく作成するファイル\n",
    "FINAL_EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "\n",
    "TARGET_EVAL_SIZE = 50 \n",
    "print(f\"--- Fixing Evaluation Set (FINAL ATTEMPT) ---\")\n",
    "print(f\"Output file: {FINAL_EVAL_PAPERS_FILE}\")\n",
    "\n",
    "# --- 2. 現状の評価セット（_v2.csv）から「有効なもの」だけを取得 ---\n",
    "print(f\"\\nLoading current evaluation set: {CURRENT_EVAL_FILE}\")\n",
    "df_eval_current = pd.read_csv(CURRENT_EVAL_FILE)\n",
    "current_eval_dois = tuple(df_eval_current['cited_datapaper_doi'].unique())\n",
    "\n",
    "valid_eval_dois = []\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    for doi in current_eval_dois:\n",
    "        query_gt = \"SELECT COUNT(citing_doi) FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        count = conn.execute(query_gt, (doi,)).fetchone()[0]\n",
    "        if count >= 2:\n",
    "            valid_eval_dois.append(doi)\n",
    "\n",
    "df_eval_valid = df_eval_current[df_eval_current['cited_datapaper_doi'].isin(valid_eval_dois)].copy()\n",
    "n_needed = TARGET_EVAL_SIZE - len(df_eval_valid)\n",
    "print(f\"Found {len(df_eval_valid)} valid papers (Human Used >= 2). Need {n_needed} more.\")\n",
    "\n",
    "# --- 3. ★「完全な」使用済みDOIプールを作成 ---\n",
    "df_train = pd.read_csv(TRAINING_PAPERS_FILE)\n",
    "df_orig_eval = pd.read_csv(ORIGINAL_EVAL_FILE)\n",
    "\n",
    "# 訓練(150) + _v2(50) + オリジナル(54) の全てのDOIを結合\n",
    "used_dois = set(df_train['cited_datapaper_doi']) \\\n",
    "            .union(set(df_eval_current['cited_datapaper_doi'])) \\\n",
    "            .union(set(df_orig_eval['cited_datapaper_doi']))\n",
    "print(f\"Total DOIs already in use (all known files): {len(used_dois)}\")\n",
    "\n",
    "# --- 4. 不足分の候補を選定 ---\n",
    "if n_needed > 0:\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        # 元の母集団（LLM=Usedが2件以上）の全リストを取得\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "                cited_datapaper_doi,\n",
    "                COUNT(citing_doi) AS llm_used_count\n",
    "            FROM positive_candidates\n",
    "            WHERE llm_annotation_status = 1\n",
    "            GROUP BY cited_datapaper_doi\n",
    "            HAVING COUNT(citing_doi) >= 2;\n",
    "        \"\"\"\n",
    "        df_all_eligible = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # 未使用のDOIプールを作成（★これが「真の」未使用プール）\n",
    "        df_available = df_all_eligible[~df_all_eligible['cited_datapaper_doi'].isin(used_dois)]\n",
    "        print(f\"Available 'FRESH' pool size: {len(df_available)}\")\n",
    "        \n",
    "        if len(df_available) < n_needed:\n",
    "            print(f\"❌ Error: Not enough 'FRESH' papers in the pool.\")\n",
    "        else:\n",
    "            # 被引用数が多い順にソートし、上位n_needed件を取得\n",
    "            df_replacements = df_available.sort_values(by='llm_used_count', ascending=False).head(n_needed)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"✅ Selected {n_needed} new 'FRESH' data papers:\")\n",
    "            print(df_replacements.to_markdown(index=False))\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # --- 5. DBの一括更新 ---\n",
    "            new_dois_to_update = tuple(df_replacements['cited_datapaper_doi'].unique())\n",
    "            print(f\"\\nUpdating DB for {len(new_dois_to_update)} new papers...\")\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            # ★これらの論文は「FRESH」なので、human_status=0 であることが保証されている\n",
    "            update_query = f\"\"\"\n",
    "                UPDATE positive_candidates\n",
    "                SET human_annotation_status = 1\n",
    "                WHERE cited_datapaper_doi IN ({','.join('?' for _ in new_dois_to_update)})\n",
    "                  AND llm_annotation_status = 1\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query, new_dois_to_update)\n",
    "            updated_rows = cursor.rowcount\n",
    "            conn.commit()\n",
    "            print(f\"✅ DB Update Complete. {updated_rows:,} rows affected.\")\n",
    "\n",
    "            # --- 6. 追加した {n_needed} 件の「検証」 ---\n",
    "            print(\"\\nVerifying new papers (post-update)...\")\n",
    "            final_valid_replacements = []\n",
    "            for doi in new_dois_to_update:\n",
    "                query_gt = \"SELECT COUNT(citing_doi) FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "                count = conn.execute(query_gt, (doi,)).fetchone()[0]\n",
    "                if count >= 2:\n",
    "                    final_valid_replacements.append(doi)\n",
    "                    print(f\"  -> {doi} is VALID (Count: {count})\")\n",
    "                else:\n",
    "                    print(f\"  -> ⚠️ WARNING: Replacement paper {doi} still has < 2 Human_Used count ({count}).\")\n",
    "            \n",
    "            if len(final_valid_replacements) == n_needed:\n",
    "                print(\"✅ All new replacements are valid.\")\n",
    "            \n",
    "            # --- 7. 最終的なCSVの作成と保存 ---\n",
    "            df_final_eval_set = pd.concat([df_eval_valid, df_replacements], ignore_index=True)\n",
    "            print(f\"\\nSaving final evaluation set with {len(df_final_eval_set)} papers to NEW FILE: {FINAL_EVAL_PAPERS_FILE}...\")\n",
    "            df_final_eval_set.to_csv(FINAL_EVAL_PAPERS_FILE, index=False)\n",
    "            print(\"✅ New CSV file created successfully.\")\n",
    "            print(\"\\n--- Process Complete ---\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nEvaluation set already has {len(df_eval_valid)} valid papers. Saving as new file...\")\n",
    "    df_eval_valid.to_csv(FINAL_EVAL_PAPERS_FILE, index=False)\n",
    "    print(\"✅ New CSV file created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31423d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    BertPreTrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import accelerate\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a572be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'CrossEncoderMarginModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. カスタムモデルクラスの定義 ---\n",
    "# (C-Encoder (Margin) の定義をロードのために再掲)\n",
    "class CrossEncoderMarginModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(CrossEncoderMarginModel, self).__init__(config)\n",
    "        self.scorer = AutoModelForSequenceClassification.from_config(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        # 推論時はscorerだけを使用\n",
    "        return self.scorer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(\"Custom model class 'CrossEncoderMarginModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3164b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model from: models/cencoder_margin_v2/best_model\n",
      "Total papers in DB: 11,619,136\n",
      "Resources loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 設定とリソースのロード ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "\n",
    "# ▼▼▼ 訓練済みのC-Encoder (Margin) モデルのパス ▼▼▼\n",
    "TRAINED_MODEL_PATH = \"models/cencoder_margin_v2/best_model\"\n",
    "MODEL_CHECKPOINT = \"allenai/longformer-base-4096\"\n",
    "\n",
    "# 評価クエリ\n",
    "EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50.csv\"\n",
    "\n",
    "# 推定用の設定\n",
    "MAX_LENGTH = 2048\n",
    "# (実験計画でのバッチサイズを使用)\n",
    "ESTIMATION_BATCH_SIZE = 16 \n",
    "\n",
    "print(f\"Loading tokenizer & model from: {TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "model = CrossEncoderMarginModel.from_pretrained(TRAINED_MODEL_PATH, num_labels=1).to(device)\n",
    "model.eval()\n",
    "\n",
    "# DBの総ベクトル数\n",
    "try:\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        TOTAL_DB_PAPERS = conn.execute(\"SELECT COUNT(doi) FROM papers WHERE abstract IS NOT NULL AND abstract != ''\").fetchone()[0]\n",
    "    print(f\"Total papers in DB: {TOTAL_DB_PAPERS:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not count DB: {e}\")\n",
    "    TOTAL_DB_PAPERS = 11619136 # フォールバック\n",
    "\n",
    "print(\"Resources loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d71c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sample data...\n",
      "Sample query: The increase in globalization has led to the redefinition of the tax policy perceptions of countries...\n",
      "Sample candidates count: 16\n"
     ]
    }
   ],
   "source": [
    "# --- 4. サンプルデータの取得 ---\n",
    "def get_sample_data(db_path, eval_papers_file, batch_size):\n",
    "    \"\"\"\n",
    "    1件のクエリ と 1バッチ分の候補論文 をDBから取得\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        # 1. 1件のクエリを取得\n",
    "        df_eval_papers = pd.read_csv(eval_papers_file)\n",
    "        sample_data_paper_doi = df_eval_papers.iloc[0]['cited_datapaper_doi']\n",
    "        \n",
    "        query_gt = \"SELECT citing_doi FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        query_doi = conn.execute(query_gt, (sample_data_paper_doi,)).fetchone()[0]\n",
    "        query_abstract = conn.execute(\"SELECT abstract FROM papers WHERE doi = ?\", (query_doi,)).fetchone()[0]\n",
    "        \n",
    "        # 2. 1バッチ分の候補論文を適当に取得\n",
    "        candidates_abstracts = []\n",
    "        rows = conn.execute(f\"SELECT abstract FROM papers WHERE abstract IS NOT NULL AND abstract != '' LIMIT {batch_size}\").fetchall()\n",
    "        candidates_abstracts = [row[0] for row in rows]\n",
    "        \n",
    "    return query_abstract, candidates_abstracts\n",
    "\n",
    "print(\"Fetching sample data...\")\n",
    "query_text, candidate_texts = get_sample_data(DB_PATH, EVAL_PAPERS_FILE, ESTIMATION_BATCH_SIZE)\n",
    "print(f\"Sample query: {query_text[:100]}...\")\n",
    "print(f\"Sample candidates count: {len(candidate_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3501601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Estimating Inference Time ---\n",
      "Running inference for 1 batch (Size=16)...\n",
      "Time taken for 1 batch: 1.1927 seconds\n",
      "\n",
      "==================================================\n",
      "--- Total Time Estimation ---\n",
      "Total papers in DB: 11,619,136\n",
      "Batch size: 16\n",
      "Total batches PER QUERY: 726,196\n",
      "Total queries to evaluate: 50\n",
      "\n",
      "--- PER QUERY (1件あたり) ---\n",
      "Estimated time per query: 866130.01 seconds\n",
      "Estimated time per query: 240.59 hours\n",
      "\n",
      "--- TOTAL (全50件) ---\n",
      "Estimated TOTAL time for 50 queries: 12029.58 hours\n",
      "Estimated TOTAL time for 50 queries: 501.23 days\n",
      "==================================================\n",
      "\n",
      "⚠️ 警告: 推定時間が1週間を超えています。\n",
      "Cross-Encoderモデルでの全DB（1160万件）評価は現実的ではありません。\n",
      "評価戦略を「リランキング評価」（上位1000件のみを評価）に見直すことを強く推奨します。\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 時間計測と総時間の推定 ---\n",
    "print(\"\\n--- Estimating Inference Time ---\")\n",
    "\n",
    "# 1. サンプルバッチをトークナイズ\n",
    "inputs = tokenizer(\n",
    "    [query_text] * ESTIMATION_BATCH_SIZE, # クエリをバッチサイズ分複製\n",
    "    candidate_texts,                     # 候補リスト\n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# 2. 1バッチの推論時間を計測\n",
    "print(f\"Running inference for 1 batch (Size={ESTIMATION_BATCH_SIZE})...\")\n",
    "torch.cuda.synchronize() # GPUの処理を待つ\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # C-Encoder (Margin) は .scorer を呼び出す\n",
    "    outputs = model.scorer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    # (Siameseモデルとは異なり、ここで重いLongformerが動く)\n",
    "\n",
    "torch.cuda.synchronize() # GPUの処理が終わるのを待つ\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "time_per_batch = end_time - start_time\n",
    "print(f\"Time taken for 1 batch: {time_per_batch:.4f} seconds\")\n",
    "\n",
    "# 3. 総時間の推定\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Total Time Estimation ---\")\n",
    "num_queries = 50 # 評価クエリの総数\n",
    "total_batches_per_query = (TOTAL_DB_PAPERS + ESTIMATION_BATCH_SIZE - 1) // ESTIMATION_BATCH_SIZE\n",
    "\n",
    "print(f\"Total papers in DB: {TOTAL_DB_PAPERS:,}\")\n",
    "print(f\"Batch size: {ESTIMATION_BATCH_SIZE}\")\n",
    "print(f\"Total batches PER QUERY: {total_batches_per_query:,}\")\n",
    "print(f\"Total queries to evaluate: {num_queries}\")\n",
    "\n",
    "# 1クエリあたりの総時間\n",
    "time_per_query_sec = time_per_batch * total_batches_per_query\n",
    "time_per_query_hours = time_per_query_sec / 3600\n",
    "\n",
    "print(\"\\n--- PER QUERY (1件あたり) ---\")\n",
    "print(f\"Estimated time per query: {time_per_query_sec:.2f} seconds\")\n",
    "print(f\"Estimated time per query: {time_per_query_hours:.2f} hours\")\n",
    "\n",
    "# 全評価にかかる総時間\n",
    "total_time_hours = time_per_query_hours * num_queries\n",
    "total_time_days = total_time_hours / 24\n",
    "\n",
    "print(\"\\n--- TOTAL (全50件) ---\")\n",
    "print(f\"Estimated TOTAL time for {num_queries} queries: {total_time_hours:.2f} hours\")\n",
    "print(f\"Estimated TOTAL time for {num_queries} queries: {total_time_days:.2f} days\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if total_time_days > 7:\n",
    "    print(\"\\n⚠️ 警告: 推定時間が1週間を超えています。\")\n",
    "    print(\"Cross-Encoderモデルでの全DB（1160万件）評価は現実的ではありません。\")\n",
    "    print(\"評価戦略を「リランキング評価」（上位1000件のみを評価）に見直すことを強く推奨します。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

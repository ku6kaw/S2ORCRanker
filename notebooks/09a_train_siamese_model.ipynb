{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhm6VcH4EE8I"
   },
   "source": [
    "## import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97450,
     "status": "ok",
     "timestamp": 1762635686688,
     "user": {
      "displayName": "くろかわ",
      "userId": "15316305676594698534"
     },
     "user_tz": -540
    },
    "id": "cLgcaxkE_UXx",
    "outputId": "65b89bff-b2a5-40fd-c57a-a6097326803c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功！Dockerコンテナ経由でGPUを認識しました: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "# Dockerコンテナが正しくGPUを認識していれば、ここでTrueと表示されます\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ 成功！Dockerコンテナ経由でGPUを認識しました: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ 失敗。GPUが認識されていません。DockerのGPU設定を確認してください。\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8UOND98EJn_"
   },
   "source": [
    "## setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "# DockerfileのWORKDIR (/app) からの相対パス\n",
    "TRAINING_FILE = \"data/processed/training_dataset_abstract.csv\"\n",
    "\n",
    "# 使用するベースモデル（SciBERT）\n",
    "MODEL_CHECKPOINT = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# 訓練済みモデルの保存先\n",
    "OUTPUT_MODEL_DIR = \"models/siamese_scibert_v1\"\n",
    "\n",
    "# モデルのハイパーパラメータ\n",
    "MAX_LENGTH = 512       # 入力トークンの最大長（アブストラクト1件あたり）\n",
    "BATCH_SIZE = 16        # GPUメモリに応じたバッチサイズ\n",
    "EPOCHS = 3             # 訓練エポック数\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vjbDOddEMBL"
   },
   "source": [
    "## モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'SiameseSciBERT' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. カスタムモデルクラスの定義 ---\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    \"\"\"アテンションマスクを考慮したMean Pooling層\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class SiameseSciBERT(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    手法1：二入力ベクトル比較型（Siamese-SciBERT）\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SiameseSciBERT, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.classifier = nn.Linear(config.hidden_size * 4, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    # ▼▼▼ 修正点: 引数名を変更 ▼▼▼\n",
    "    # (input_ids_a -> input_ids, attention_mask_a -> attention_mask)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        input_ids_b=None,\n",
    "        attention_mask_b=None,\n",
    "        labels=None,\n",
    "        **kwargs \n",
    "    ):\n",
    "        \n",
    "        # --- 1. エンコーダー層 ---\n",
    "        # ▼▼▼ 修正点: 引数名を変更 ▼▼▼\n",
    "        output_a = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "\n",
    "        # --- 2. プーリング層 ---\n",
    "        # ▼▼▼ 修正点: 引数名を変更 ▼▼▼\n",
    "        vec_x = self.pooler(output_a.last_hidden_state, attention_mask)\n",
    "        vec_y = self.pooler(output_b.last_hidden_state, attention_mask_b)\n",
    "\n",
    "        # --- 3. 特徴量エンジニアリング層 ---\n",
    "        diff = torch.abs(vec_x - vec_y)\n",
    "        prod = vec_x * vec_y\n",
    "        concatenated_features = torch.cat([vec_x, vec_y, diff, prod], dim=1)\n",
    "\n",
    "        # --- 4. 分類ヘッド ---\n",
    "        logits = self.classifier(concatenated_features)\n",
    "\n",
    "        # --- 5. 損失の計算 ---\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "print(\"Custom model class 'SiameseSciBERT' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDWbJ9jrESF1"
   },
   "source": [
    "## データロードとトークナイズ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 28252\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 7063\n",
      "    })\n",
      "})\n",
      "Initializing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset for Siamese model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae6b025b73b4c45aa1573756f1deb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/28252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b34afb7cd3424891eb77f449671b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. データセットの読み込みとトークン化 ---\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(TRAINING_FILE)\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b', 'label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "dataset_split = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_split['train'],\n",
    "    'validation': dataset_split['test']\n",
    "})\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_siamese_function(examples):\n",
    "    tokenized_a = tokenizer(examples[\"abstract_a\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    tokenized_b = tokenizer(examples[\"abstract_b\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    # ▼▼▼ 修正点: キー名を変更 ▼▼▼\n",
    "    # (\"input_ids_a\" -> \"input_ids\", \"attention_mask_a\" -> \"attention_mask\")\n",
    "    return {\n",
    "        \"input_ids\": tokenized_a[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_a[\"attention_mask\"],\n",
    "        \"input_ids_b\": tokenized_b[\"input_ids\"],\n",
    "        \"attention_mask_b\": tokenized_b[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing dataset for Siamese model...\")\n",
    "tokenized_datasets = dataset.map(tokenize_siamese_function, batched=True, num_proc=4)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"abstract_a\", \"abstract_b\", \"data_paper_doi\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1T60SXYEXIT"
   },
   "source": [
    "## モデルのロードと訓練設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom model: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SiameseSciBERT were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model loaded.\n",
      "Training arguments and metrics set.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. モデルのロードと訓練設定 ---\n",
    "print(f\"Loading custom model: {MODEL_CHECKPOINT}\")\n",
    "# 2クラス分類（label 0 or 1）のモデルとして、カスタムクラスをロード\n",
    "model = SiameseSciBERT.from_pretrained(MODEL_CHECKPOINT, num_labels=2).to(device)\n",
    "print(\"Custom model loaded.\")\n",
    "\n",
    "# 評価指標を計算する関数\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# 訓練の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=100, # 100ステップごとにログを表示\n",
    ")\n",
    "print(\"Training arguments and metrics set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRb5IvbqEbqB"
   },
   "source": [
    "## 訓練開始\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training (Siamese Model) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='5298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/5298 03:55 < 69:12:19, 0.02 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Model Training (Siamese Model) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Model Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 6. 訓練の開始 ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Model Training (Siamese Model) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV6txPRWEdJy"
   },
   "source": [
    "## モデルの保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. モデルの保存 ---\n",
    "print(\"Training complete. Saving best model...\")\n",
    "best_model_path = os.path.join(OUTPUT_MODEL_DIR, \"best_model\")\n",
    "trainer.save_model(best_model_path)\n",
    "\n",
    "# docker-compose.ymlのvolumes設定により、このコンテナ内の/app/models/への保存は、\n",
    "# 自動的にローカルPCの/models/フォルダにも反映（同期）されます。\n",
    "print(f\"Model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練結果の可視化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"--- Visualizing Training Results ---\")\n",
    "\n",
    "# 1. 訓練履歴(log_history)を取得\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# 2. ログをPandas DataFrameに変換\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "# 3. 訓練ログ(loss)と検証ログ(eval_lossなど)を分離\n",
    "df_train = df_log[df_log['loss'].notna()].copy()\n",
    "df_eval = df_log[df_log['eval_loss'].notna()].copy()\n",
    "\n",
    "# 'epoch'列を整数型に（表示のため）\n",
    "if 'epoch' in df_train.columns:\n",
    "    df_train['epoch'] = df_train['epoch'].astype(int)\n",
    "if 'epoch' in df_eval.columns:\n",
    "    df_eval['epoch'] = df_eval['epoch'].astype(int)\n",
    "\n",
    "# 4. グラフの描画 (2つのグラフを縦に並べる)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- グラフ1: 損失 (Loss) の推移 ---\n",
    "sns.lineplot(data=df_train, x='epoch', y='loss', label='Training Loss', ax=ax1, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_loss', label='Validation Loss', ax=ax1, marker='o')\n",
    "ax1.set_title('Training vs. Validation Loss', fontsize=16)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# --- グラフ2: 評価指標 (Metrics) の推移 ---\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_f1', label='Validation F1-Score', ax=ax2, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_accuracy', label='Validation Accuracy', ax=ax2, marker='o')\n",
    "ax2.set_title('Validation Metrics', fontsize=16)\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1) # 精度は0%から100%の範囲で表示\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- ▼▼▼ 修正点: best_metricの参照方法を変更 ▼▼▼ ---\n",
    "print(\"\\n--- Best Model Evaluation Metrics (from validation set) ---\")\n",
    "\n",
    "# df_eval (このセルの前半で作成した検証結果のDataFrame) を使用\n",
    "if not df_eval.empty:\n",
    "    # 'eval_loss' が最小だった行（=ベストモデル）を取得\n",
    "    best_run = df_eval.loc[df_eval['eval_loss'].idxmin()]\n",
    "    \n",
    "    print(f\"Best Epoch (based on min eval_loss): {best_run['epoch']}\")\n",
    "    print(f\"Best Validation Loss: {best_run['eval_loss']:.4f}\")\n",
    "    print(f\"Best Validation F1: {best_run['eval_f1']:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {best_run['eval_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"No evaluation steps were completed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNoDsEiplEKZuEZ6M6XVYXJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

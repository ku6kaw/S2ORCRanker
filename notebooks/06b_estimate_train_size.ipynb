{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab600b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Estimating Final Training Set Size (v2: Based on 1 manual check per group) ---\n",
      "\n",
      "==================================================\n",
      "--- Estimation Results for Training Dataset ---\n",
      "Ë®ìÁ∑¥„Éá„Éº„Çø‰ΩúÊàê„ÅÆËµ∑ÁÇπ„Å®„Å™„Çã„Éá„Éº„ÇøË´ñÊñáÊï∞ÔºàÂÖÉ„É™„Çπ„ÉàÔºâ: 150\n",
      "„ÅÜ„Å°„ÄÅÊ≠£‰æã„Éö„Ç¢„Çí‰ΩúÊàêÂèØËÉΩ„Å™„Éá„Éº„ÇøË´ñÊñá„ÅÆÊï∞ ('Used' >= 2): 150\n",
      "‚áí ÁõÆË¶ñÁ¢∫Ë™ç„ÅåÂøÖË¶Å„Å™Ë´ñÊñáÁ∑èÊï∞ÔºàÊúÄ‰ΩéÔºâ: 150‰ª∂\n",
      "--------------------------------------------------\n",
      "‰ΩúÊàêÂèØËÉΩ„Å™Ê≠£‰æã„Éö„Ç¢„ÅÆÁ∑èÊï∞: 1,438\n",
      "==================================================\n",
      "\n",
      "--- Total Dataset Size Estimation (Positive:Negative = 1:4) ---\n",
      "  - Negative Pairs to be sampled: 5,752\n",
      "  - Total Estimated Dataset Size: 7,190 pairs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Step 1: Ë®≠ÂÆö ---\n",
    "DB_PATH = \"../data/processed/s2orc_filtered.db\"\n",
    "# Ë©ï‰æ°Áî®50‰ª∂„Éê„Éº„Ç∏„Éß„É≥„ÅÆË®ìÁ∑¥„Éá„Éº„ÇøË´ñÊñá„É™„Çπ„Éà\n",
    "TRAINING_DATAPAPERS_FILE = \"../data/datapapers/sampled/training_data_papers_50.csv\"\n",
    "NEGATIVE_SAMPLE_RATIO = 4 # Ê≠£‰æã1„Å´ÂØæ„Åô„ÇãË≤†‰æã„ÅÆÊï∞\n",
    "\n",
    "# --- Step 2: Ë®ìÁ∑¥„Éá„Éº„Çø„Çª„ÉÉ„ÉàË¶èÊ®°„ÅÆË¶ãÁ©ç„ÇÇ„Çä ---\n",
    "\n",
    "def estimate_final_training_size_v2():\n",
    "    \"\"\"\n",
    "    ÁõÆË¶ñÁ¢∫Ë™ç„ÇíÂêÑ„Ç∞„É´„Éº„Éó1‰ª∂Ë°å„ÅÜÂâçÊèê„Åß„ÄÅÊßãÁØâÂèØËÉΩ„Å™Ë®ìÁ∑¥„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË¶èÊ®°„ÇíË¶ãÁ©ç„ÇÇ„Çã„ÄÇ\n",
    "    \"\"\"\n",
    "    print(\"--- Estimating Final Training Set Size (v2: Based on 1 manual check per group) ---\")\n",
    "\n",
    "    if not os.path.exists(DB_PATH) or not os.path.exists(TRAINING_DATAPAPERS_FILE):\n",
    "        print(f\"‚ùå Error: Database or training data paper file not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            # 1. Ë®ìÁ∑¥Áî®„ÅÆ„Éá„Éº„ÇøË´ñÊñáDOI„É™„Çπ„Éà„ÇíÂèñÂæó\n",
    "            df_train_papers = pd.read_csv(TRAINING_DATAPAPERS_FILE)\n",
    "            train_dois = tuple(df_train_papers['cited_datapaper_doi'].unique()) # SQLÁî®„Å´„Çø„Éó„É´„Å´Â§âÊèõ\n",
    "\n",
    "            # 2. ÂêÑË®ìÁ∑¥Áî®„Éá„Éº„ÇøË´ñÊñá„Å´„Å§„ÅÑ„Å¶„ÄÅ'Used'„Å®Âà§ÂÆö„Åï„Çå„ÅüÂºïÁî®Ë´ñÊñá„ÅÆÊï∞„Çí„Ç´„Ç¶„É≥„Éà\n",
    "            placeholders = ','.join('?' for _ in train_dois)\n",
    "            query = f\"\"\"\n",
    "                SELECT\n",
    "                    cited_datapaper_doi,\n",
    "                    COUNT(citing_doi) AS used_count\n",
    "                FROM\n",
    "                    positive_candidates\n",
    "                WHERE\n",
    "                    llm_annotation_status = 1\n",
    "                    AND cited_datapaper_doi IN ({placeholders})\n",
    "                GROUP BY\n",
    "                    cited_datapaper_doi\n",
    "                HAVING\n",
    "                    COUNT(citing_doi) >= 2; -- „Éö„Ç¢‰ΩúÊàê„Å´„ÅØÊúÄ‰Ωé2‰ª∂ÂøÖË¶Å\n",
    "            \"\"\"\n",
    "            df_used_counts = pd.read_sql_query(query, conn, params=train_dois)\n",
    "\n",
    "            if df_used_counts.empty:\n",
    "                print(\"\\nNo data papers in the training set can form positive pairs (need >= 2 'Used' papers).\")\n",
    "                return\n",
    "            \n",
    "            # 3. ‰ΩúÊàêÂèØËÉΩ„Å™„Éö„Ç¢Êï∞„ÇíË®àÁÆó (k‰ª∂„Åã„Çâk-1„Éö„Ç¢)\n",
    "            df_used_counts['creatable_pairs'] = df_used_counts['used_count'] - 1\n",
    "            \n",
    "            total_positive_pairs = df_used_counts['creatable_pairs'].sum()\n",
    "            num_groups_to_check = len(df_used_counts) # ÁõÆË¶ñÁ¢∫Ë™ç„ÅåÂøÖË¶Å„Å™„Ç∞„É´„Éº„ÉóÊï∞\n",
    "\n",
    "            # --- 4. ÁµêÊûú„ÅÆÂ†±Âëä ---\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"--- Estimation Results for Training Dataset ---\")\n",
    "            print(f\"Ë®ìÁ∑¥„Éá„Éº„Çø‰ΩúÊàê„ÅÆËµ∑ÁÇπ„Å®„Å™„Çã„Éá„Éº„ÇøË´ñÊñáÊï∞ÔºàÂÖÉ„É™„Çπ„ÉàÔºâ: {len(train_dois):,}\")\n",
    "            print(f\"„ÅÜ„Å°„ÄÅÊ≠£‰æã„Éö„Ç¢„Çí‰ΩúÊàêÂèØËÉΩ„Å™„Éá„Éº„ÇøË´ñÊñá„ÅÆÊï∞ ('Used' >= 2): {num_groups_to_check:,}\")\n",
    "            print(f\"‚áí ÁõÆË¶ñÁ¢∫Ë™ç„ÅåÂøÖË¶Å„Å™Ë´ñÊñáÁ∑èÊï∞ÔºàÊúÄ‰ΩéÔºâ: {num_groups_to_check:,}‰ª∂\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"‰ΩúÊàêÂèØËÉΩ„Å™Ê≠£‰æã„Éö„Ç¢„ÅÆÁ∑èÊï∞: {int(total_positive_pairs):,}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            print(f\"\\n--- Total Dataset Size Estimation (Positive:Negative = 1:{NEGATIVE_SAMPLE_RATIO}) ---\")\n",
    "            total_negative_pairs = total_positive_pairs * NEGATIVE_SAMPLE_RATIO\n",
    "            total_dataset_size = total_positive_pairs + total_negative_pairs\n",
    "            print(f\"  - Negative Pairs to be sampled: {int(total_negative_pairs):,}\")\n",
    "            print(f\"  - Total Estimated Dataset Size: {int(total_dataset_size):,} pairs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üí• An error occurred: {e}\")\n",
    "\n",
    "# --- ÂÆüË°å ---\n",
    "estimate_final_training_size_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afc2a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Planning Balanced Additional Manual Verification ---\n",
      "Target: Add approximately 1,600 more positive pairs.\n",
      "Constraint: Max 5 additional checks per data paper group.\n",
      "\n",
      "==================================================\n",
      "--- Recommended Balanced Verification Plan ---\n",
      "Total additional verifications needed: 16 papers\n",
      "Expected additional positive pairs: 1,610 pairs\n",
      "==================================================\n",
      "\n",
      "--- Details of Verification Tasks (Top 20 by gain) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited_datapaper_doi</th>\n",
       "      <th>verify_m_th_paper</th>\n",
       "      <th>additional_pairs</th>\n",
       "      <th>original_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>4</td>\n",
       "      <td>175</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>6</td>\n",
       "      <td>173</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1038/S41597-020-0369-Y</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cited_datapaper_doi  verify_m_th_paper  additional_pairs  original_k\n",
       "0   10.1038/S41597-020-0453-3                  2               177         179\n",
       "1   10.1038/S41597-020-0453-3                  3               176         179\n",
       "2   10.1038/S41597-020-0453-3                  4               175         179\n",
       "3   10.1038/S41597-020-0453-3                  5               174         179\n",
       "4   10.1038/S41597-020-0453-3                  6               173         179\n",
       "5   10.5194/ESSD-13-4349-2021                  2                81          83\n",
       "6   10.5194/ESSD-13-4349-2021                  3                80          83\n",
       "7   10.5194/ESSD-13-4349-2021                  4                79          83\n",
       "8   10.5194/ESSD-13-4349-2021                  5                78          83\n",
       "9   10.5194/ESSD-13-4349-2021                  6                77          83\n",
       "10  10.1038/S41597-020-0534-3                  2                60          62\n",
       "11  10.1038/S41597-020-0534-3                  3                59          62\n",
       "12  10.1038/S41597-020-0534-3                  4                58          62\n",
       "13  10.1038/S41597-020-0534-3                  5                57          62\n",
       "14  10.1038/S41597-020-0534-3                  6                56          62\n",
       "15  10.1038/S41597-020-0369-Y                  2                50          52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary by Data Paper DOI (Sorted by additional checks) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited_datapaper_doi</th>\n",
       "      <th>additional_verifications</th>\n",
       "      <th>max_m_verified</th>\n",
       "      <th>original_group_size</th>\n",
       "      <th>total_additional_pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1038/S41597-020-0453-3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>179</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1038/S41597-020-0534-3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.5194/ESSD-13-4349-2021</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1038/S41597-020-0369-Y</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cited_datapaper_doi  additional_verifications  max_m_verified  \\\n",
       "1  10.1038/S41597-020-0453-3                         5               6   \n",
       "2  10.1038/S41597-020-0534-3                         5               6   \n",
       "3  10.5194/ESSD-13-4349-2021                         5               6   \n",
       "0  10.1038/S41597-020-0369-Y                         1               2   \n",
       "\n",
       "   original_group_size  total_additional_pairs  \n",
       "1                  179                     875  \n",
       "2                   62                     290  \n",
       "3                   83                     395  \n",
       "0                   52                      50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import math\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Step 1: Ë®≠ÂÆö ---\n",
    "DB_PATH = \"../data/processed/s2orc_filtered.db\"\n",
    "TRAINING_DATAPAPERS_FILE = \"../data/datapapers/sampled/training_data_papers_50.csv\"\n",
    "\n",
    "# ‚ñº‚ñº‚ñº ÁõÆÊ®ô„Å®„Åô„Çã„ÄåËøΩÂä†„ÄçÊ≠£‰æã„Éö„Ç¢Êï∞„ÇíË®≠ÂÆö ‚ñº‚ñº‚ñº\n",
    "TARGET_ADDITIONAL_PAIRS = 1600\n",
    "\n",
    "# ‚ñº‚ñº‚ñº Êñ∞Ë¶èË®≠ÂÆö: 1„Å§„ÅÆ„Éá„Éº„ÇøË´ñÊñá„Ç∞„É´„Éº„Éó„ÅÇ„Åü„Çä„ÄÅÊúÄÂ§ß‰Ωï‰ª∂„Åæ„ÅßËøΩÂä†„ÅßÁ¢∫Ë™ç„Åô„Çã„Åã ‚ñº‚ñº‚ñº\n",
    "MAX_CHECKS_PER_GROUP = 5 # ‰æã„Åà„Å∞„ÄÅÊúÄÂ§ß5‰ª∂„Åæ„Åß\n",
    "\n",
    "# --- Step 2: ËøΩÂä†„Ç¢„Éé„ÉÜ„Éº„Ç∑„Éß„É≥Ë®àÁîª„ÅÆ„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ ---\n",
    "\n",
    "def plan_balanced_additional_verification():\n",
    "    \"\"\"\n",
    "    ÁõÆÊ®ô„ÅÆËøΩÂä†„Éö„Ç¢Êï∞„Å´Âü∫„Å•„Åç„ÄÅÂêÑ„Ç∞„É´„Éº„Éó„ÅÆ‰∏äÈôê„ÇíËÄÉÊÖÆ„Åó„Å™„Åå„Çâ„ÄÅ\n",
    "    „Å©„ÅÆ„Ç∞„É´„Éº„Éó„Å´ËøΩÂä†„Åß‰Ωï‰ª∂„ÅÆÁõÆË¶ñÁ¢∫Ë™ç„ÇíË°å„ÅÜ„Åπ„Åç„Åã„ÇíË®àÁîª„Åô„Çã„ÄÇ\n",
    "    \"\"\"\n",
    "    print(\"--- Planning Balanced Additional Manual Verification ---\")\n",
    "    print(f\"Target: Add approximately {TARGET_ADDITIONAL_PAIRS:,} more positive pairs.\")\n",
    "    print(f\"Constraint: Max {MAX_CHECKS_PER_GROUP} additional checks per data paper group.\")\n",
    "\n",
    "    if not os.path.exists(DB_PATH) or not os.path.exists(TRAINING_DATAPAPERS_FILE):\n",
    "        print(f\"‚ùå Error: Database or training data paper file not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1. Ë®ìÁ∑¥Áî®„Éá„Éº„ÇøË´ñÊñá„É™„Çπ„Éà„Å®„Åù„ÅÆ'Used'Êï∞„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "        df_train = pd.read_csv(TRAINING_DATAPAPERS_FILE)\n",
    "        if 'used_paper_count' not in df_train.columns:\n",
    "             print(\"‚ùå Error: 'used_paper_count' column not found.\")\n",
    "             return\n",
    "             \n",
    "        # 2. ÂêÑ„Ç∞„É´„Éº„Éó„Åß„ÄÅËøΩÂä†Á¢∫Ë™ç„Åó„ÅüÂ†¥Âêà„ÅÆ„ÄåËøΩÂä†„Éö„Ç¢Êï∞„Äç„ÇíË®àÁÆó\n",
    "        potential_gains = []\n",
    "        for index, row in df_train.iterrows():\n",
    "            k = row['used_paper_count']\n",
    "            # 2‰ª∂ÁõÆ„Åã„Çâ„ÄÅÊúÄÂ§ß„Åß MAX_CHECKS_PER_GROUP + 1 ‰ª∂ÁõÆ„Åæ„Åß„ÅÆÁ¢∫Ë™ç„ÇíËÄÉÊÖÆ\n",
    "            # („Åô„Åß„Å´1‰ª∂„ÅØÁ¢∫Ë™çÊ∏à„Åø„Å™„ÅÆ„Åß„ÄÅËøΩÂä†„ÅØÊúÄÂ§ß MAX_CHECKS_PER_GROUP ‰ª∂)\n",
    "            for m in range(2, min(k + 1, 2 + MAX_CHECKS_PER_GROUP)): # ‰øÆÊ≠£ÁÇπ\n",
    "                additional_pairs = k - m # m‰ª∂ÁõÆ„ÇíÁ¢∫Ë™ç„Åó„ÅüÂ†¥Âêà„ÅÆËøΩÂä†„Éö„Ç¢Êï∞\n",
    "                if additional_pairs >= 0: # ‰øÆÊ≠£ÁÇπ: >=0 „Å´Â§âÊõ¥ÔºàÊúÄÂæå„ÅÆÁ¢∫Ë™ç„Åß„ÇÇ„Éö„Ç¢„ÅØ0Â¢ó„Åà„ÇãÔºâ\n",
    "                    potential_gains.append({\n",
    "                        'cited_datapaper_doi': row['cited_datapaper_doi'],\n",
    "                        'verify_m_th_paper': m, # ‰Ωï‰ª∂ÁõÆ„ÅÆÁ¢∫Ë™ç„Åã\n",
    "                        'additional_pairs': additional_pairs,\n",
    "                        'original_k': k\n",
    "                    })\n",
    "        \n",
    "        if not potential_gains:\n",
    "            print(\"No potential gains found.\")\n",
    "            return\n",
    "\n",
    "        # 3. ËøΩÂä†„Éö„Ç¢Êï∞„ÅåÂ§ö„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà\n",
    "        df_gains = pd.DataFrame(potential_gains).sort_values(by=['additional_pairs', 'original_k'], ascending=[False, False])\n",
    "\n",
    "        # 4. ÁõÆÊ®ô„Å´ÈÅî„Åô„Çã„Åæ„Åß„ÄÅ„Ç∞„É´„Éº„Éó‰∏äÈôê„ÇíÂÆà„Çä„Å™„Åå„Çâ„Çø„Çπ„ÇØ„ÇíÈÅ∏Êäû\n",
    "        cumulative_pairs = 0\n",
    "        tasks_to_perform = []\n",
    "        checks_per_group = {} # ÂêÑ„Ç∞„É´„Éº„Éó„ÅßÈÅ∏Êäû„Åï„Çå„ÅüËøΩÂä†Á¢∫Ë™çÊï∞„ÇíË®òÈå≤\n",
    "\n",
    "        for index, task in df_gains.iterrows():\n",
    "            doi = task['cited_datapaper_doi']\n",
    "            current_checks = checks_per_group.get(doi, 0)\n",
    "            \n",
    "            # „Ç∞„É´„Éº„Éó„Åî„Å®„ÅÆ‰∏äÈôê„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "            if current_checks < MAX_CHECKS_PER_GROUP:\n",
    "                tasks_to_perform.append(task)\n",
    "                cumulative_pairs += task['additional_pairs']\n",
    "                checks_per_group[doi] = current_checks + 1\n",
    "                \n",
    "                # ÁõÆÊ®ô„Éö„Ç¢Êï∞„Å´ÈÅî„Åó„Åü„ÇâÁµÇ‰∫Ü\n",
    "                if cumulative_pairs >= TARGET_ADDITIONAL_PAIRS:\n",
    "                    break\n",
    "                \n",
    "        # --- 5. ÁµêÊûú„ÅÆÂ†±Âëä ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"--- Recommended Balanced Verification Plan ---\")\n",
    "        print(f\"Total additional verifications needed: {len(tasks_to_perform):,} papers\")\n",
    "        print(f\"Expected additional positive pairs: {int(cumulative_pairs):,} pairs\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\n--- Details of Verification Tasks (Top 20 by gain) ---\")\n",
    "        display(pd.DataFrame(tasks_to_perform).head(20))\n",
    "        \n",
    "        print(\"\\n--- Summary by Data Paper DOI (Sorted by additional checks) ---\")\n",
    "        df_summary = pd.DataFrame(tasks_to_perform)\n",
    "        summary_table = df_summary.groupby('cited_datapaper_doi').agg(\n",
    "            additional_verifications=('verify_m_th_paper', 'count'),\n",
    "            max_m_verified=('verify_m_th_paper', 'max'),\n",
    "            original_group_size=('original_k', 'first'),\n",
    "            total_additional_pairs=('additional_pairs', 'sum')\n",
    "        ).reset_index().sort_values(by='additional_verifications', ascending=False)\n",
    "        \n",
    "        display(summary_table.head(20)) # Ë°®Á§∫‰ª∂Êï∞„ÇíÂ¢ó„ÇÑ„Åô\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üí• An error occurred: {e}\")\n",
    "\n",
    "# --- ÂÆüË°å ---\n",
    "plan_balanced_additional_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

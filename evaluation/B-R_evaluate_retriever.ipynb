{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2c990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n",
      "Custom model class 'SiameseRankNetModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. ライブラリのインポート ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sqlite3\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# --- 2. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# --- 3. 訓練時と同じモデルクラスの定義 ---\n",
    "class SiameseRankNetModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseRankNetModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "        # self.init_weights() # from_pretrainedを使うので不要\n",
    "\n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output \n",
    "\n",
    "    def _calculate_score(self, vec_a, vec_b):\n",
    "        diff = torch.abs(vec_a - vec_b)\n",
    "        prod = vec_a * vec_b\n",
    "        features = torch.cat([vec_a, vec_b, diff, prod], dim=1)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # 推論用に、単一のベクトルを返すシンプルなforwardに修正\n",
    "        return self._get_vector(input_ids, attention_mask)\n",
    "\n",
    "print(\"Custom model class 'SiameseRankNetModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8a4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k values for evaluation: [1, 5, 10, 30, 50, 100, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000]\n",
      "Top k results to save: 30000\n",
      "Paths and settings are configured.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. パスと設定 ---\n",
    "\n",
    "# --- 入力ファイルパス ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "TRAINED_MODEL_PATH = \"models/sbert_ranknet_v3/best_model\"\n",
    "EMBEDDINGS_FILE = \"data/processed/ranknet_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_FILE = \"data/processed/ranknet_doi_map.json\"\n",
    "EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50.csv\"\n",
    "TRAIN_FILE = \"data/processed/training_dataset_hard_negatives_1to3.csv\"\n",
    "\n",
    "# --- 出力ファイルパス ---\n",
    "RICH_RESULTS_FILE = \"data/processed/ranknet_evaluation_results_v2.json\"\n",
    "\n",
    "# --- モデルと評価のハイパーパラメータ ---\n",
    "MAX_LENGTH = 512\n",
    "EMBEDDING_DIM = 768 # SciBERT-baseの隠れ層サイズ\n",
    "EVAL_BATCH_SIZE = 4096  #  vektor化時のバッチサイズ\n",
    "SCORE_BATCH_SIZE = 131072 # スコア計算時のバッチサイズ (GPUメモリに応じて調整)\n",
    "\n",
    "# 評価するkの値\n",
    "# ランキング上位の精度を見るための小さいk\n",
    "small_k_values = [1, 5, 10, 30, 50, 100, 200, 300, 500]\n",
    "# 網羅性を見るための大きいk (1000刻みで30000まで)\n",
    "large_k_values = list(range(1000, 30001, 1000))\n",
    "\n",
    "EVAL_K_VALUES = sorted(list(set(small_k_values + large_k_values)))\n",
    "\n",
    "# SAVE_TOP_K も評価の最大値に合わせる\n",
    "SAVE_TOP_K = 30000 \n",
    "\n",
    "print(f\"k values for evaluation: {EVAL_K_VALUES}\")\n",
    "print(f\"Top k results to save: {SAVE_TOP_K}\")\n",
    "print(\"Paths and settings are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model from: models/sbert_ranknet_v3/best_model\n",
      "Loading DOI map from data/processed/ranknet_doi_map.json...\n",
      "Loading embeddings with mmap_mode from data/processed/ranknet_scibert_cls_embeddings.npy...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 8923496448 into shape (8923496448,768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmemmap(EMBEDDINGS_FILE, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m total_vectors \u001b[38;5;241m=\u001b[39m corpus_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEMBEDDING_DIM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_vectors\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 8923496448 into shape (8923496448,768)"
     ]
    }
   ],
   "source": [
    "# --- 5. リソースのロード ---\n",
    "print(f\"Loading tokenizer & model from: {TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "model = SiameseRankNetModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading DOI map from {DOI_MAP_FILE}...\")\n",
    "with open(DOI_MAP_FILE, 'r') as f:\n",
    "    doi_to_index = json.load(f)\n",
    "index_to_doi = {v: k for k, v in doi_to_index.items()}\n",
    "\n",
    "print(f\"Loading embeddings with mmap_mode from {EMBEDDINGS_FILE}...\")\n",
    "# memmapで1次元配列として読み込む\n",
    "raw_embeddings = np.memmap(EMBEDDINGS_FILE, dtype=np.float32, mode='r')\n",
    "\n",
    "# 論文の総数を正しく計算\n",
    "total_papers = raw_embeddings.shape[0] // EMBEDDING_DIM\n",
    "print(f\"Total elements: {raw_embeddings.shape[0]:,}, Total papers: {total_papers:,}\")\n",
    "\n",
    "# 正しい形状にreshape\n",
    "corpus_embeddings = raw_embeddings.reshape(total_papers, EMBEDDING_DIM)\n",
    "print(f\"Embeddings reshaped to: {corpus_embeddings.shape}\")\n",
    "\n",
    "# これ以降のコードで使うのは total_papers\n",
    "total_vectors = total_papers \n",
    "print(f\"Loaded {total_vectors:,} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d81264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 評価データ（クエリと正解）の準備 ---\n",
    "print(\"Preparing evaluation queries and ground truth...\")\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "train_dois = set(df_train['abstract_a']) | set(df_train['abstract_b'])\n",
    "# 訓練データDOIのインデックスセットを事前に作成\n",
    "train_indices = {doi_to_index[doi] for doi in train_dois if doi in doi_to_index}\n",
    "print(f\"Loaded {len(train_dois):,} training DOIs to exclude.\")\n",
    "\n",
    "df_eval_papers = pd.read_csv(EVAL_PAPERS_FILE)\n",
    "eval_data_paper_dois = tuple(df_eval_papers['cited_datapaper_doi'].unique())\n",
    "\n",
    "evaluation_queries = [] \n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    for data_paper_doi in tqdm(eval_data_paper_dois, desc=\"Building Ground Truth\"):\n",
    "        query_gt = \"SELECT citing_doi FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        gt_rows = conn.execute(query_gt, (data_paper_doi,)).fetchall()\n",
    "        ground_truth_dois = {row[0] for row in gt_rows}\n",
    "        \n",
    "        if len(ground_truth_dois) >= 2:\n",
    "            query_doi = ground_truth_dois.pop() \n",
    "            query_text_res = conn.execute(\"SELECT abstract FROM papers WHERE doi = ?\", (query_doi,)).fetchone()\n",
    "            if query_text_res and query_text_res[0]:\n",
    "                evaluation_queries.append({\n",
    "                    \"query_doi\": query_doi,\n",
    "                    \"query_abstract\": query_text_res[0],\n",
    "                    \"ground_truth_dois\": list(ground_truth_dois)\n",
    "                })\n",
    "\n",
    "print(f\"Prepared {len(evaluation_queries)} valid evaluation queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. 評価の実行 ---\n",
    "print(f\"Starting evaluation for {len(evaluation_queries)} queries...\")\n",
    "\n",
    "all_query_results = []\n",
    "total_start_time = time.time()\n",
    "max_rank_to_check = max(max(EVAL_K_VALUES), SAVE_TOP_K)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for query_data in tqdm(evaluation_queries, desc=\"Evaluating Queries\"):\n",
    "        \n",
    "        # 1. クエリをベクトル化\n",
    "        inputs = tokenizer(query_data[\"query_abstract\"], padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        query_vector = model(**inputs)\n",
    "\n",
    "        # 2. 全候補論文とのスコアをバッチ計算\n",
    "        all_scores = []\n",
    "        for i in range(0, total_vectors, SCORE_BATCH_SIZE):\n",
    "            candidate_vectors_batch = torch.from_numpy(corpus_embeddings[i : i + SCORE_BATCH_SIZE]).to(device)\n",
    "            query_vector_tiled = query_vector.expand(len(candidate_vectors_batch), -1)\n",
    "            scores_batch = model._calculate_score(query_vector_tiled, candidate_vectors_batch)\n",
    "            all_scores.append(scores_batch.cpu())\n",
    "        \n",
    "        final_scores = torch.cat(all_scores, dim=0).flatten().numpy()\n",
    "\n",
    "        # 3. スコア順に並び替え\n",
    "        sorted_indices = np.argsort(final_scores)[::-1]\n",
    "        \n",
    "        # 4. 評価とリッチな結果の保存\n",
    "        ground_truth_indices = {doi_to_index[doi] for doi in query_data[\"ground_truth_dois\"] if doi in doi_to_index}\n",
    "        \n",
    "        ranks_of_hits = []\n",
    "        top_k_results = []\n",
    "        rank_counter = 0\n",
    "\n",
    "        for doc_index in sorted_indices:\n",
    "            if doc_index in train_indices:\n",
    "                continue\n",
    "            \n",
    "            rank_counter += 1\n",
    "            is_correct = doc_index in ground_truth_indices\n",
    "            \n",
    "            if is_correct:\n",
    "                ranks_of_hits.append(rank_counter)\n",
    "            \n",
    "            if rank_counter <= SAVE_TOP_K:\n",
    "                top_k_results.append({\n",
    "                    \"rank\": rank_counter,\n",
    "                    \"doi\": index_to_doi.get(int(doc_index), \"N/A\"),\n",
    "                    \"score\": float(final_scores[doc_index]),\n",
    "                    \"is_correct\": is_correct\n",
    "                })\n",
    "            \n",
    "            # 探索打ち切り判定\n",
    "            if rank_counter >= max_rank_to_check:\n",
    "                # 全ての正解を見つけた場合、さらに早く打ち切る\n",
    "                if len(ranks_of_hits) == len(ground_truth_indices):\n",
    "                    break\n",
    "        \n",
    "        all_query_results.append({\n",
    "            \"query_doi\": query_data[\"query_doi\"],\n",
    "            \"ground_truth_dois\": query_data[\"ground_truth_dois\"],\n",
    "            \"ranks_of_hits\": sorted(ranks_of_hits),\n",
    "            \"top_k_results\": top_k_results\n",
    "        })\n",
    "\n",
    "print(f\"\\nEvaluation finished. Total time: {(time.time() - total_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. 最終結果の集計と保存 ---\n",
    "\n",
    "# 8-1. リッチな結果をJSONファイルに保存\n",
    "print(f\"Saving rich evaluation results to {RICH_RESULTS_FILE}...\")\n",
    "with open(RICH_RESULTS_FILE, 'w') as f:\n",
    "    json.dump(all_query_results, f, indent=2)\n",
    "print(\"Rich results saved.\")\n",
    "\n",
    "# 8-2. 平均スコアのサマリーを計算\n",
    "mrr_scores = []\n",
    "recall_at_k_scores = {k: [] for k in EVAL_K_VALUES}\n",
    "\n",
    "for res in all_query_results:\n",
    "    ranks = res[\"ranks_of_hits\"]\n",
    "    \n",
    "    # MRRの計算 (最初のヒットの順位)\n",
    "    if ranks:\n",
    "        mrr_scores.append(1.0 / ranks[0])\n",
    "    else:\n",
    "        mrr_scores.append(0.0)\n",
    "    \n",
    "    # Recall@kの計算\n",
    "    for k in EVAL_K_VALUES:\n",
    "        # 上位kに1つでも正解があればヒット (Recall=1)\n",
    "        if ranks and any(r <= k for r in ranks):\n",
    "            recall_at_k_scores[k].append(1.0)\n",
    "        else:\n",
    "            recall_at_k_scores[k].append(0.0)\n",
    "\n",
    "# 平均を計算\n",
    "final_mrr = np.mean(mrr_scores)\n",
    "final_recall_at_k = {k: np.mean(scores) for k, scores in recall_at_k_scores.items()}\n",
    "\n",
    "# --- 9. 結果の表示 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"--- Final Evaluation Results: S-BERT (RankNet) ---\")\n",
    "print(f\"(Based on {len(evaluation_queries)} queries)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"MRR (Mean Reciprocal Rank): {final_mrr:.4f}\\n\")\n",
    "print(\"--- Recall@K ---\")\n",
    "df_results = pd.DataFrame(columns=['Metric', 'Value'])\n",
    "for k, recall in final_recall_at_k.items():\n",
    "    metric_name = f\"Recall@{k}\"\n",
    "    print(f\"{metric_name:<10}: {recall:.4f} ({(recall * 100):.2f}%)\")\n",
    "    df_results.loc[len(df_results)] = [metric_name, recall]\n",
    "\n",
    "df_results.loc[len(df_results)] = ['MRR', final_mrr]\n",
    "print(\"=\"*50)\n",
    "\n",
    "# (参考) 正解が1つも見つからなかったクエリの数\n",
    "not_found_count = len([res for res in all_query_results if not res[\"ranks_of_hits\"]])\n",
    "print(f\"Queries where NO hit was found: {not_found_count} / {len(evaluation_queries)}\")\n",
    "\n",
    "print(\"\\n--- Results Table ---\")\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2c990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n",
      "Custom model class 'SiameseRankNetModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. ライブラリのインポート ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sqlite3\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# --- 2. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# --- 3. 訓練時と同じモデルクラスの定義 ---\n",
    "class SiameseRankNetModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseRankNetModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "        # self.init_weights() # from_pretrainedを使うので不要\n",
    "\n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output \n",
    "\n",
    "    def _calculate_score(self, vec_a, vec_b):\n",
    "        diff = torch.abs(vec_a - vec_b)\n",
    "        prod = vec_a * vec_b\n",
    "        features = torch.cat([vec_a, vec_b, diff, prod], dim=1)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # 推論用に、単一のベクトルを返すシンプルなforwardに修正\n",
    "        return self._get_vector(input_ids, attention_mask)\n",
    "\n",
    "print(\"Custom model class 'SiameseRankNetModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8a4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k values for evaluation: [1, 5, 10, 30, 50, 100, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000]\n",
      "Top k results to save: 30000\n",
      "Paths and settings are configured.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. パスと設定 ---\n",
    "\n",
    "# --- 入力ファイルパス ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "TRAINED_MODEL_PATH = \"models/sbert_ranknet_v3/best_model\"\n",
    "EMBEDDINGS_FILE = \"data/processed/ranknet_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_FILE = \"data/processed/ranknet_doi_map.json\"\n",
    "EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "TRAIN_FILE = \"data/processed/training_dataset_hard_negatives_1to3.csv\"\n",
    "\n",
    "# --- 出力ファイルパス ---\n",
    "RICH_RESULTS_FILE = \"data/processed/ranknet_evaluation_results_v2.json\"\n",
    "\n",
    "# --- モデルと評価のハイパーパラメータ ---\n",
    "MAX_LENGTH = 512\n",
    "EMBEDDING_DIM = 768 # SciBERT-baseの隠れ層サイズ\n",
    "EVAL_BATCH_SIZE = 4096  #  vektor化時のバッチサイズ\n",
    "SCORE_BATCH_SIZE = 131072 # スコア計算時のバッチサイズ (GPUメモリに応じて調整)\n",
    "\n",
    "# 評価するkの値\n",
    "# ランキング上位の精度を見るための小さいk\n",
    "small_k_values = [1, 5, 10, 30, 50, 100, 200, 300, 500]\n",
    "# 網羅性を見るための大きいk (1000刻みで30000まで)\n",
    "large_k_values = list(range(1000, 30001, 1000))\n",
    "\n",
    "EVAL_K_VALUES = sorted(list(set(small_k_values + large_k_values)))\n",
    "\n",
    "# SAVE_TOP_K も評価の最大値に合わせる\n",
    "SAVE_TOP_K = 30000 \n",
    "\n",
    "print(f\"k values for evaluation: {EVAL_K_VALUES}\")\n",
    "print(f\"Top k results to save: {SAVE_TOP_K}\")\n",
    "print(\"Paths and settings are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc8e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model from: models/sbert_ranknet_v3/best_model\n",
      "Loading DOI map from data/processed/ranknet_doi_map.json...\n",
      "Loading embeddings with mmap_mode from data/processed/ranknet_scibert_cls_embeddings.npy...\n",
      "Total elements: 8,923,496,448, Total papers: 11,619,136\n",
      "Embeddings reshaped to: (11619136, 768)\n",
      "Loaded 11,619,136 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. リソースのロード ---\n",
    "print(f\"Loading tokenizer & model from: {TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "model = SiameseRankNetModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading DOI map from {DOI_MAP_FILE}...\")\n",
    "with open(DOI_MAP_FILE, 'r') as f:\n",
    "    doi_to_index = json.load(f)\n",
    "index_to_doi = {v: k for k, v in doi_to_index.items()}\n",
    "\n",
    "print(f\"Loading embeddings with mmap_mode from {EMBEDDINGS_FILE}...\")\n",
    "# memmapで1次元配列として読み込む\n",
    "raw_embeddings = np.memmap(EMBEDDINGS_FILE, dtype=np.float32, mode='r')\n",
    "\n",
    "# 論文の総数を正しく計算\n",
    "total_papers = raw_embeddings.shape[0] // EMBEDDING_DIM\n",
    "print(f\"Total elements: {raw_embeddings.shape[0]:,}, Total papers: {total_papers:,}\")\n",
    "\n",
    "# 正しい形状にreshape\n",
    "corpus_embeddings = raw_embeddings.reshape(total_papers, EMBEDDING_DIM)\n",
    "print(f\"Embeddings reshaped to: {corpus_embeddings.shape}\")\n",
    "\n",
    "# これ以降のコードで使うのは total_papers\n",
    "total_vectors = total_papers \n",
    "print(f\"Loaded {total_vectors:,} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d81264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation queries and ground truth...\n",
      "Loaded 2,338 training DOIs to exclude.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1263601de44b628fb44bc8c710d373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Ground Truth:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 valid evaluation queries.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 評価データ（クエリと正解）の準備 ---\n",
    "print(\"Preparing evaluation queries and ground truth...\")\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "train_dois = set(df_train['abstract_a']) | set(df_train['abstract_b'])\n",
    "# 訓練データDOIのインデックスセットを事前に作成\n",
    "train_indices = {doi_to_index[doi] for doi in train_dois if doi in doi_to_index}\n",
    "print(f\"Loaded {len(train_dois):,} training DOIs to exclude.\")\n",
    "\n",
    "df_eval_papers = pd.read_csv(EVAL_PAPERS_FILE)\n",
    "eval_data_paper_dois = tuple(df_eval_papers['cited_datapaper_doi'].unique())\n",
    "\n",
    "evaluation_queries = [] \n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    for data_paper_doi in tqdm(eval_data_paper_dois, desc=\"Building Ground Truth\"):\n",
    "        query_gt = \"SELECT citing_doi FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        gt_rows = conn.execute(query_gt, (data_paper_doi,)).fetchall()\n",
    "        ground_truth_dois = {row[0] for row in gt_rows}\n",
    "        \n",
    "        if len(ground_truth_dois) >= 2:\n",
    "            query_doi = ground_truth_dois.pop() \n",
    "            query_text_res = conn.execute(\"SELECT abstract FROM papers WHERE doi = ?\", (query_doi,)).fetchone()\n",
    "            if query_text_res and query_text_res[0]:\n",
    "                evaluation_queries.append({\n",
    "                    \"query_doi\": query_doi,\n",
    "                    \"query_abstract\": query_text_res[0],\n",
    "                    \"ground_truth_dois\": list(ground_truth_dois)\n",
    "                })\n",
    "\n",
    "print(f\"Prepared {len(evaluation_queries)} valid evaluation queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae7eb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 50 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412bbf4f434b41978d223e74ed286a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Queries:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/3533372913.py:18: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905979055/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  candidate_vectors_batch = torch.from_numpy(corpus_embeddings[i : i + SCORE_BATCH_SIZE]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation finished. Total time: 5.62 minutes.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 評価の実行 ---\n",
    "print(f\"Starting evaluation for {len(evaluation_queries)} queries...\")\n",
    "\n",
    "all_query_results = []\n",
    "total_start_time = time.time()\n",
    "max_rank_to_check = max(max(EVAL_K_VALUES), SAVE_TOP_K)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for query_data in tqdm(evaluation_queries, desc=\"Evaluating Queries\"):\n",
    "        \n",
    "        # 1. クエリをベクトル化\n",
    "        inputs = tokenizer(query_data[\"query_abstract\"], padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        query_vector = model(**inputs)\n",
    "\n",
    "        # 2. 全候補論文とのスコアをバッチ計算\n",
    "        all_scores = []\n",
    "        for i in range(0, total_vectors, SCORE_BATCH_SIZE):\n",
    "            candidate_vectors_batch = torch.from_numpy(corpus_embeddings[i : i + SCORE_BATCH_SIZE]).to(device)\n",
    "            query_vector_tiled = query_vector.expand(len(candidate_vectors_batch), -1)\n",
    "            scores_batch = model._calculate_score(query_vector_tiled, candidate_vectors_batch)\n",
    "            all_scores.append(scores_batch.cpu())\n",
    "        \n",
    "        final_scores = torch.cat(all_scores, dim=0).flatten().numpy()\n",
    "\n",
    "        # 3. スコア順に並び替え\n",
    "        sorted_indices = np.argsort(final_scores)[::-1]\n",
    "        \n",
    "        # 4. 評価とリッチな結果の保存\n",
    "        ground_truth_indices = {doi_to_index[doi] for doi in query_data[\"ground_truth_dois\"] if doi in doi_to_index}\n",
    "        \n",
    "        ranks_of_hits = []\n",
    "        top_k_results = []\n",
    "        rank_counter = 0\n",
    "\n",
    "        for doc_index in sorted_indices:\n",
    "            if doc_index in train_indices:\n",
    "                continue\n",
    "            \n",
    "            rank_counter += 1\n",
    "            is_correct = doc_index in ground_truth_indices\n",
    "            \n",
    "            if is_correct:\n",
    "                ranks_of_hits.append(rank_counter)\n",
    "            \n",
    "            if rank_counter <= SAVE_TOP_K:\n",
    "                top_k_results.append({\n",
    "                    \"rank\": rank_counter,\n",
    "                    \"doi\": index_to_doi.get(int(doc_index), \"N/A\"),\n",
    "                    \"score\": float(final_scores[doc_index]),\n",
    "                    \"is_correct\": is_correct\n",
    "                })\n",
    "            \n",
    "            # 探索打ち切り判定\n",
    "            if rank_counter >= max_rank_to_check:\n",
    "                # 全ての正解を見つけた場合、さらに早く打ち切る\n",
    "                if len(ranks_of_hits) == len(ground_truth_indices):\n",
    "                    break\n",
    "        \n",
    "        all_query_results.append({\n",
    "            \"query_doi\": query_data[\"query_doi\"],\n",
    "            \"ground_truth_dois\": query_data[\"ground_truth_dois\"],\n",
    "            \"ranks_of_hits\": sorted(ranks_of_hits),\n",
    "            \"top_k_results\": top_k_results\n",
    "        })\n",
    "\n",
    "print(f\"\\nEvaluation finished. Total time: {(time.time() - total_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509f1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rich evaluation results to data/processed/ranknet_evaluation_results_v2.json...\n",
      "Rich results saved.\n",
      "\n",
      "==================================================\n",
      "--- Final Evaluation Results: S-BERT (RankNet) ---\n",
      "(Based on 50 queries)\n",
      "==================================================\n",
      "MRR (Mean Reciprocal Rank): 0.0001\n",
      "\n",
      "--- Recall@K ---\n",
      "Recall@1  : 0.0000 (0.00%)\n",
      "Recall@5  : 0.0000 (0.00%)\n",
      "Recall@10 : 0.0000 (0.00%)\n",
      "Recall@30 : 0.0000 (0.00%)\n",
      "Recall@50 : 0.0000 (0.00%)\n",
      "Recall@100: 0.0000 (0.00%)\n",
      "Recall@200: 0.0000 (0.00%)\n",
      "Recall@300: 0.0000 (0.00%)\n",
      "Recall@500: 0.0000 (0.00%)\n",
      "Recall@1000: 0.0600 (6.00%)\n",
      "Recall@2000: 0.0800 (8.00%)\n",
      "Recall@3000: 0.1200 (12.00%)\n",
      "Recall@4000: 0.1400 (14.00%)\n",
      "Recall@5000: 0.1400 (14.00%)\n",
      "Recall@6000: 0.1400 (14.00%)\n",
      "Recall@7000: 0.1400 (14.00%)\n",
      "Recall@8000: 0.1400 (14.00%)\n",
      "Recall@9000: 0.1600 (16.00%)\n",
      "Recall@10000: 0.1600 (16.00%)\n",
      "Recall@11000: 0.1600 (16.00%)\n",
      "Recall@12000: 0.1600 (16.00%)\n",
      "Recall@13000: 0.1600 (16.00%)\n",
      "Recall@14000: 0.1600 (16.00%)\n",
      "Recall@15000: 0.1600 (16.00%)\n",
      "Recall@16000: 0.1800 (18.00%)\n",
      "Recall@17000: 0.1800 (18.00%)\n",
      "Recall@18000: 0.1800 (18.00%)\n",
      "Recall@19000: 0.1800 (18.00%)\n",
      "Recall@20000: 0.1800 (18.00%)\n",
      "Recall@21000: 0.2000 (20.00%)\n",
      "Recall@22000: 0.2200 (22.00%)\n",
      "Recall@23000: 0.2400 (24.00%)\n",
      "Recall@24000: 0.2400 (24.00%)\n",
      "Recall@25000: 0.2400 (24.00%)\n",
      "Recall@26000: 0.2400 (24.00%)\n",
      "Recall@27000: 0.2400 (24.00%)\n",
      "Recall@28000: 0.2400 (24.00%)\n",
      "Recall@29000: 0.2600 (26.00%)\n",
      "Recall@30000: 0.2600 (26.00%)\n",
      "==================================================\n",
      "Queries where NO hit was found: 0 / 50\n",
      "\n",
      "--- Results Table ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall@1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall@5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall@10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recall@30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recall@50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Recall@100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Recall@200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recall@300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Recall@500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Recall@1000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Recall@2000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Recall@3000</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Recall@4000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Recall@5000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Recall@6000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Recall@7000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Recall@8000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Recall@9000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Recall@10000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Recall@11000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Recall@12000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Recall@13000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Recall@14000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Recall@15000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Recall@16000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Recall@17000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Recall@18000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Recall@19000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Recall@20000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Recall@21000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Recall@22000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Recall@23000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Recall@24000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Recall@25000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recall@26000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Recall@27000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Recall@28000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Recall@29000</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Recall@30000</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MRR</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric     Value\n",
       "0       Recall@1  0.000000\n",
       "1       Recall@5  0.000000\n",
       "2      Recall@10  0.000000\n",
       "3      Recall@30  0.000000\n",
       "4      Recall@50  0.000000\n",
       "5     Recall@100  0.000000\n",
       "6     Recall@200  0.000000\n",
       "7     Recall@300  0.000000\n",
       "8     Recall@500  0.000000\n",
       "9    Recall@1000  0.060000\n",
       "10   Recall@2000  0.080000\n",
       "11   Recall@3000  0.120000\n",
       "12   Recall@4000  0.140000\n",
       "13   Recall@5000  0.140000\n",
       "14   Recall@6000  0.140000\n",
       "15   Recall@7000  0.140000\n",
       "16   Recall@8000  0.140000\n",
       "17   Recall@9000  0.160000\n",
       "18  Recall@10000  0.160000\n",
       "19  Recall@11000  0.160000\n",
       "20  Recall@12000  0.160000\n",
       "21  Recall@13000  0.160000\n",
       "22  Recall@14000  0.160000\n",
       "23  Recall@15000  0.160000\n",
       "24  Recall@16000  0.180000\n",
       "25  Recall@17000  0.180000\n",
       "26  Recall@18000  0.180000\n",
       "27  Recall@19000  0.180000\n",
       "28  Recall@20000  0.180000\n",
       "29  Recall@21000  0.200000\n",
       "30  Recall@22000  0.220000\n",
       "31  Recall@23000  0.240000\n",
       "32  Recall@24000  0.240000\n",
       "33  Recall@25000  0.240000\n",
       "34  Recall@26000  0.240000\n",
       "35  Recall@27000  0.240000\n",
       "36  Recall@28000  0.240000\n",
       "37  Recall@29000  0.260000\n",
       "38  Recall@30000  0.260000\n",
       "39           MRR  0.000129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 8. 最終結果の集計と保存 ---\n",
    "\n",
    "# 8-1. リッチな結果をJSONファイルに保存\n",
    "print(f\"Saving rich evaluation results to {RICH_RESULTS_FILE}...\")\n",
    "with open(RICH_RESULTS_FILE, 'w') as f:\n",
    "    json.dump(all_query_results, f, indent=2)\n",
    "print(\"Rich results saved.\")\n",
    "\n",
    "# 8-2. 平均スコアのサマリーを計算\n",
    "mrr_scores = []\n",
    "recall_at_k_scores = {k: [] for k in EVAL_K_VALUES}\n",
    "\n",
    "for res in all_query_results:\n",
    "    ranks = res[\"ranks_of_hits\"]\n",
    "    \n",
    "    # MRRの計算 (最初のヒットの順位)\n",
    "    if ranks:\n",
    "        mrr_scores.append(1.0 / ranks[0])\n",
    "    else:\n",
    "        mrr_scores.append(0.0)\n",
    "    \n",
    "    # Recall@kの計算\n",
    "    for k in EVAL_K_VALUES:\n",
    "        # 上位kに1つでも正解があればヒット (Recall=1)\n",
    "        if ranks and any(r <= k for r in ranks):\n",
    "            recall_at_k_scores[k].append(1.0)\n",
    "        else:\n",
    "            recall_at_k_scores[k].append(0.0)\n",
    "\n",
    "# 平均を計算\n",
    "final_mrr = np.mean(mrr_scores)\n",
    "final_recall_at_k = {k: np.mean(scores) for k, scores in recall_at_k_scores.items()}\n",
    "\n",
    "# --- 9. 結果の表示 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"--- Final Evaluation Results: S-BERT (RankNet) ---\")\n",
    "print(f\"(Based on {len(evaluation_queries)} queries)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"MRR (Mean Reciprocal Rank): {final_mrr:.4f}\\n\")\n",
    "print(\"--- Recall@K ---\")\n",
    "df_results = pd.DataFrame(columns=['Metric', 'Value'])\n",
    "for k, recall in final_recall_at_k.items():\n",
    "    metric_name = f\"Recall@{k}\"\n",
    "    print(f\"{metric_name:<10}: {recall:.4f} ({(recall * 100):.2f}%)\")\n",
    "    df_results.loc[len(df_results)] = [metric_name, recall]\n",
    "\n",
    "df_results.loc[len(df_results)] = ['MRR', final_mrr]\n",
    "print(\"=\"*50)\n",
    "\n",
    "# (参考) 正解が1つも見つからなかったクエリの数\n",
    "not_found_count = len([res for res in all_query_results if not res[\"ranks_of_hits\"]])\n",
    "print(f\"Queries where NO hit was found: {not_found_count} / {len(evaluation_queries)}\")\n",
    "\n",
    "print(\"\\n--- Results Table ---\")\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

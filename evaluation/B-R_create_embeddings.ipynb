{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65dc4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import accelerate\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import faiss \n",
    "import shutil\n",
    "\n",
    "# CUDAのデバッグ用\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99f04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'SiameseRankNetModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. カスタムモデルクラスの定義 (CLS Pooling) ---\n",
    "# (notebooks/18d... と同一の定義)\n",
    "\n",
    "class SiameseRankNetModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseRankNetModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "        self.init_weights()\n",
    "    \n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output \n",
    "\n",
    "    def _calculate_score(self, vec_a, vec_b):\n",
    "        diff = torch.abs(vec_a - vec_b)\n",
    "        prod = vec_a * vec_b\n",
    "        features = torch.cat([vec_a, vec_b, diff, prod], dim=1)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "    def forward(self, input_ids=None, **kwargs):\n",
    "        # このスクリプトではforwardは使わないが、ロードのために必要\n",
    "        pass\n",
    "\n",
    "print(\"Custom model class 'SiameseRankNetModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87793c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading TRAINED model from: models/sbert_ranknet_v3/best_model\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 設定とモデルロード ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "# ▼▼▼ 訓練済みのS-BERT (RankNet) モデルのパス ▼▼▼\n",
    "TRAINED_MODEL_PATH = \"models/sbert_ranknet_v3/best_model\" \n",
    "\n",
    "# ▼▼▼ 新しい出力ファイル ▼▼▼\n",
    "EMBEDDINGS_OUTPUT_FILE = \"data/processed/ranknet_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_OUTPUT_FILE = \"data/processed/ranknet_doi_map.json\"\n",
    "FAISS_INDEX_OUTPUT_FILE = \"data/processed/ranknet_scibert.faiss\"\n",
    "\n",
    "# --- 一時ディレクトリ (再開機能) ---\n",
    "TEMP_EMBED_DIR = \"data/processed/embeddings_tmp_ranknet\"\n",
    "TEMP_DOI_DIR = \"data/processed/dois_tmp_ranknet\"\n",
    "\n",
    "# ハイパーパラメータ\n",
    "MAX_LENGTH = 512\n",
    "INFERENCE_BATCH_SIZE = 512 # GPUで一度に処理するバッチサイズ\n",
    "\n",
    "# --- モデルとトークナイザのロード ---\n",
    "print(f\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "print(f\"Loading TRAINED model from: {TRAINED_MODEL_PATH}\")\n",
    "# 訓練済みの重みをロード\n",
    "model = SiameseRankNetModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval() # 評価モード\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5078e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database generator defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. データベースからのデータ読み込み (ジェネレータ) ---\n",
    "\n",
    "def get_abstract_batches(db_path, batch_size=1000):\n",
    "    print(f\"Opening database connection: {db_path}\")\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        query = \"SELECT COUNT(doi) FROM papers WHERE abstract IS NOT NULL AND abstract != ''\"\n",
    "        total_rows = cursor.execute(query).fetchone()[0]\n",
    "        print(f\"Total abstracts to process: {total_rows:,}\")\n",
    "        yield total_rows \n",
    "        \n",
    "        cursor.execute(\"SELECT doi, abstract FROM papers WHERE abstract IS NOT NULL AND abstract != ''\")\n",
    "        batch = []\n",
    "        for row in cursor:\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "print(\"Database generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67c884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/processed/dois_tmp_ranknet for completed batches...\n",
      "Found 0 completed batches to skip.\n",
      "Opening database connection: data/processed/s2orc_filtered.db\n",
      "Total abstracts to process: 11,619,136\n",
      "Starting/Resuming embedding generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ae25b794fb415181d35fb59bc7b53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing Batches:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation complete. All batches saved in chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 全アブストラクトのベクトル化を実行 ---\n",
    "\n",
    "os.makedirs(TEMP_EMBED_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DOI_DIR, exist_ok=True)\n",
    "\n",
    "# --- 再開機能 ---\n",
    "completed_indices = set()\n",
    "print(f\"Scanning {TEMP_DOI_DIR} for completed batches...\")\n",
    "for f in os.listdir(TEMP_DOI_DIR):\n",
    "    if f.startswith('batch_') and f.endswith('.json'):\n",
    "        try: completed_indices.add(int(f[6:11]))\n",
    "        except: pass\n",
    "print(f\"Found {len(completed_indices)} completed batches to skip.\")\n",
    "\n",
    "# torch.no_grad() で勾配計算を無効化\n",
    "with torch.no_grad():\n",
    "    \n",
    "    db_batch_size = 1000 # DBから一度に読み込むサイズ\n",
    "    batch_generator = get_abstract_batches(DB_PATH, batch_size=db_batch_size)\n",
    "    total_rows = next(batch_generator)\n",
    "    total_batches = (total_rows + db_batch_size - 1) // db_batch_size\n",
    "    \n",
    "    print(\"Starting/Resuming embedding generation...\")\n",
    "    pbar = tqdm(enumerate(batch_generator), total=total_batches, desc=\"Vectorizing Batches\")\n",
    "    \n",
    "    for i, batch in pbar:\n",
    "        \n",
    "        if i in completed_indices:\n",
    "            pbar.set_description(f\"Skipping Batch {i}\")\n",
    "            continue \n",
    "\n",
    "        pbar.set_description(f\"Processing Batch {i}\")\n",
    "        dois, abstracts = zip(*batch)\n",
    "        \n",
    "        batch_embeddings_list = []\n",
    "        for j in range(0, len(abstracts), INFERENCE_BATCH_SIZE):\n",
    "            sub_batch_abstracts = abstracts[j : j + INFERENCE_BATCH_SIZE]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                list(sub_batch_abstracts), \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=MAX_LENGTH, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # ▼▼▼ 修正点: 訓練済みモデルの「_get_vector」関数でベクトル化 ▼▼▼\n",
    "            embeddings = model._get_vector(\n",
    "                input_ids=inputs['input_ids'], \n",
    "                attention_mask=inputs['attention_mask']\n",
    "            )\n",
    "            batch_embeddings_list.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        embeddings_cpu = np.vstack(batch_embeddings_list).astype(np.float32)\n",
    "        \n",
    "        # --- チャンク（一時ファイル）としてディスクに保存 ---\n",
    "        embed_filename = os.path.join(TEMP_EMBED_DIR, f\"batch_{i:05d}.npy\")\n",
    "        doi_filename = os.path.join(TEMP_DOI_DIR, f\"batch_{i:05d}.json\")\n",
    "        \n",
    "        np.save(embed_filename, embeddings_cpu)\n",
    "        with open(doi_filename, 'w') as f:\n",
    "            json.dump(dois, f)\n",
    "\n",
    "print(f\"\\nEmbedding generation complete. All batches saved in chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77cd0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging embedding chunks from disk...\n",
      "Reading 11620 DOI chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aed8ffb9741449b938159a1b4388b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading DOI chunks:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors to merge: 11,619,136\n",
      "Merging 11620 Embedding chunks into data/processed/ranknet_scibert_cls_embeddings.npy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d105658cdc234fbe84f712e4fb553882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging Embedding Chunks:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings saved to data/processed/ranknet_scibert_cls_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# --- 6. チャンクの結合と保存 ---\n",
    "print(\"Merging embedding chunks from disk...\")\n",
    "\n",
    "# 1. DOIチャンクを読み込み、最終的なDOIリストと総数を確定\n",
    "all_dois = []\n",
    "doi_files = sorted([f for f in os.listdir(TEMP_DOI_DIR) if f.endswith('.json')])\n",
    "total_rows_processed = 0\n",
    "\n",
    "print(f\"Reading {len(doi_files)} DOI chunks...\")\n",
    "for f in tqdm(doi_files, desc=\"Reading DOI chunks\"):\n",
    "    with open(os.path.join(TEMP_DOI_DIR, f), 'r') as fp:\n",
    "        batch_dois = json.load(fp)\n",
    "        all_dois.extend(batch_dois)\n",
    "        total_rows_processed += len(batch_dois)\n",
    "\n",
    "print(f\"Total vectors to merge: {total_rows_processed:,}\")\n",
    "\n",
    "# 2. メモリマップファイルを作成\n",
    "d = 768 # SciBERTの次元数\n",
    "final_embeddings = np.memmap(\n",
    "    EMBEDDINGS_OUTPUT_FILE, \n",
    "    dtype=np.float32, \n",
    "    mode='w+', \n",
    "    shape=(total_rows_processed, d)\n",
    ")\n",
    "\n",
    "# 3. チャンクを結合\n",
    "print(f\"Merging {len(doi_files)} Embedding chunks into {EMBEDDINGS_OUTPUT_FILE}...\")\n",
    "current_index = 0\n",
    "for f in tqdm(doi_files, desc=\"Merging Embedding Chunks\"):\n",
    "    batch_npy_file = os.path.join(TEMP_EMBED_DIR, f.replace('.json', '.npy'))\n",
    "    batch_data = np.load(batch_npy_file)\n",
    "    \n",
    "    start_index = current_index\n",
    "    end_index = start_index + len(batch_data)\n",
    "    \n",
    "    final_embeddings[start_index:end_index] = batch_data\n",
    "    current_index = end_index\n",
    "\n",
    "final_embeddings.flush()\n",
    "del final_embeddings\n",
    "print(f\"Final embeddings saved to {EMBEDDINGS_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DOI-to-Index map to data/processed/ranknet_doi_map.json...\n",
      "Cleaning up temporary directories...\n",
      "Total embeddings saved: 11619136\n"
     ]
    }
   ],
   "source": [
    "# --- 7. DOIマップの保存と一時ファイルの削除 ---\n",
    "\n",
    "print(f\"Saving DOI-to-Index map to {DOI_MAP_OUTPUT_FILE}...\")\n",
    "doi_to_index_map = {doi: i for i, doi in enumerate(all_dois)}\n",
    "with open(DOI_MAP_OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(doi_to_index_map, f)\n",
    "\n",
    "print(f\"Cleaning up temporary directories...\")\n",
    "shutil.rmtree(TEMP_EMBED_DIR)\n",
    "shutil.rmtree(TEMP_DOI_DIR)\n",
    "\n",
    "print(f\"Total embeddings saved: {len(doi_to_index_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5df37a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Faiss Index ---\n",
      "Calculated vector count: 11,619,136\n",
      "Loading embeddings from data/processed/ranknet_scibert_cls_embeddings.npy (mmap_mode)...\n",
      "Faiss index type: IndexFlatL2 (Dimensions: 768)\n",
      "Adding vectors to the index (this may take time)...\n",
      "Total vectors in index: 11619136\n",
      "Saving Faiss index to data/processed/ranknet_scibert.faiss...\n",
      "\n",
      "--- Faiss Indexing Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Faissインデックスの構築と保存 ---\n",
    "print(\"\\n--- Building Faiss Index ---\")\n",
    "d = 768 \n",
    "\n",
    "file_size = os.path.getsize(EMBEDDINGS_OUTPUT_FILE)\n",
    "dtype_size = np.dtype(np.float32).itemsize\n",
    "total_vectors = file_size // (d * dtype_size)\n",
    "print(f\"Calculated vector count: {total_vectors:,}\")\n",
    "\n",
    "print(f\"Loading embeddings from {EMBEDDINGS_OUTPUT_FILE} (mmap_mode)...\")\n",
    "embeddings_mmap = np.memmap(\n",
    "    EMBEDDINGS_OUTPUT_FILE,\n",
    "    dtype=np.float32,\n",
    "    mode='r',\n",
    "    shape=(total_vectors, d)\n",
    ")\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(f\"Faiss index type: IndexFlatL2 (Dimensions: {d})\")\n",
    "\n",
    "print(\"Adding vectors to the index (this may take time)...\")\n",
    "index.add(embeddings_mmap)\n",
    "print(f\"Total vectors in index: {index.ntotal}\")\n",
    "\n",
    "print(f\"Saving Faiss index to {FAISS_INDEX_OUTPUT_FILE}...\")\n",
    "faiss.write_index(index, FAISS_INDEX_OUTPUT_FILE)\n",
    "\n",
    "print(\"\\n--- Faiss Indexing Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

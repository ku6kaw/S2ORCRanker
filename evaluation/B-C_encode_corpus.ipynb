{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ae068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "Corpus File: data/raw/all_papers_corpus.csv\n",
      "Model Path: models/sbert_contrastive_with_head_v1/best_model\n",
      "Output File: data/processed/corpus_embeddings_sbert_contrastive_head_hard_neg.npy\n",
      "Batch Size: 256\n",
      "Device: cuda\n",
      "---------------------\n",
      "\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'models/sbert_contrastive_with_head_v1/best_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models/sbert_contrastive_with_head_v1/best_model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model and tokenizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m model \u001b[38;5;241m=\u001b[39m SiameseContrastiveWithHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 134\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading corpus from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCORPUS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:880\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2067\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2068\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2069\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2070\u001b[0m     )\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2073\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2074\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2076\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2077\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2078\u001b[0m     )\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'models/sbert_contrastive_with_head_v1/best_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models/sbert_contrastive_with_head_v1/best_model' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BertPreTrainedModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "CORPUS_FILE = \"data/raw/all_papers_corpus.csv\"\n",
    "ABSTRACT_COLUMN = \"abstract\"\n",
    "PAPER_ID_COLUMN = \"paper_id\"\n",
    "MODEL_PATH = \"models/sbert_contrastive_with_head_v1/best_model\" \n",
    "OUTPUT_EMBEDDINGS_FILE = \"data/processed/corpus_embeddings_sbert_contrastive_head_hard_neg.npy\"\n",
    "PROGRESS_FILE = \"data/processed/encoding_progress.log\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Corpus File: {CORPUS_FILE}\")\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "print(f\"Output File: {OUTPUT_EMBEDDINGS_FILE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(\"---------------------\\n\")\n",
    "\n",
    "# --- 2. 必要なクラス定義 ---\n",
    "# ▼▼▼ ここに訓練スクリプトからクラス定義をコピー ▼▼▼\n",
    "class SiameseContrastiveWithHeadModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseContrastiveWithHeadModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        \n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        \"\"\"この関数をエンコード時に直接呼び出す\"\"\"\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        # 推論時にはベクトルを返すだけで良いので、_get_vectorを直接使う\n",
    "        # このforwardはTrainer用なので、直接は呼ばない\n",
    "        return self._get_vector(input_ids, attention_mask)\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# --- 3. エンコード関数 ---\n",
    "def encode_corpus(model, dataloader, device, output_file, progress_file):\n",
    "    model.eval()\n",
    "    \n",
    "    start_index = 0\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            try:\n",
    "                start_index = int(f.read().strip())\n",
    "                print(f\"Resuming from index {start_index}...\")\n",
    "            except ValueError:\n",
    "                start_index = 0\n",
    "    \n",
    "    total_samples = len(dataloader.dataset)\n",
    "    embedding_dim = model.config.hidden_size\n",
    "    embeddings_mmap = np.memmap(output_file, dtype=np.float32, mode='w+', shape=(total_samples, embedding_dim))\n",
    "    \n",
    "    processed_batches = start_index // dataloader.batch_size\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(enumerate(dataloader), total=len(dataloader), initial=processed_batches)\n",
    "            for i, batch in pbar:\n",
    "                if i < processed_batches:\n",
    "                    continue\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # ▼▼▼ _get_vectorを直接呼び出すように変更 ▼▼▼\n",
    "                    embeddings = model._get_vector(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                start = i * dataloader.batch_size\n",
    "                end = start + embeddings.shape[0]\n",
    "                embeddings_mmap[start:end] = embeddings.cpu().numpy()\n",
    "                \n",
    "                if (i + 1) % 100 == 0:\n",
    "                    embeddings_mmap.flush()\n",
    "                    with open(progress_file, 'w') as f:\n",
    "                        f.write(str(end))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nFlushing final embeddings to disk...\")\n",
    "        embeddings_mmap.flush()\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(str(total_samples))\n",
    "        print(\"Encoding process finished.\")\n",
    "\n",
    "# --- 4. メイン処理 ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = SiameseContrastiveWithHeadModel.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    print(\"Model and tokenizer loaded.\")\n",
    "    \n",
    "    print(f\"Loading corpus from {CORPUS_FILE}...\")\n",
    "    df_corpus = pd.read_csv(CORPUS_FILE)\n",
    "    corpus_texts = df_corpus[ABSTRACT_COLUMN].tolist()\n",
    "    print(f\"Loaded {len(corpus_texts):,} abstracts.\")\n",
    "\n",
    "    corpus_dataset = CorpusDataset(corpus_texts, tokenizer, MAX_LENGTH)\n",
    "    corpus_dataloader = DataLoader(corpus_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True) # pin_memory=Trueを追加\n",
    "    \n",
    "    encode_corpus(model, corpus_dataloader, DEVICE, OUTPUT_EMBEDDINGS_FILE, PROGRESS_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

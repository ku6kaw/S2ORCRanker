{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214aef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import accelerate\n",
    "import sqlite3\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba102e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'SiameseRankNetModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. カスタムモデルクラスの定義 ---\n",
    "class SiameseRankNetModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    S-BERT (RankNet) モデル\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SiameseRankNetModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1) # 最終的に1つのスコアを出力\n",
    "        )\n",
    "        self.init_weights()\n",
    "    \n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        \"\"\"SciBERTとCLSプーリングを実行するヘルパー関数\"\"\"\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output # CLSトークンのベクトル\n",
    "\n",
    "    def _calculate_score(self, vec_a, vec_b):\n",
    "        \"\"\"2つのベクトルから1つのスコアを計算するヘルパー関数\"\"\"\n",
    "        diff = torch.abs(vec_a - vec_b)\n",
    "        prod = vec_a * vec_b\n",
    "        features = torch.cat([vec_a, vec_b, diff, prod], dim=1)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "    def forward(self, input_ids=None, **kwargs):\n",
    "        pass # 推論時には不要\n",
    "\n",
    "print(\"Custom model class 'SiameseRankNetModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4e64be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model from: models/sbert_ranknet_v4/best_model\n",
      "Loading DOI map from data/processed/ranknet_v4_doi_map.json...\n",
      "Loading Embeddings (mmap_mode): data/processed/ranknet_v4_scibert_cls_embeddings.npy\n",
      "Loaded 11,619,136 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 設定とリソースのロード ---\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "\n",
    "# 訓練済みモデル\n",
    "TRAINED_MODEL_PATH = \"models/sbert_ranknet_v4/best_model\"\n",
    "\n",
    "# ベクトルDBとマップファイル (v4モデルで作成したもの)\n",
    "EMBEDDINGS_FILE = \"data/processed/ranknet_v4_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_FILE = \"data/processed/ranknet_v4_doi_map.json\"\n",
    "\n",
    "# 評価データ\n",
    "EVAL_PAPERS_FILE = \"data/datapapers/sampled/evaluation_data_papers_50_v2.csv\"\n",
    "TRAIN_FILE = \"data/processed/training_dataset_hard_negatives_1to3.csv\"\n",
    "\n",
    "# 出力ファイル\n",
    "RICH_RESULTS_FILE = \"data/processed/ranknet_v4_evaluation_results.json\"\n",
    "\n",
    "# 評価するkの値\n",
    "# ランキング上位の精度を見るための小さいk\n",
    "small_k_values = [1, 5, 10, 30, 50, 100, 200, 300, 500]\n",
    "# 網羅性を見るための大きいk (1000刻みで30000まで)\n",
    "large_k_values = list(range(1000, 30001, 1000))\n",
    "\n",
    "EVAL_K_VALUES = sorted(list(set(small_k_values + large_k_values)))\n",
    "\n",
    "# SAVE_TOP_K も評価の最大値に合わせる\n",
    "SAVE_TOP_K = 30000 \n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"Loading tokenizer & model from: {TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "model = SiameseRankNetModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading DOI map from {DOI_MAP_FILE}...\")\n",
    "with open(DOI_MAP_FILE, 'r') as f:\n",
    "    doi_to_index = json.load(f)\n",
    "id_to_doi = {v: k for k, v in doi_to_index.items()}\n",
    "\n",
    "print(f\"Loading Embeddings (mmap_mode): {EMBEDDINGS_FILE}\")\n",
    "d = 768\n",
    "file_size = os.path.getsize(EMBEDDINGS_FILE)\n",
    "dtype_size = np.dtype(np.float32).itemsize\n",
    "total_vectors = file_size // (d * dtype_size)\n",
    "\n",
    "# メモリマップでロード\n",
    "all_db_embeddings = np.memmap(\n",
    "    EMBEDDINGS_FILE, dtype=np.float32, mode='r', shape=(total_vectors, d)\n",
    ")\n",
    "print(f\"Loaded {total_vectors:,} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80a8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation queries and ground truth...\n",
      "Loaded 2,338 training DOIs to exclude.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d805d2b2f1412faf3bea8d0f15d659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Ground Truth:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 valid evaluation queries.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 評価データ（クエリと正解）の準備 ---\n",
    "print(\"Preparing evaluation queries and ground truth...\")\n",
    "\n",
    "# 1. 訓練データ（除外リスト）のDOIをロード\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_train = df_train.dropna(subset=['abstract_a', 'abstract_b'])\n",
    "train_dois = set(df_train['abstract_a']) | set(df_train['abstract_b'])\n",
    "print(f\"Loaded {len(train_dois):,} training DOIs to exclude.\")\n",
    "\n",
    "# 2. 評価用データ論文（50件）のリストをロード\n",
    "df_eval_papers = pd.read_csv(EVAL_PAPERS_FILE)\n",
    "eval_data_paper_dois = tuple(df_eval_papers['cited_datapaper_doi'].unique())\n",
    "\n",
    "# 3. 「クエリ論文」と「正解DOIリスト」を作成\n",
    "evaluation_queries = [] \n",
    "\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    for data_paper_doi in tqdm(eval_data_paper_dois, desc=\"Building Ground Truth\"):\n",
    "        query_gt = \"SELECT citing_doi FROM positive_candidates WHERE cited_datapaper_doi = ? AND human_annotation_status = 1\"\n",
    "        gt_rows = conn.execute(query_gt, (data_paper_doi,)).fetchall()\n",
    "        ground_truth_dois = {row[0] for row in gt_rows}\n",
    "        \n",
    "        if len(ground_truth_dois) >= 2:\n",
    "            query_doi = ground_truth_dois.pop() # 1件をクエリとして使用\n",
    "            query_text = conn.execute(\"SELECT abstract FROM papers WHERE doi = ?\", (query_doi,)).fetchone()\n",
    "            \n",
    "            if query_text:\n",
    "                evaluation_queries.append({\n",
    "                    \"query_doi\": query_doi,\n",
    "                    \"query_abstract\": query_text[0],\n",
    "                    \"ground_truth_dois\": list(ground_truth_dois)\n",
    "                })\n",
    "\n",
    "print(f\"Prepared {len(evaluation_queries)} valid evaluation queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f39ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation... Scoring ALL 11,619,136 candidates.\n",
      "Loading ALL embeddings into GPU memory...\n",
      "Successfully loaded torch.Size([11619136, 768]) tensor onto GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5976e5e4fb8e409a949a6ef740d13c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Queries (Total):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 評価の実行 (全DBスコア計算) ---\n",
    "print(f\"Starting evaluation... Scoring ALL {total_vectors:,} candidates.\")\n",
    "\n",
    "# 1. 全DBのベクトルをGPUにロード (33GB)\n",
    "# (これは成功しているのでそのまま維持)\n",
    "print(\"Loading ALL embeddings into GPU memory...\")\n",
    "all_db_embeddings_gpu = torch.tensor(all_db_embeddings).to(device)\n",
    "print(f\"Successfully loaded {all_db_embeddings_gpu.shape} tensor onto GPU.\")\n",
    "\n",
    "all_first_hit_ranks = [] \n",
    "all_recalls_at_k = {k: [] for k in EVAL_K_VALUES}\n",
    "all_rich_results = [] \n",
    "\n",
    "# ▼▼▼ 修正点: 計算時のバッチサイズを安全な値に設定 ▼▼▼\n",
    "# (100万件なら一時メモリは約3GBで済み、残り14GBの空きに十分収まる)\n",
    "CALC_BATCH_SIZE = 100000 \n",
    "\n",
    "with torch.no_grad():\n",
    "    for query_data in tqdm(evaluation_queries, desc=\"Evaluating Queries (Total)\"):\n",
    "        \n",
    "        query_abstract = query_data[\"query_abstract\"]\n",
    "        ground_truth = set(query_data[\"ground_truth_dois\"])\n",
    "\n",
    "        # 1. クエリをベクトル化\n",
    "        inputs = tokenizer(query_abstract, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        query_vector = model._get_vector(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        # --- 2. 全DB（1160万件）のスコアをバッチ処理で計算 ---\n",
    "        all_scores = []\n",
    "        \n",
    "        # ▼▼▼ 修正点: 固定バッチサイズでループ ▼▼▼\n",
    "        for i in range(0, total_vectors, CALC_BATCH_SIZE):\n",
    "            end = min(i + CALC_BATCH_SIZE, total_vectors)\n",
    "            \n",
    "            # DBベクトルからバッチ分をスライス (これはViewなのでメモリを食わない)\n",
    "            candidates_chunk = all_db_embeddings_gpu[i:end]\n",
    "            \n",
    "            # クエリベクトルを複製 (ここでVRAMを使用)\n",
    "            query_vector_tiled = query_vector.repeat(len(candidates_chunk), 1)\n",
    "\n",
    "            # スコア計算\n",
    "            scores_chunk = model._calculate_score(query_vector_tiled, candidates_chunk)\n",
    "            \n",
    "            # 結果をCPUに移してリストに追加 (VRAM解放)\n",
    "            all_scores.append(scores_chunk.cpu().numpy())\n",
    "            \n",
    "            # 明示的に削除してVRAM確保\n",
    "            del query_vector_tiled\n",
    "            del scores_chunk\n",
    "        \n",
    "        # 全スコアを結合\n",
    "        final_scores = np.vstack(all_scores).flatten()\n",
    "        \n",
    "        # 3. スコア順に並び替え\n",
    "        sorted_indices = np.argsort(final_scores)[::-1] # 降順\n",
    "        \n",
    "        # 4. 採点 と 結果保存\n",
    "        first_hit_rank = 0 \n",
    "        hits_count = 0\n",
    "        hits_at_k = {k: 0 for k in EVAL_K_VALUES}\n",
    "        top_k_results_list = []\n",
    "        ranks_of_all_hits = []\n",
    "        current_rank = 1\n",
    "        \n",
    "        for idx in sorted_indices:\n",
    "            if current_rank > SAVE_TOP_K and hits_count == len(ground_truth):\n",
    "                break\n",
    "                \n",
    "            if idx not in id_to_doi: continue\n",
    "            doi = id_to_doi[idx]\n",
    "            if doi in train_dois: continue\n",
    "            \n",
    "            is_correct = (doi in ground_truth)\n",
    "            \n",
    "            if is_correct:\n",
    "                hits_count += 1\n",
    "                ranks_of_all_hits.append(current_rank)\n",
    "                if first_hit_rank == 0:\n",
    "                    first_hit_rank = current_rank\n",
    "            \n",
    "            for k in EVAL_K_VALUES:\n",
    "                if current_rank <= k and is_correct:\n",
    "                    hits_at_k[k] += 1\n",
    "            \n",
    "            if current_rank <= SAVE_TOP_K:\n",
    "                top_k_results_list.append({\n",
    "                    \"rank\": current_rank,\n",
    "                    \"doi\": doi,\n",
    "                    \"score\": float(final_scores[idx]),\n",
    "                    \"is_correct\": is_correct\n",
    "                })\n",
    "            \n",
    "            current_rank += 1 \n",
    "\n",
    "        # 5. 集計\n",
    "        all_first_hit_ranks.append(first_hit_rank)\n",
    "            \n",
    "        for k in EVAL_K_VALUES:\n",
    "            recall = hits_at_k[k] / len(ground_truth) if ground_truth else 0\n",
    "            all_recalls_at_k[k].append(recall)\n",
    "            \n",
    "        all_rich_results.append({\n",
    "            \"query_doi\": query_data[\"query_doi\"],\n",
    "            \"total_ground_truth\": len(ground_truth),\n",
    "            \"ground_truth_dois\": list(ground_truth),\n",
    "            \"first_hit_rank\": first_hit_rank,\n",
    "            \"ranks_of_all_hits\": ranks_of_all_hits,\n",
    "            \"top_k_results\": top_k_results_list\n",
    "        })\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4abf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rich evaluation results to data/processed/ranknet_v4_evaluation_results.json...\n",
      "Rich results saved.\n",
      "\n",
      "==================================================\n",
      "--- Final Evaluation Results: S-BERT (RankNet v4) ---\n",
      "(Based on 50 queries, scoring ALL 11,619,136 documents)\n",
      "==================================================\n",
      "MRR (Mean Reciprocal Rank): 0.0003\n",
      "Ranks of First Hit (0 = Not Found):\n",
      "[1122, 1629, 496, 6816, 16003, 403, 84983, 83030, 3072, 47930, 414548, 202572, 7744, 113385, 146150, 558062, 213067, 3522, 17964, 14237, 660, 2013739, 414, 2485, 8723, 102664, 825798, 228892, 225148, 9235003, 10853678, 205815, 3436, 744877, 162885, 3253, 91634, 605134, 60003, 258781, 234485, 5317290, 122220, 74552, 293573, 114330, 442, 555053, 21202, 407755]\n",
      "\n",
      "--- Recall@K ---\n",
      "Recall@1   : 0.0000 (0.00%)\n",
      "Recall@5   : 0.0000 (0.00%)\n",
      "Recall@10  : 0.0000 (0.00%)\n",
      "Recall@30  : 0.0000 (0.00%)\n",
      "Recall@50  : 0.0000 (0.00%)\n",
      "Recall@100 : 0.0000 (0.00%)\n",
      "Recall@200 : 0.0000 (0.00%)\n",
      "Recall@300 : 0.0000 (0.00%)\n",
      "Recall@500 : 0.0178 (1.78%)\n",
      "Recall@1000: 0.0305 (3.05%)\n",
      "Recall@2000: 0.0324 (3.24%)\n",
      "Recall@3000: 0.0391 (3.91%)\n",
      "Recall@4000: 0.0604 (6.04%)\n",
      "Recall@5000: 0.0654 (6.54%)\n",
      "Recall@6000: 0.0714 (7.14%)\n",
      "Recall@7000: 0.0731 (7.31%)\n",
      "Recall@8000: 0.0767 (7.67%)\n",
      "Recall@9000: 0.0834 (8.34%)\n",
      "Recall@10000: 0.0834 (8.34%)\n",
      "Recall@11000: 0.0834 (8.34%)\n",
      "Recall@12000: 0.0848 (8.48%)\n",
      "Recall@13000: 0.0870 (8.70%)\n",
      "Recall@14000: 0.0923 (9.23%)\n",
      "Recall@15000: 0.1138 (11.38%)\n",
      "Recall@16000: 0.1145 (11.45%)\n",
      "Recall@17000: 0.1230 (12.30%)\n",
      "Recall@18000: 0.1333 (13.33%)\n",
      "Recall@19000: 0.1400 (14.00%)\n",
      "Recall@20000: 0.1497 (14.97%)\n",
      "Recall@21000: 0.1497 (14.97%)\n",
      "Recall@22000: 0.1547 (15.47%)\n",
      "Recall@23000: 0.1554 (15.54%)\n",
      "Recall@24000: 0.1564 (15.64%)\n",
      "Recall@25000: 0.1564 (15.64%)\n",
      "Recall@26000: 0.1564 (15.64%)\n",
      "Recall@27000: 0.1571 (15.71%)\n",
      "Recall@28000: 0.1571 (15.71%)\n",
      "Recall@29000: 0.1591 (15.91%)\n",
      "Recall@30000: 0.1630 (16.30%)\n",
      "==================================================\n",
      "Queries where first hit was NOT found: 0 / 50\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 最終結果の集計 ---\n",
    "\n",
    "print(f\"Saving rich evaluation results to {RICH_RESULTS_FILE}...\")\n",
    "with open(RICH_RESULTS_FILE, 'w') as f:\n",
    "    json.dump(all_rich_results, f, indent=2)\n",
    "print(\"Rich results saved.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"--- Final Evaluation Results: S-BERT (RankNet v4) ---\")\n",
    "print(f\"(Based on {len(evaluation_queries)} queries, scoring ALL {total_vectors:,} documents)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# MRR\n",
    "mrr_scores = [1.0 / r for r in all_first_hit_ranks if r > 0]\n",
    "mrr = np.mean(mrr_scores) if mrr_scores else 0.0\n",
    "print(f\"MRR (Mean Reciprocal Rank): {mrr:.4f}\")\n",
    "\n",
    "print(f\"Ranks of First Hit (0 = Not Found):\")\n",
    "print(all_first_hit_ranks)\n",
    "\n",
    "print(\"\\n--- Recall@K ---\")\n",
    "for k in EVAL_K_VALUES:\n",
    "    recall_k = np.mean(all_recalls_at_k[k])\n",
    "    print(f\"Recall@{k:<4}: {recall_k:.4f} ({(recall_k * 100):.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "not_found_count = len([r for r in all_first_hit_ranks if r == 0])\n",
    "print(f\"Queries where first hit was NOT found: {not_found_count} / {len(evaluation_queries)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

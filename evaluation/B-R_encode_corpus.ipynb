{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2030d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available: NVIDIA RTX A6000\n",
      "Loading tokenizer & model from: models/sbert_ranknet_v4/best_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2fa9b011c54be9a6ff735dcaabbdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4594803b874c8eba00899c91f120ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading DOIs:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34de2a0e8e884db39bf0d88af1649643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging Embeddings:   0%|          | 0/11620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# セル 1\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# セル 2: モデル定義 (ロード用)\n",
    "class SiameseRankNetModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseRankNetModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "        self.init_weights()\n",
    "    \n",
    "    def _get_vector(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output \n",
    "\n",
    "    def forward(self, input_ids=None, **kwargs): pass\n",
    "\n",
    "# セル 3: 設定\n",
    "DB_PATH = \"data/processed/s2orc_filtered.db\"\n",
    "# ▼▼▼ 今回訓練したモデルのパス ▼▼▼\n",
    "TRAINED_MODEL_PATH = \"models/sbert_ranknet_v4/best_model\" \n",
    "\n",
    "EMBEDDINGS_OUTPUT_FILE = \"data/processed/ranknet_v4_scibert_cls_embeddings.npy\"\n",
    "DOI_MAP_OUTPUT_FILE = \"data/processed/ranknet_v4_doi_map.json\"\n",
    "TEMP_EMBED_DIR = \"data/processed/embeddings_tmp_ranknet_v4\"\n",
    "TEMP_DOI_DIR = \"data/processed/dois_tmp_ranknet_v4\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "INFERENCE_BATCH_SIZE = 512\n",
    "\n",
    "print(f\"Loading tokenizer & model from: {TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)\n",
    "model = SiameseRankNetModel.from_pretrained(TRAINED_MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "# セル 4: DB読み込みジェネレータ\n",
    "def get_abstract_batches(db_path, batch_size=1000):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        total_rows = cursor.execute(\"SELECT COUNT(doi) FROM papers WHERE abstract IS NOT NULL AND abstract != ''\").fetchone()[0]\n",
    "        yield total_rows \n",
    "        cursor.execute(\"SELECT doi, abstract FROM papers WHERE abstract IS NOT NULL AND abstract != ''\")\n",
    "        batch = []\n",
    "        for row in cursor:\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size: yield batch; batch = []\n",
    "        if batch: yield batch\n",
    "\n",
    "# セル 5: ベクトル化実行\n",
    "os.makedirs(TEMP_EMBED_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DOI_DIR, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_generator = get_abstract_batches(DB_PATH, batch_size=1000)\n",
    "    total_rows = next(batch_generator)\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(batch_generator), total=(total_rows+999)//1000, desc=\"Vectorizing\"):\n",
    "        dois, abstracts = zip(*batch)\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for j in range(0, len(abstracts), INFERENCE_BATCH_SIZE):\n",
    "            sub_abstracts = abstracts[j : j + INFERENCE_BATCH_SIZE]\n",
    "            inputs = tokenizer(list(sub_abstracts), padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "            embeddings = model._get_vector(inputs['input_ids'], inputs['attention_mask'])\n",
    "            batch_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "        np.save(os.path.join(TEMP_EMBED_DIR, f\"batch_{i:05d}.npy\"), np.vstack(batch_embeddings).astype(np.float32))\n",
    "        with open(os.path.join(TEMP_DOI_DIR, f\"batch_{i:05d}.json\"), 'w') as f: json.dump(dois, f)\n",
    "\n",
    "# セル 6: 結合と保存\n",
    "all_dois = []\n",
    "doi_files = sorted([f for f in os.listdir(TEMP_DOI_DIR) if f.endswith('.json')])\n",
    "for f in tqdm(doi_files, desc=\"Reading DOIs\"):\n",
    "    with open(os.path.join(TEMP_DOI_DIR, f), 'r') as fp: all_dois.extend(json.load(fp))\n",
    "\n",
    "final_embeddings = np.memmap(EMBEDDINGS_OUTPUT_FILE, dtype=np.float32, mode='w+', shape=(len(all_dois), 768))\n",
    "idx = 0\n",
    "for f in tqdm(doi_files, desc=\"Merging Embeddings\"):\n",
    "    data = np.load(os.path.join(TEMP_EMBED_DIR, f.replace('.json', '.npy')))\n",
    "    final_embeddings[idx : idx + len(data)] = data\n",
    "    idx += len(data)\n",
    "final_embeddings.flush()\n",
    "\n",
    "with open(DOI_MAP_OUTPUT_FILE, 'w') as f: json.dump({doi: i for i, doi in enumerate(all_dois)}, f)\n",
    "shutil.rmtree(TEMP_EMBED_DIR)\n",
    "shutil.rmtree(TEMP_DOI_DIR)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

Loading dataset from: data/processed/training_dataset_abstract_cleaned_v3.csv
Converting dataset to Triplets (Anchor, Positive, Negative)...
Performing Group Shuffle Split based on 'anchor' (or 'abstract_a')...
Train set: 5840, Validation set: 1171
Tokenizing...
Map (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5840/5840 [00:04<00:00, 1447.47 examples/s]
Map (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1171/1171 [00:01<00:00, 902.89 examples/s]
Loading model: BAAI/bge-reranker-v2-m3
Some weights of CrossEncoderMarginModel were not initialized from the model checkpoint at BAAI/bge-reranker-v2-m3 and are newly initialized: ['scorer.classifier.dense.bias', 'scorer.classifier.dense.weight', 'scorer.classifier.out_proj.bias', 'scorer.classifier.out_proj.weight', 'scorer.roberta.embeddings.LayerNorm.bias', 'scorer.roberta.embeddings.LayerNorm.weight', 'scorer.roberta.embeddings.position_embeddings.weight', 'scorer.roberta.embeddings.token_type_embeddings.weight', 'scorer.roberta.embeddings.word_embeddings.weight', 'scorer.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.0.attention.output.dense.bias', 'scorer.roberta.encoder.layer.0.attention.output.dense.weight', 'scorer.roberta.encoder.layer.0.attention.self.key.bias', 'scorer.roberta.encoder.layer.0.attention.self.key.weight', 'scorer.roberta.encoder.layer.0.attention.self.query.bias', 'scorer.roberta.encoder.layer.0.attention.self.query.weight', 'scorer.roberta.encoder.layer.0.attention.self.value.bias', 'scorer.roberta.encoder.layer.0.attention.self.value.weight', 'scorer.roberta.encoder.layer.0.intermediate.dense.bias', 'scorer.roberta.encoder.layer.0.intermediate.dense.weight', 'scorer.roberta.encoder.layer.0.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.0.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.0.output.dense.bias', 'scorer.roberta.encoder.layer.0.output.dense.weight', 'scorer.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.1.attention.output.dense.bias', 'scorer.roberta.encoder.layer.1.attention.output.dense.weight', 'scorer.roberta.encoder.layer.1.attention.self.key.bias', 'scorer.roberta.encoder.layer.1.attention.self.key.weight', 'scorer.roberta.encoder.layer.1.attention.self.query.bias', 'scorer.roberta.encoder.layer.1.attention.self.query.weight', 'scorer.roberta.encoder.layer.1.attention.self.value.bias', 'scorer.roberta.encoder.layer.1.attention.self.value.weight', 'scorer.roberta.encoder.layer.1.intermediate.dense.bias', 'scorer.roberta.encoder.layer.1.intermediate.dense.weight', 'scorer.roberta.encoder.layer.1.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.1.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.1.output.dense.bias', 'scorer.roberta.encoder.layer.1.output.dense.weight', 'scorer.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.10.attention.output.dense.bias', 'scorer.roberta.encoder.layer.10.attention.output.dense.weight', 'scorer.roberta.encoder.layer.10.attention.self.key.bias', 'scorer.roberta.encoder.layer.10.attention.self.key.weight', 'scorer.roberta.encoder.layer.10.attention.self.query.bias', 'scorer.roberta.encoder.layer.10.attention.self.query.weight', 'scorer.roberta.encoder.layer.10.attention.self.value.bias', 'scorer.roberta.encoder.layer.10.attention.self.value.weight', 'scorer.roberta.encoder.layer.10.intermediate.dense.bias', 'scorer.roberta.encoder.layer.10.intermediate.dense.weight', 'scorer.roberta.encoder.layer.10.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.10.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.10.output.dense.bias', 'scorer.roberta.encoder.layer.10.output.dense.weight', 'scorer.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'scorer.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'scorer.roberta.encoder.layer.11.attention.output.dense.bias', 'scorer.roberta.encoder.layer.11.attention.output.dense.weight', 'scorer.roberta.encoder.layer.11.attention.self.key.bias', 'scorer.roberta.encoder.layer.11.attention.self.key.weight', 'scorer.roberta.encoder.layer.11.attention.self.query.bias', 'scorer.roberta.encoder.layer.11.attention.self.query.weight', 'scorer.roberta.encoder.layer.11.attention.self.value.bias', 'scorer.roberta.encoder.layer.11.attention.self.value.weight', 'scorer.roberta.encoder.layer.11
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Early stopping enabled with patience: 3
/home/kurokawa/Projects/S2ORCRanker/src/training/trainer.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MarginRankingTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Starting training...
                                                                                                                                                                                                  
{'loss': 1.0286, 'grad_norm': 80.67028045654297, 'learning_rate': 1.0273972602739726e-07, 'epoch': 0.0}
{'loss': 1.1287, 'grad_norm': 74.43391418457031, 'learning_rate': 2.1689497716894978e-07, 'epoch': 0.0}
{'loss': 1.1524, 'grad_norm': 78.4277114868164, 'learning_rate': 3.310502283105023e-07, 'epoch': 0.01}
{'loss': 0.896, 'grad_norm': 80.2348403930664, 'learning_rate': 4.452054794520548e-07, 'epoch': 0.01}
{'loss': 1.0951, 'grad_norm': 77.00043487548828, 'learning_rate': 5.593607305936073e-07, 'epoch': 0.01}
                                                                                                                                                                                                  
{'eval_loss': 0.9994948506355286, 'eval_runtime': 394.58, 'eval_samples_per_second': 2.968, 'eval_steps_per_second': 2.968, 'epoch': 0.01}
{'loss': 0.958, 'grad_norm': 77.16425323486328, 'learning_rate': 6.735159817351598e-07, 'epoch': 0.01}
{'loss': 1.0158, 'grad_norm': 72.4515380859375, 'learning_rate': 7.876712328767124e-07, 'epoch': 0.01}
{'loss': 0.8993, 'grad_norm': 69.40263366699219, 'learning_rate': 9.018264840182649e-07, 'epoch': 0.01}
{'loss': 0.9421, 'grad_norm': 70.62908172607422, 'learning_rate': 1.0159817351598173e-06, 'epoch': 0.02}
{'loss': 1.0073, 'grad_norm': 69.22880554199219, 'learning_rate': 1.13013698630137e-06, 'epoch': 0.02}
{'eval_loss': 0.9987688660621643, 'eval_runtime': 394.1344, 'eval_samples_per_second': 2.971, 'eval_steps_per_second': 2.971, 'epoch': 0.02}
{'loss': 0.856, 'grad_norm': 64.89927673339844, 'learning_rate': 1.2442922374429224e-06, 'epoch': 0.02}
{'loss': 1.0258, 'grad_norm': 65.05386352539062, 'learning_rate': 1.358447488584475e-06, 'epoch': 0.02}
{'loss': 0.8789, 'grad_norm': 59.50712966918945, 'learning_rate': 1.4726027397260275e-06, 'epoch': 0.02}
{'loss': 0.8841, 'grad_norm': 58.83030700683594, 'learning_rate': 1.5867579908675801e-06, 'epoch': 0.02}
{'loss': 1.1746, 'grad_norm': 57.180809020996094, 'learning_rate': 1.7009132420091326e-06, 'epoch': 0.03}
{'eval_loss': 0.9955578446388245, 'eval_runtime': 394.2268, 'eval_samples_per_second': 2.97, 'eval_steps_per_second': 2.97, 'epoch': 0.03}
{'loss': 1.0601, 'grad_norm': 55.95252990722656, 'learning_rate': 1.8150684931506852e-06, 'epoch': 0.03}
{'loss': 0.9015, 'grad_norm': 50.71930694580078, 'learning_rate': 1.9292237442922376e-06, 'epoch': 0.03}
{'loss': 1.0922, 'grad_norm': 46.69029998779297, 'learning_rate': 2.04337899543379e-06, 'epoch': 0.03}
{'loss': 0.8658, 'grad_norm': 48.908203125, 'learning_rate': 2.1575342465753425e-06, 'epoch': 0.03}
{'loss': 0.8695, 'grad_norm': 48.17256164550781, 'learning_rate': 2.2716894977168953e-06, 'epoch': 0.03}
{'eval_loss': 0.9951526522636414, 'eval_runtime': 394.0696, 'eval_samples_per_second': 2.972, 'eval_steps_per_second': 2.972, 'epoch': 0.03}
{'loss': 0.8518, 'grad_norm': 43.62661361694336, 'learning_rate': 2.3858447488584478e-06, 'epoch': 0.04}
{'loss': 1.0682, 'grad_norm': 42.57529830932617, 'learning_rate': 2.5e-06, 'epoch': 0.04}
{'loss': 0.9286, 'grad_norm': 42.938072204589844, 'learning_rate': 2.614155251141553e-06, 'epoch': 0.04}
{'loss': 1.0531, 'grad_norm': 40.47333526611328, 'learning_rate': 2.728310502283105e-06, 'epoch': 0.04}
{'loss': 0.8725, 'grad_norm': 38.612220764160156, 'learning_rate': 2.842465753424658e-06, 'epoch': 0.04}
{'eval_loss': 0.996179461479187, 'eval_runtime': 394.0166, 'eval_samples_per_second': 2.972, 'eval_steps_per_second': 2.972, 'epoch': 0.04}
{'loss': 1.0656, 'grad_norm': 38.33600616455078, 'learning_rate': 2.9566210045662103e-06, 'epoch': 0.04}
{'loss': 0.8608, 'grad_norm': 36.66815948486328, 'learning_rate': 3.070776255707763e-06, 'epoch': 0.05}
{'loss': 1.0724, 'grad_norm': 37.94717025756836, 'learning_rate': 3.184931506849315e-06, 'epoch': 0.05}
{'loss': 1.006, 'grad_norm': 35.67967224121094, 'learning_rate': 3.299086757990868e-06, 'epoch': 0.05}
{'loss': 1.0962, 'grad_norm': 37.43656539916992, 'learning_rate': 3.4132420091324205e-06, 'epoch': 0.05}
{'eval_loss': 0.9976499080657959, 'eval_runtime': 393.9139, 'eval_samples_per_second': 2.973, 'eval_steps_per_second': 2.973, 'epoch': 0.05}
{'loss': 1.1346, 'grad_norm': 33.09691619873047, 'learning_rate': 3.527397260273973e-06, 'epoch': 0.05}
{'loss': 0.9869, 'grad_norm': 34.066829681396484, 'learning_rate': 3.6415525114155254e-06, 'epoch': 0.05}
{'loss': 0.9499, 'grad_norm': 35.221107482910156, 'learning_rate': 3.755707762557078e-06, 'epoch': 0.06}
{'loss': 0.8907, 'grad_norm': 32.8131103515625, 'learning_rate': 3.869863013698631e-06, 'epoch': 0.06}
{'loss': 1.0074, 'grad_norm': 31.887407302856445, 'learning_rate': 3.9840182648401835e-06, 'epoch': 0.06}
{'eval_loss': 0.9977829456329346, 'eval_runtime': 393.8928, 'eval_samples_per_second': 2.973, 'eval_steps_per_second': 2.973, 'epoch': 0.06}
{'train_runtime': 3165.8437, 'train_samples_per_second': 5.534, 'train_steps_per_second': 5.534, 'train_loss': 0.9878978429521833, 'epoch': 0.06}
Saving best model to /home/kurokawa/Projects/S2ORCRanker/models/checkpoints/cross_encoder/Cross_BGE_M3_RandomNeg/best_model

# configs/evaluate.yaml

defaults:
  - _self_

debug: false

seed: 42

logging:
  use_wandb: true
  project_name: "s2orc-ranker"
  run_name: "eval_retriever_bi_encoder"
  tags: ["evaluation", "retriever", "bi-encoder"]

# データ設定
data:
  # 全文(Abstract)が格納されているSQLite DB
  db_path: "data/processed/s2orc_filtered.db"
  
  # 評価用データセット（クエリと正解ペア）
  eval_data_path: "data/datapapers/sampled/evaluation_data_papers_50_v2.csv"
  
  # ベクトル保存先ディレクトリ
  output_dir: "data/processed/embeddings"
  
  # 保存ファイル名
  embeddings_file: "corpus_embeddings.npy"
  doi_map_file: "corpus_doi_map.json"

# モデル設定
model:
  # 学習済みモデルのパス（run_train.pyの出力先を指定）
  # 例: models/output_model/best_model
  path: "models/output_model/best_model"
  
  # ベースモデル名（トークナイザ読み込み用）
  base_name: "allenai/scibert_scivocab_uncased"
  max_length: 512
  
  # 推論時のバッチサイズ (VRAMに合わせて調整: A6000なら大きくできる)
  batch_size: 256 # A6000なら512~1024でもいけるかも

# 評価設定
evaluation:
  # Recall@K の Kリスト
  k_values: [1, 5, 10, 50, 100, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000]
  
  # 結果の保存先
  result_file: "evaluation_results.json"

  use_faiss: true
  gpu_search: true

  save_candidates: true
  candidates_file: "candidates_for_reranking.json"
  candidates_k: 50000
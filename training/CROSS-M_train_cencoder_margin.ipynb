{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    BertPreTrainedModel, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import accuracy_score\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# CUDAのデバッグ用\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 設定 (本番用) ---\n",
    "\n",
    "TRAINING_FILE = \"data/processed/training_dataset_abstract_cleaned_v3.csv\"\n",
    "MODEL_CHECKPOINT = \"allenai/longformer-base-4096\"\n",
    "OUTPUT_MODEL_DIR = \"models/cencoder_margin_v1\" # 本番用の保存先\n",
    "\n",
    "# --- 実験計画からのハイパーパラメータ ---\n",
    "MAX_LENGTH = 2048\n",
    "LEARNING_RATE = 2e-5\n",
    "MARGIN_RANKING_MARGIN = 1.0\n",
    "EPOCHS = 3       # ▼▼▼ 3エポックで実験 ▼▼▼\n",
    "BATCH_SIZE = 16  # ▼▼▼ バッチサイズ16 ▼▼▼\n",
    "GRAD_ACCUM_STEPS = 4 # ▼▼▼ 勾配累積4 (実質バッチ 16*4=64) ▼▼▼\n",
    "\n",
    "print(f\"Configuration set for PRODUCTION (Batch Size: {BATCH_SIZE}, Grad Accum: {GRAD_ACCUM_STEPS}, Epochs: {EPOCHS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c70b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. カスタムモデルクラスの定義 ---\n",
    "\n",
    "class CrossEncoderMarginModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    C-Encoder (Margin) モデル\n",
    "    内部で C-Encoder (BCE) と同じモデルを2回呼び出す\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(CrossEncoderMarginModel, self).__init__(config)\n",
    "        # Longformerベースのスコア計算機 (num_labels=1)\n",
    "        self.scorer = AutoModelForSequenceClassification.from_config(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        input_ids_neg=None,\n",
    "        attention_mask_neg=None,\n",
    "        labels=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        # 1. ポジティブペアのスコアを計算 (入力: (Anchor, Positive))\n",
    "        output_pos = self.scorer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        score_positive = output_pos.logits\n",
    "\n",
    "        # 2. ネガティブペアのスコアを計算 (入力: (Anchor, Negative))\n",
    "        output_neg = self.scorer(input_ids=input_ids_neg, attention_mask=attention_mask_neg)\n",
    "        score_negative = output_neg.logits\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=None,\n",
    "            logits=(score_positive, score_negative),\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "print(\"Custom model class 'CrossEncoderMarginModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. データセットの読み込みとTripletへの変換 ---\n",
    "print(f\"Loading full dataset: {TRAINING_FILE}\")\n",
    "df_full = pd.read_csv(TRAINING_FILE)\n",
    "df_full = df_full.dropna(subset=['abstract_a', 'abstract_b', 'label'])\n",
    "df_full['label'] = df_full['label'].astype(int)\n",
    "print(f\"Full dataset size: {len(df_full)}\")\n",
    "\n",
    "# 全データセットを正例ペアと負例ペアに分割\n",
    "pos_df = df_full[df_full['label'] == 1]\n",
    "neg_df = df_full[df_full['label'] == 0]\n",
    "\n",
    "if pos_df.empty or neg_df.empty:\n",
    "    raise ValueError(\"Full dataset must contain both positive and negative samples.\")\n",
    "\n",
    "# --- Tripletの作成 (Anchor, Positive, Negative) ---\n",
    "print(\"Creating triplets from full dataset...\")\n",
    "triplets = []\n",
    "negative_abstracts = neg_df['abstract_b'].tolist()\n",
    "\n",
    "for index, row in tqdm(pos_df.iterrows(), total=len(pos_df), desc=\"Creating Triplets\"):\n",
    "    anchor = row['abstract_a']\n",
    "    positive = row['abstract_b']\n",
    "    negative = np.random.choice(negative_abstracts)\n",
    "    \n",
    "    triplets.append({\n",
    "        'anchor': anchor,\n",
    "        'positive': positive,\n",
    "        'negative': negative\n",
    "    })\n",
    "\n",
    "df_triplets = pd.DataFrame(triplets)\n",
    "df_triplets['labels'] = 0 # ダミーのlabels列\n",
    "print(f\"Created {len(df_triplets)} triplets.\")\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df_triplets)\n",
    "# 本番なので、検証データ(validation)も多め(20%)に確保\n",
    "dataset_split = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_split['train'],\n",
    "    'validation': dataset_split['test']\n",
    "})\n",
    "print(f\"Triplet dataset loaded: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. トークナイズ処理 ---\n",
    "print(\"Initializing Longformer tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_cencoder_margin_function(examples):\n",
    "    # (Anchor, Positive) ペアをトークン化\n",
    "    tokenized_pos = tokenizer(\n",
    "        examples[\"anchor\"], examples[\"positive\"], \n",
    "        padding=\"max_length\", truncation=True, max_length=MAX_LENGTH\n",
    "    )\n",
    "    # (Anchor, Negative) ペアをトークン化\n",
    "    tokenized_neg = tokenizer(\n",
    "        examples[\"anchor\"], examples[\"negative\"], \n",
    "        padding=\"max_length\", truncation=True, max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized_pos[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_pos[\"attention_mask\"],\n",
    "        \"input_ids_neg\": tokenized_neg[\"input_ids\"],\n",
    "        \"attention_mask_neg\": tokenized_neg[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing C-Encoder Margin dataset...\")\n",
    "tokenized_datasets = dataset.map(tokenize_cencoder_margin_function, batched=True, num_proc=4,\n",
    "                                 remove_columns=[\"anchor\", \"positive\", \"negative\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. カスタムTrainerの定義 ---\n",
    "\n",
    "class MarginRankingTrainer(Trainer):\n",
    "    def __init__(self, *args, margin=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.margin = margin\n",
    "        self.loss_fct = nn.MarginRankingLoss(margin=self.margin)\n",
    "        print(f\"MarginRankingTrainer initialized with margin={self.margin}\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if \"labels\" in inputs:\n",
    "            inputs.pop(\"labels\")\n",
    "            \n",
    "        outputs = model(**inputs)\n",
    "        score_positive, score_negative = outputs.logits\n",
    "        \n",
    "        # ターゲット 'y' は 1 を設定\n",
    "        target = torch.ones_like(score_positive)\n",
    "        loss = self.loss_fct(score_positive, score_negative, target)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"Custom 'MarginRankingTrainer' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. モデルのロードと訓練設定 ---\n",
    "print(f\"Loading custom model: {MODEL_CHECKPOINT}\")\n",
    "# ★num_labels=1★ でロード (スコア計算機として)\n",
    "model = CrossEncoderMarginModel.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=1 \n",
    ").to(device)\n",
    "print(\"Custom model loaded.\")\n",
    "\n",
    "# 評価指標を計算する関数\n",
    "def compute_metrics_margin(eval_pred):\n",
    "    score_pos, score_neg = eval_pred.predictions\n",
    "    preds = (score_pos.squeeze() > score_neg.squeeze())\n",
    "    accuracy = preds.mean().item()\n",
    "    return {'rank_accuracy': accuracy}\n",
    "\n",
    "# 訓練の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE, # 16\n",
    "    per_device_eval_batch_size=BATCH_SIZE, # 16\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS, # 4\n",
    "    num_train_epochs=EPOCHS, # 3\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # エポックごとに評価\n",
    "    save_strategy=\"epoch\",       # エポックごとに保存\n",
    "    load_best_model_at_end=True, # 最高のモデルを最後にロード\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,           # 100ステップごとにログ表示\n",
    "    warmup_ratio=0.1,            # 10%ウォームアップ\n",
    "    gradient_checkpointing=True, # メモリ節約\n",
    ")\n",
    "print(\"Training arguments set for PRODUCTION.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. 訓練の開始 ---\n",
    "trainer = MarginRankingTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_margin,\n",
    "    margin=MARGIN_RANKING_MARGIN\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Model Training (C-Encoder Margin PRODUCTION) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dca466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. モデルの保存 ---\n",
    "print(\"Training complete. Saving best model...\")\n",
    "best_model_path = os.path.join(OUTPUT_MODEL_DIR, \"best_model\")\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. 訓練結果の可視化 ---\n",
    "print(\"\\n--- Visualizing Training Results ---\")\n",
    "log_history = trainer.state.log_history\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "df_train = df_log[df_log['loss'].notna()].copy()\n",
    "df_eval = df_log[df_log['eval_loss'].notna()].copy()\n",
    "\n",
    "if 'epoch' in df_train.columns:\n",
    "    df_train['epoch'] = df_train['epoch'].astype(int)\n",
    "if 'epoch' in df_eval.columns:\n",
    "    df_eval['epoch'] = df_eval['epoch'].astype(int)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "if not df_train.empty:\n",
    "    sns.lineplot(data=df_train, x='epoch', y='loss', label='Training Loss', ax=ax1, marker='o')\n",
    "if not df_eval.empty:\n",
    "    sns.lineplot(data=df_eval, x='epoch', y='eval_loss', label='Validation Loss', ax=ax1, marker='o')\n",
    "ax1.set_title('Training vs. Validation Loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "if not df_eval.empty:\n",
    "    sns.lineplot(data=df_eval, x='epoch', y='eval_rank_accuracy', label='Validation Rank Accuracy', ax=ax2, marker='o')\n",
    "ax2.set_title('Validation Metrics')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Best Model Evaluation Metrics (from validation set) ---\")\n",
    "if not df_eval.empty:\n",
    "    best_run = df_eval.loc[df_eval['eval_loss'].idxmin()]\n",
    "    print(f\"Best Epoch (based on min eval_loss): {best_run['epoch']}\")\n",
    "    print(f\"Best Validation Loss: {best_run['eval_loss']:.4f}\")\n",
    "    print(f\"Best Validation Rank Accuracy: {best_run['eval_rank_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"No evaluation steps were completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

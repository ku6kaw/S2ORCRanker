{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ef3993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertPreTrainedModel, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f151ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set for Contrastive Loss.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "\n",
    "# 最終版のクリーニング済みデータ\n",
    "TRAINING_FILE = \"data/processed/training_dataset_abstract_cleaned_v3.csv\"\n",
    "\n",
    "# 使用するベースモデル（SciBERT）\n",
    "MODEL_CHECKPOINT = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# 訓練済みモデルの保存先\n",
    "OUTPUT_MODEL_DIR = \"models/sbert_contrastive_v1\"\n",
    "\n",
    "# モデルのハイパーパラメータ\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16 # Colab T4 GPU (16GB) を想定\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "CONTRASTIVE_MARGIN = 0.5 # コントラスティブ損失のマージン\n",
    "METRICS_THRESHOLD = 0.5 # 評価時に「正例」と判断する距離のしきい値\n",
    "\n",
    "print(\"Configuration set for Contrastive Loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe500ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model class 'SiameseContrastiveModel' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. カスタムモデルクラスの定義 ---\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    \"\"\"アテンションマスクを考慮したMean Pooling層\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class SiameseContrastiveModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    S-BERT (Contrastive) モデル\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SiameseContrastiveModel, self).__init__(config)\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        input_ids_b=None,\n",
    "        attention_mask_b=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 論文Aと論文Bを、同じ重みのBERTで個別に処理\n",
    "        output_a = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "\n",
    "        # Mean Poolingでベクトル化\n",
    "        vec_x = self.pooler(output_a.last_hidden_state, attention_mask)\n",
    "        vec_y = self.pooler(output_b.last_hidden_state, attention_mask_b)\n",
    "\n",
    "        # 損失(loss)はこのモデル内では計算せず、Trainer側で計算する\n",
    "        # logitsフィールドに、計算した2つのベクトルをタプルとして渡す\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=None,\n",
    "            logits=(vec_x, vec_y),\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "print(\"Custom model class 'SiameseContrastiveModel' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3ee09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: data/processed/training_dataset_abstract_cleaned_v3.csv\n",
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 27699\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 6925\n",
      "    })\n",
      "})\n",
      "Initializing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset for Siamese model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1227664bb3f48d6978164ea3ba12319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/27699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb72277464942c092635341e44fcd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. データセットの読み込みとトークン化 ---\n",
    "print(f\"Loading dataset: {TRAINING_FILE}\")\n",
    "df = pd.read_csv(TRAINING_FILE)\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b', 'label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "dataset_split = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_split['train'],\n",
    "    'validation': dataset_split['test']\n",
    "})\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_siamese_function(examples):\n",
    "    tokenized_a = tokenizer(examples[\"abstract_a\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    tokenized_b = tokenizer(examples[\"abstract_b\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    return {\n",
    "        \"input_ids\": tokenized_a[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_a[\"attention_mask\"],\n",
    "        \"input_ids_b\": tokenized_b[\"input_ids\"],\n",
    "        \"attention_mask_b\": tokenized_b[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing dataset for Siamese model...\")\n",
    "tokenized_datasets = dataset.map(tokenize_siamese_function, batched=True, num_proc=4)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"abstract_a\", \"abstract_b\", \"data_paper_doi\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afd911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom 'ContrastiveTrainer' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. カスタムTrainerの定義 ---\n",
    "\n",
    "class ContrastiveTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Contrastive Lossを計算するためにTrainerを継承\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, margin=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.margin = margin\n",
    "        print(f\"ContrastiveTrainer initialized with margin={self.margin}\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # 'labels' を inputs 辞書から取り出す\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # モデルのforwardパスを実行（vec_x, vec_yがlogitsとして返ってくる）\n",
    "        outputs = model(**inputs)\n",
    "        vec_x, vec_y = outputs.logits\n",
    "        \n",
    "        # 損失関数の計算 (Cosine Distance)\n",
    "        distance = 1 - F.cosine_similarity(vec_x, vec_y)\n",
    "        \n",
    "        # Contrastive Lossの計算\n",
    "        loss_positive = distance\n",
    "        loss_negative = F.relu(self.margin - distance) # marginより遠ければ0\n",
    "        \n",
    "        # ラベルに応じて損失を適用\n",
    "        loss = (labels.float() * loss_positive) + ((1 - labels.float()) * loss_negative)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"Custom 'ContrastiveTrainer' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2429605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom model: allenai/scibert_scivocab_uncased\n",
      "Custom model loaded.\n",
      "Training arguments and metrics set.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. モデルのロードと訓練設定 ---\n",
    "print(f\"Loading custom model: {MODEL_CHECKPOINT}\")\n",
    "model = SiameseContrastiveModel.from_pretrained(MODEL_CHECKPOINT, num_labels=2).to(device)\n",
    "print(\"Custom model loaded.\")\n",
    "\n",
    "# 評価指標を計算する関数 (距離ベース)\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred.predictions は (vec_x, vec_y) のタプル\n",
    "    vec_x, vec_y = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # 距離を計算\n",
    "    distance = 1 - F.cosine_similarity(torch.tensor(vec_x), torch.tensor(vec_y))\n",
    "    \n",
    "    # しきい値(METRICS_THRESHOLD)に基づいて予測 (0 or 1)\n",
    "    # 距離がしきい値より「小さければ」正例(1)、大きければ負例(0)\n",
    "    preds = (distance < METRICS_THRESHOLD).int()\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# 訓練の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    ")\n",
    "print(\"Training arguments and metrics set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d998954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastiveTrainer initialized with margin=0.5\n",
      "\n",
      "--- Starting Model Training (Contrastive Loss) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ContrastiveTrainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     margin\u001b[38;5;241m=\u001b[39mCONTRASTIVE_MARGIN \u001b[38;5;66;03m# カスタムTrainerにマージンを渡す\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Model Training (Contrastive Loss) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Model Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2740\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2740\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 7. 訓練の開始 ---\n",
    "trainer = ContrastiveTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    margin=CONTRASTIVE_MARGIN # カスタムTrainerにマージンを渡す\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Model Training (Contrastive Loss) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. モデルの保存 ---\n",
    "print(\"Training complete. Saving best model...\")\n",
    "best_model_path = os.path.join(OUTPUT_MODEL_DIR, \"best_model\")\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. 訓練結果の可視化 ---\n",
    "print(\"\\n--- Visualizing Training Results ---\")\n",
    "log_history = trainer.state.log_history\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "df_train = df_log[df_log['loss'].notna()].copy()\n",
    "df_eval = df_log[df_log['eval_loss'].notna()].copy()\n",
    "\n",
    "if 'epoch' in df_train.columns:\n",
    "    df_train['epoch'] = df_train['epoch'].astype(int)\n",
    "if 'epoch' in df_eval.columns:\n",
    "    df_eval['epoch'] = df_eval['epoch'].astype(int)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "sns.lineplot(data=df_train, x='epoch', y='loss', label='Training Loss', ax=ax1, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_loss', label='Validation Loss', ax=ax1, marker='o')\n",
    "ax1.set_title('Training vs. Validation Loss', fontsize=16)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_f1', label='Validation F1-Score', ax=ax2, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_accuracy', label='Validation Accuracy', ax=ax2, marker='o')\n",
    "ax2.set_title('Validation Metrics', fontsize=16)\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Best Model Evaluation Metrics (from validation set) ---\")\n",
    "if not df_eval.empty:\n",
    "    best_run = df_eval.loc[df_eval['eval_loss'].idxmin()]\n",
    "    print(f\"Best Epoch (based on min eval_loss): {best_run['epoch']}\")\n",
    "    print(f\"Best Validation Loss: {best_run['eval_loss']:.4f}\")\n",
    "    print(f\"Best Validation F1: {best_run['eval_f1']:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {best_run['eval_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"No evaluation steps were completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

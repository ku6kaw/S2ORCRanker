{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f807f",
   "metadata": {},
   "source": [
    "## import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e875253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available. Device: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. GPUの確認 ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not found. Running on CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366ddf6",
   "metadata": {},
   "source": [
    "## setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set for Longformer.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 設定 ---\n",
    "\n",
    "# ▼▼▼ 修正点: 最終版のクリーニング済みデータを使用 ▼▼▼\n",
    "TRAINING_FILE = \"data/processed/training_dataset_abstract_cleaned_v3.csv\"\n",
    "\n",
    "# ▼▼▼ 修正点: モデルをSciBERTからLongformerに変更 ▼▼▼\n",
    "MODEL_CHECKPOINT = \"allenai/longformer-base-4096\"\n",
    "\n",
    "# 訓練済みモデルの保存先\n",
    "OUTPUT_MODEL_DIR = \"models/cross_encoder_longformer_v1\"\n",
    "\n",
    "# ▼▼▼ 修正点: トークン長の分析結果に基づき、最大長を2048に設定 ▼▼▼\n",
    "# (分析では最大1998だったため、2048あればほぼ全てをカバーできる)\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# ▼▼▼ 警告: Longformerはメモリ消費が激しいため、バッチサイズを小さく設定 ▼▼▼\n",
    "BATCH_SIZE = 4 # Colab T4 GPU (16GB) では 4程度。メモリ不足なら 2 や 1 に減らす\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "print(\"Configuration set for Longformer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d293a5d",
   "metadata": {},
   "source": [
    "## model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7af67",
   "metadata": {},
   "source": [
    "## dataload, tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a69c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (SMOKE TEST: 200 rows)...\n",
      "Loaded 200 pairs.\n",
      "Dataset split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 160\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['abstract_a', 'abstract_b', 'label', 'data_paper_doi'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "})\n",
      "Initializing Longformer tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset (max_length=2048)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c846fa0b96f845a6a96e7330cbb2d436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff4c7cc1d9a47c0802ac034bec78b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. データセットの読み込みとトークン化 ---\n",
    "\n",
    "# ▼▼▼ 動作確認用の設定 ▼▼▼\n",
    "SMOKE_TEST_SIZE = 200 # 読み込むデータ件数を200件に制限\n",
    "\n",
    "print(f\"Loading dataset (SMOKE TEST: {SMOKE_TEST_SIZE} rows)...\")\n",
    "# ▼▼▼ 修正点: nrowsで読み込む行数を制限 ▼▼▼\n",
    "df = pd.read_csv(TRAINING_FILE, nrows=SMOKE_TEST_SIZE)\n",
    "\n",
    "df = df.dropna(subset=['abstract_a', 'abstract_b', 'label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(f\"Loaded {len(df)} pairs.\")\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "dataset_split = raw_dataset.train_test_split(test_size=0.2, seed=42) # 200件のうち20%を検証用\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_split['train'],\n",
    "    'validation': dataset_split['test']\n",
    "})\n",
    "print(f\"Dataset split: {dataset}\")\n",
    "\n",
    "# トークナイザのロード\n",
    "print(\"Initializing Longformer tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Cross-Encoder (NSP類似型) のためのトークン化関数\n",
    "def tokenize_nsp_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"abstract_a\"], \n",
    "        examples[\"abstract_b\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "print(f\"Tokenizing dataset (max_length={MAX_LENGTH})...\")\n",
    "tokenized_datasets = dataset.map(tokenize_nsp_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"abstract_a\", \"abstract_b\", \"data_paper_doi\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1274cee",
   "metadata": {},
   "source": [
    "## model load, train setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebab402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Training arguments set for SMOKE TEST.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. モデルのロードと訓練設定 ---\n",
    "print(f\"Loading model: {MODEL_CHECKPOINT}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2).to(device)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 評価指標を計算する関数\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# ▼▼▼ 動作確認用の設定 ▼▼▼\n",
    "MAX_TRAIN_STEPS = 10  # 10ステップで訓練を強制終了\n",
    "\n",
    "# 訓練の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    # ▼▼▼ 修正点: 訓練をすぐに終わらせるための設定 ▼▼▼\n",
    "    num_train_epochs=1,            # 1エポック\n",
    "    max_steps=MAX_TRAIN_STEPS,     # 10ステップで強制終了\n",
    "    \n",
    "    evaluation_strategy=\"steps\",   # ステップごとに評価\n",
    "    eval_steps=5,                  # 5ステップごとに評価\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5,\n",
    "    # ▲▲▲ ---------------------------------- ▲▲▲\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=1,\n",
    "    gradient_checkpointing=True, \n",
    ")\n",
    "print(\"Training arguments set for SMOKE TEST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709acea",
   "metadata": {},
   "source": [
    "## train start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training (Longformer Cross-Encoder) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 訓練の開始 ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Model Training (Longformer Cross-Encoder) ---\")\n",
    "trainer.train()\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a98ba",
   "metadata": {},
   "source": [
    "## save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b944dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. モデルの保存 ---\n",
    "print(\"Training complete. Saving best model...\")\n",
    "best_model_path = os.path.join(OUTPUT_MODEL_DIR, \"best_model\")\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c1b14",
   "metadata": {},
   "source": [
    "## visualize training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. 訓練結果の可視化 ---\n",
    "print(\"\\n--- Visualizing Training Results ---\")\n",
    "log_history = trainer.state.log_history\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "df_train = df_log[df_log['loss'].notna()].copy()\n",
    "df_eval = df_log[df_log['eval_loss'].notna()].copy()\n",
    "\n",
    "# 'epoch'列を整数型に（表示のため）\n",
    "if 'epoch' in df_train.columns:\n",
    "    df_train['epoch'] = df_train['epoch'].astype(int)\n",
    "if 'epoch' in df_eval.columns:\n",
    "    df_eval['epoch'] = df_eval['epoch'].astype(int)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# グラフ1: 損失 (Loss) の推移\n",
    "sns.lineplot(data=df_train, x='epoch', y='loss', label='Training Loss', ax=ax1, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_loss', label='Validation Loss', ax=ax1, marker='o')\n",
    "ax1.set_title('Training vs. Validation Loss', fontsize=16)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# グラフ2: 評価指標 (Metrics) の推移\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_f1', label='Validation F1-Score', ax=ax2, marker='o')\n",
    "sns.lineplot(data=df_eval, x='epoch', y='eval_accuracy', label='Validation Accuracy', ax=ax2, marker='o')\n",
    "ax2.set_title('Validation Metrics', fontsize=16)\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 最終的なベストモデルの評価結果を表示\n",
    "print(\"\\n--- Best Model Evaluation Metrics (from validation set) ---\")\n",
    "if not df_eval.empty:\n",
    "    best_run = df_eval.loc[df_eval['eval_loss'].idxmin()]\n",
    "    print(f\"Best Epoch (based on min eval_loss): {best_run['epoch']}\")\n",
    "    print(f\"Best Validation Loss: {best_run['eval_loss']:.4f}\")\n",
    "    print(f\"Best Validation F1: {best_run['eval_f1']:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {best_run['eval_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"No evaluation steps were completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
